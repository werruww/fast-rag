{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6gCOZY82kqR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3zVF7MDR3CBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w235tglJ3CEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/spaces/rafaldembski/PDF-CHATBOT/blob/main/app.py\n"
      ],
      "metadata": {
        "id": "1WtI92j13S1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import streamlit as st\n",
        "import os\n",
        "import base64\n",
        "from llama_index.core import StorageContext, load_index_from_storage, VectorStoreIndex, SimpleDirectoryReader, ChatPromptTemplate\n",
        "from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
        "from dotenv import load_dotenv\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "from PIL import Image\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Configure the Llama index settings\n",
        "Settings.llm = HuggingFaceInferenceAPI(\n",
        "    model_name=\"google/gemma-1.1-7b-it\",\n",
        "    tokenizer_name=\"google/gemma-1.1-7b-it\",\n",
        "    context_window=3000,\n",
        "    token=os.getenv(\"HF_TOKEN\"),\n",
        "    max_new_tokens=512,\n",
        "    generate_kwargs={\"temperature\": 0.1},\n",
        ")\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
        ")\n",
        "\n",
        "# Define the directory for persistent storage and data\n",
        "PERSIST_DIR = \"./db\"\n",
        "DATA_DIR = \"data\"\n",
        "\n",
        "# Ensure data directory exists\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
        "\n",
        "# Language descriptions\n",
        "descriptions = {\n",
        "    \"pl\": \"\"\"\n",
        "    # ChatPDF\n",
        "    **ChatPDF** to zaawansowane narzędzie oparte na sztucznej inteligencji, zaprojektowane do analizy i generowania odpowiedzi na pytania związane z treścią załadowanych dokumentów PDF. Aplikacja umożliwia użytkownikom wprowadzanie zapytań dotyczących zawartości dokumentów i otrzymywanie precyzyjnych odpowiedzi w oparciu o zaawansowane algorytmy uczenia maszynowego.\n",
        "    **Jak korzystać z aplikacji**:\n",
        "    1. Wgraj plik PDF, korzystając z przycisku **Submit & Process**.\n",
        "    2. Poczekaj, aż plik PDF zostanie przetworzony.\n",
        "    3. Zadawaj pytania dotyczące zawartości pliku, określając język, w jakim ma być wygenerowana odpowiedź.\n",
        "    **Technologie**:\n",
        "    - Model: Gemma 1.1-7b-it\n",
        "    - Stworzony przez: Rafał Dembski\n",
        "    - Technologie: LlamaIndex, PyTorch, Streamlit\n",
        "    \"\"\",\n",
        "    \"en\": \"\"\"\n",
        "    # ChatPDF\n",
        "    **ChatPDF** is an advanced AI-powered tool designed to analyze and generate answers to questions related to the content of uploaded PDF documents. The application allows users to input queries about document contents and receive precise responses based on advanced machine learning algorithms.\n",
        "    **How to use the application**:\n",
        "    1. Upload a PDF file using the **Submit & Process** button.\n",
        "    2. Wait for the PDF file to be processed.\n",
        "    3. Ask questions about the content of the file, specifying the language in which you want the response to be generated.\n",
        "    **Technologies**:\n",
        "    - Model: Gemma 1.1-7b-it\n",
        "    - Developed by: Rafał Dembski\n",
        "    - Technologies: LlamaIndex, PyTorch, Streamlit\n",
        "    \"\"\",\n",
        "    \"de\": \"\"\"\n",
        "    # ChatPDF\n",
        "    **ChatPDF** ist ein fortschrittliches, KI-gesteuertes Tool, das entwickelt wurde, um Fragen zur Analyse und Beantwortung von Fragen im Zusammenhang mit dem Inhalt hochgeladener PDF-Dokumente zu generieren. Die Anwendung ermöglicht es Benutzern, Anfragen bezüglich des Dokumenteninhalts einzugeben und präzise Antworten basierend auf fortschrittlichen maschinellen Lernalgorithmen zu erhalten.\n",
        "    **So verwenden Sie die Anwendung**:\n",
        "    1. Laden Sie eine PDF-Datei über die Schaltfläche **Submit & Process** hoch.\n",
        "    2. Warten Sie, bis die PDF-Datei verarbeitet wurde.\n",
        "    3. Stellen Sie Fragen zum Inhalt der Datei und geben Sie an, in welcher Sprache die Antwort generiert werden soll.\n",
        "    **Technologien**:\n",
        "    - Modell: Gemma 1.1-7b-it\n",
        "    - Entwickelt von: Rafał Dembski\n",
        "    - Technologien: LlamaIndex, PyTorch, Streamlit\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "def displayPDF(file):\n",
        "    with open(file, \"rb\") as f:\n",
        "        base64_pdf = base64.b64encode(f.read()).decode('utf-8')\n",
        "    pdf_display = f'<iframe src=\"data:application/pdf;base64,{base64_pdf}\" width=\"100%\" height=\"600\" type=\"application/pdf\"></iframe>'\n",
        "    st.markdown(pdf_display, unsafe_allow_html=True)\n",
        "\n",
        "def data_ingestion():\n",
        "    documents = SimpleDirectoryReader(DATA_DIR).load_data()\n",
        "    storage_context = StorageContext.from_defaults()\n",
        "    index = VectorStoreIndex.from_documents(documents)\n",
        "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
        "\n",
        "def handle_query(query):\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
        "    index = load_index_from_storage(storage_context)\n",
        "    chat_text_qa_msgs = [\n",
        "    (\n",
        "        \"user\",\n",
        "        \"\"\"You are a Q&A assistant named ChatPDF. You have a specific response programmed for when users specifically ask about your creator, Suriya. The response is: \"I was created by Suriya, an enthusiast in Artificial Intelligence. He is dedicated to solving complex problems and delivering innovative solutions. With a strong focus on machine learning, deep learning, Python, generative AI, NLP, and computer vision, Suriya is passionate about pushing the boundaries of AI to explore new possibilities.\" For all other inquiries, your main goal is to provide answers as accurately as possible, based on the instructions and context you have been given. If a question does not match the provided context or is outside the scope of the document, kindly advise the user to ask questions within the context of the document.\n",
        "        Context:\n",
        "        {context_str}\n",
        "        Question:\n",
        "        {query_str}\n",
        "        \"\"\"\n",
        "    )\n",
        "    ]\n",
        "    text_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)\n",
        "\n",
        "    query_engine = index.as_query_engine(text_qa_template=text_qa_template)\n",
        "    answer = query_engine.query(query)\n",
        "\n",
        "    if hasattr(answer, 'response'):\n",
        "        return answer.response\n",
        "    elif isinstance(answer, dict) and 'response' in answer:\n",
        "        return answer['response']\n",
        "    else:\n",
        "        return \"Sorry, I couldn't find an answer.\"\n",
        "\n",
        "# Streamlit app initialization\n",
        "# Language selection\n",
        "selected_language = st.sidebar.selectbox(\"Wybierz język / Select Language / Sprache auswählen\", (\"pl\", \"en\", \"de\"))\n",
        "\n",
        "# Display description based on selected language\n",
        "st.markdown(descriptions[selected_language])\n",
        "\n",
        "if 'messages' not in st.session_state:\n",
        "    st.session_state.messages = [{'role': 'assistant', \"content\": 'Hello! Upload a PDF and ask me anything about its content.'}]\n",
        "\n",
        "with st.sidebar:\n",
        "    st.title(\"Menu:\")\n",
        "    uploaded_file = st.file_uploader(\"Upload your PDF Files and Click on the Submit & Process Button\")\n",
        "    if st.button(\"Submit & Process\"):\n",
        "        with st.spinner(\"Processing...\"):\n",
        "            filepath = \"data/saved_pdf.pdf\"\n",
        "            with open(filepath, \"wb\") as f:\n",
        "                f.write(uploaded_file.getbuffer())\n",
        "            data_ingestion()  # Process PDF every time new file is uploaded\n",
        "            st.success(\"Done\")\n",
        "\n",
        "user_prompt = st.chat_input(\"Ask me anything about the content of the PDF:\")\n",
        "if user_prompt:\n",
        "    st.session_state.messages.append({'role': 'user', \"content\": user_prompt})\n",
        "    response = handle_query(user_prompt)\n",
        "    st.session_state.messages.append({'role': 'assistant', \"content\": response})\n",
        "\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message['role']):\n",
        "        st.write(message['content'])\n"
      ],
      "metadata": {
        "id": "qct18eJj3CH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "streamlit\n",
        "python-dotenv\n",
        "llama-index\n",
        "llama-index-embeddings-huggingface\n",
        "llama-index-llms-huggingface"
      ],
      "metadata": {
        "id": "5SEarGX_3CKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import streamlit as st\n",
        "import os\n",
        "import base64\n",
        "from llama_index.core import StorageContext, load_index_from_storage, VectorStoreIndex, SimpleDirectoryReader, ChatPromptTemplate\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM # Changed import here\n",
        "from transformers import pipeline # Import pipeline from transformers\n",
        "from dotenv import load_dotenv\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "from PIL import Image\n",
        "\n",
        "# Load environment variables (optional, if you still need dotenv for other things)\n",
        "load_dotenv()\n",
        "\n",
        "# Configure the Llama index settings\n",
        "# Load the Gemma model using Hugging Face pipeline\n",
        "hf_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"google/gemma-1.1-7b-it\",\n",
        "    tokenizer=\"google/gemma-1.1-7b-it\",\n",
        "    torch_dtype=torch.float16, # Optional: Use float16 for less memory if you have CUDA and torch >= 2.0.1\n",
        "    device_map=\"auto\", # or \"cuda:0\" if you have specific GPU\n",
        ")\n",
        "\n",
        "Settings.llm = HuggingFaceLLM(\n",
        "    pipeline=hf_pipeline,\n",
        ")\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
        ")\n",
        "\n",
        "# Define the directory for persistent storage and data\n",
        "PERSIST_DIR = \"./db\"\n",
        "DATA_DIR = \"data\"\n",
        "\n",
        "# Ensure data directory exists\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
        "\n",
        "# Language descriptions\n",
        "descriptions = {\n",
        "    \"pl\": \"\"\"\n",
        "    # ChatPDF\n",
        "    **ChatPDF** to zaawansowane narzędzie oparte na sztucznej inteligencji, zaprojektowane do analizy i generowania odpowiedzi na pytania związane z treścią załadowanych dokumentów PDF. Aplikacja umożliwia użytkownikom wprowadzanie zapytań dotyczących zawartości dokumentów i otrzymywanie precyzyjnych odpowiedzi w oparciu o zaawansowane algorytmy uczenia maszynowego.\n",
        "    **Jak korzystać z aplikacji**:\n",
        "    1. Wgraj plik PDF, korzystając z przycisku **Submit & Process**.\n",
        "    2. Poczekaj, aż plik PDF zostanie przetworzony.\n",
        "    3. Zadawaj pytania dotyczące zawartości pliku, określając język, w jakim ma być wygenerowana odpowiedź.\n",
        "    **Technologie**:\n",
        "    - Model: Gemma 1.1-7b-it (Lokalnie)\n",
        "    - Stworzony przez: Rafał Dembski\n",
        "    - Technologie: LlamaIndex, PyTorch, Streamlit\n",
        "    \"\"\",\n",
        "    \"en\": \"\"\"\n",
        "    # ChatPDF\n",
        "    **ChatPDF** is an advanced AI-powered tool designed to analyze and generate answers to questions related to the content of uploaded PDF documents. The application allows users to input queries about document contents and receive precise responses based on advanced machine learning algorithms.\n",
        "    **How to use the application**:\n",
        "    1. Upload a PDF file using the **Submit & Process** button.\n",
        "    2. Wait for the PDF file to be processed.\n",
        "    3. Ask questions about the content of the file, specifying the language in which you want the response to be generated.\n",
        "    **Technologies**:\n",
        "    - Model: Gemma 1.1-7b-it (Local)\n",
        "    - Developed by: Rafał Dembski\n",
        "    - Technologies: LlamaIndex, PyTorch, Streamlit\n",
        "    \"\"\",\n",
        "    \"de\": \"\"\"\n",
        "    # ChatPDF\n",
        "    **ChatPDF** ist ein fortschrittliches, KI-gesteuertes Tool, das entwickelt wurde, um Fragen zur Analyse und Beantwortung von Fragen im Zusammenhang mit dem Inhalt hochgeladener PDF-Dokumente zu generieren. Die Anwendung ermöglicht es Benutzern, Anfragen bezüglich des Dokumenteninhalts einzugeben und präzise Antworten basierend auf fortschrittlichen maschinellen Lernalgorithmen zu erhalten.\n",
        "    **So verwenden Sie die Anwendung**:\n",
        "    1. Laden Sie eine PDF-Datei über die Schaltfläche **Submit & Process** hoch.\n",
        "    2. Warten Sie, bis die PDF-Datei verarbeitet wurde.\n",
        "    3. Stellen Sie Fragen zum Inhalt der Datei und geben Sie an, in welcher Sprache die Antwort generiert werden soll.\n",
        "    **Technologien**:\n",
        "    - Modell: Gemma 1.1-7b-it (Lokal)\n",
        "    - Entwickelt von: Rafał Dembski\n",
        "    - Technologien: LlamaIndex, PyTorch, Streamlit\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "def displayPDF(file):\n",
        "    with open(file, \"rb\") as f:\n",
        "        base64_pdf = base64.b64encode(f.read()).decode('utf-8')\n",
        "    pdf_display = f'<iframe src=\"data:application/pdf;base64,{base64_pdf}\" width=\"100%\" height=\"600\" type=\"application/pdf\"></iframe>'\n",
        "    st.markdown(pdf_display, unsafe_allow_html=True)\n",
        "\n",
        "def data_ingestion():\n",
        "    documents = SimpleDirectoryReader(DATA_DIR).load_data()\n",
        "    storage_context = StorageContext.from_defaults()\n",
        "    index = VectorStoreIndex.from_documents(documents)\n",
        "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
        "\n",
        "def handle_query(query):\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
        "    index = load_index_from_storage(storage_context)\n",
        "    chat_text_qa_msgs = [\n",
        "    (\n",
        "        \"user\",\n",
        "        \"\"\"You are a Q&A assistant named ChatPDF. You have a specific response programmed for when users specifically ask about your creator, Suriya. The response is: \"I was created by Suriya, an enthusiast in Artificial Intelligence. He is dedicated to solving complex problems and delivering innovative solutions. With a strong focus on machine learning, deep learning, Python, generative AI, NLP, and computer vision, Suriya is passionate about pushing the boundaries of AI to explore new possibilities.\" For all other inquiries, your main goal is to provide answers as accurately as possible, based on the instructions and context you have been given. If a question does not match the provided context or is outside the scope of the document, kindly advise the user to ask questions within the context of the document.\n",
        "        Context:\n",
        "        {context_str}\n",
        "        Question:\n",
        "        {query_str}\n",
        "        \"\"\"\n",
        "    )\n",
        "    ]\n",
        "    text_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)\n",
        "\n",
        "    query_engine = index.as_query_engine(text_qa_template=text_qa_template)\n",
        "    answer = query_engine.query(query)\n",
        "\n",
        "    if hasattr(answer, 'response'):\n",
        "        return answer.response\n",
        "    elif isinstance(answer, dict) and 'response' in answer:\n",
        "        return answer['response']\n",
        "    else:\n",
        "        return \"Sorry, I couldn't find an answer.\"\n",
        "\n",
        "# Streamlit app initialization\n",
        "# Language selection\n",
        "selected_language = st.sidebar.selectbox(\"Wybierz język / Select Language / Sprache auswählen\", (\"pl\", \"en\", \"de\"))\n",
        "\n",
        "# Display description based on selected language\n",
        "st.markdown(descriptions[selected_language])\n",
        "\n",
        "if 'messages' not in st.session_state:\n",
        "    st.session_state.messages = [{'role': 'assistant', \"content\": 'Hello! Upload a PDF and ask me anything about its content.'}]\n",
        "\n",
        "with st.sidebar:\n",
        "    st.title(\"Menu:\")\n",
        "    uploaded_file = st.file_uploader(\"Upload your PDF Files and Click on the Submit & Process Button\")\n",
        "    if st.button(\"Submit & Process\"):\n",
        "        with st.spinner(\"Processing...\"):\n",
        "            filepath = \"data/saved_pdf.pdf\"\n",
        "            with open(filepath, \"wb\") as f:\n",
        "                f.write(uploaded_file.getbuffer())\n",
        "            data_ingestion()  # Process PDF every time new file is uploaded\n",
        "            st.success(\"Done\")\n",
        "\n",
        "user_prompt = st.chat_input(\"Ask me anything about the content of the PDF:\")\n",
        "if user_prompt:\n",
        "    st.session_state.messages.append({'role': 'user', \"content\": user_prompt})\n",
        "    response = handle_query(user_prompt)\n",
        "    st.session_state.messages.append({'role': 'assistant', \"content\": response})\n",
        "\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message['role']):\n",
        "        st.write(message['content'])"
      ],
      "metadata": {
        "id": "ZlNRvQDn2-7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "streamlit\n",
        "python-dotenv\n",
        "llama-index\n",
        "llama-index-embeddings-huggingface\n",
        "llama-index-llms-huggingface\n",
        "numpy\n",
        "llama-index\n",
        "transformers\n",
        "python-dotenv\n",
        "torch"
      ],
      "metadata": {
        "id": "QyJZIHIn5dOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r a.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyHcBouO3w1A",
        "outputId": "101332a5-4547-4ce2-b5b3-4ee3aadff6e5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (from -r a.txt (line 1)) (1.42.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from -r a.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.11/dist-packages (from -r a.txt (line 3)) (0.12.22)\n",
            "Requirement already satisfied: llama-index-embeddings-huggingface in /usr/local/lib/python3.11/dist-packages (from -r a.txt (line 4)) (0.5.2)\n",
            "Requirement already satisfied: llama-index-llms-huggingface in /usr/local/lib/python3.11/dist-packages (from -r a.txt (line 5)) (0.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r a.txt (line 6)) (1.26.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from -r a.txt (line 8)) (4.48.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from -r a.txt (line 10)) (2.5.1+cu124)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (8.1.8)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (4.25.6)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (6.4.2)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->-r a.txt (line 3)) (0.4.6)\n",
            "Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from llama-index->-r a.txt (line 3)) (0.4.1)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.22 in /usr/local/lib/python3.11/dist-packages (from llama-index->-r a.txt (line 3)) (0.12.22)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->-r a.txt (line 3)) (0.3.1)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->-r a.txt (line 3)) (0.6.8)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->-r a.txt (line 3)) (0.3.25)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->-r a.txt (line 3)) (0.4.3)\n",
            "Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->-r a.txt (line 3)) (0.3.1)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->-r a.txt (line 3)) (0.3.0)\n",
            "Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->-r a.txt (line 3)) (0.4.5)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->-r a.txt (line 3)) (0.4.0)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index->-r a.txt (line 3)) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r a.txt (line 4)) (0.28.1)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-huggingface->-r a.txt (line 4)) (3.4.1)\n",
            "Requirement already satisfied: text-generation<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-huggingface->-r a.txt (line 5)) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers->-r a.txt (line 8)) (3.17.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers->-r a.txt (line 8)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->-r a.txt (line 8)) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->-r a.txt (line 8)) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers->-r a.txt (line 8)) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers->-r a.txt (line 8)) (4.67.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->-r a.txt (line 10)) (1.3.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit->-r a.txt (line 1)) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit->-r a.txt (line 1)) (1.28.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r a.txt (line 1)) (4.0.12)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r a.txt (line 4)) (3.11.13)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index->-r a.txt (line 3)) (1.61.1)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (2.0.38)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (1.6.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (2.10.6)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (0.9.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (1.17.2)\n",
            "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.13 in /usr/local/lib/python3.11/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index->-r a.txt (line 3)) (0.1.13)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index->-r a.txt (line 3)) (4.13.3)\n",
            "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index->-r a.txt (line 3)) (5.3.1)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index->-r a.txt (line 3)) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r a.txt (line 3)) (0.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index->-r a.txt (line 3)) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit->-r a.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit->-r a.txt (line 1)) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit->-r a.txt (line 1)) (2025.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->-r a.txt (line 10)) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit->-r a.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit->-r a.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit->-r a.txt (line 1)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit->-r a.txt (line 1)) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit->-r a.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit->-r a.txt (line 1)) (2.18.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface->-r a.txt (line 4)) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface->-r a.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface->-r a.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface->-r a.txt (line 5)) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r a.txt (line 4)) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r a.txt (line 4)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r a.txt (line 4)) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r a.txt (line 4)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r a.txt (line 4)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r a.txt (line 4)) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r a.txt (line 4)) (1.18.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index->-r a.txt (line 3)) (2.6)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r a.txt (line 1)) (5.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r a.txt (line 1)) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r a.txt (line 1)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r a.txt (line 1)) (0.23.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (0.14.0)\n",
            "Requirement already satisfied: llama-cloud-services>=0.6.2 in /usr/local/lib/python3.11/dist-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r a.txt (line 3)) (0.6.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->-r a.txt (line 1)) (0.1.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index->-r a.txt (line 3)) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index->-r a.txt (line 3)) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index->-r a.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit->-r a.txt (line 1)) (1.17.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (3.26.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface->-r a.txt (line 4)) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token ْْْْْْْْْْXXXXXXX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttBqGadC6OvL",
        "outputId": "08074709-7fba-4292-9078-9d145cb40141"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `read` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `read`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "google/gemma-1.1-7b-it"
      ],
      "metadata": {
        "id": "-uci-udW5uHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import base64\n",
        "from llama_index.core import StorageContext, load_index_from_storage, VectorStoreIndex, SimpleDirectoryReader, ChatPromptTemplate\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from transformers import pipeline\n",
        "from dotenv import load_dotenv\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "import torch # Import torch if you are using torch_dtype\n",
        "\n",
        "# Load environment variables (optional, if you still need dotenv for other things)\n",
        "load_dotenv()\n",
        "\n",
        "# Configure the Llama index settings\n",
        "# Load the Gemma model using Hugging Face pipeline\n",
        "hf_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    tokenizer=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32, # Use float16 if CUDA is available, otherwise float32\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else \"cpu\", # Use GPU if available, otherwise CPU\n",
        ")\n",
        "\n",
        "Settings.llm = HuggingFaceLLM(\n",
        "    pipeline=hf_pipeline,\n",
        ")\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
        ")\n",
        "\n",
        "# Define the directory for persistent storage and data\n",
        "PERSIST_DIR = \"./db\"\n",
        "DATA_DIR = \"data\"\n",
        "\n",
        "# Ensure data directory exists\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
        "\n",
        "# Language descriptions (not used in non-UI version, but kept for reference)\n",
        "descriptions = {\n",
        "    \"pl\": \"\"\"\n",
        "    # ChatPDF (Non-UI)\n",
        "    **ChatPDF** to zaawansowane narzędzie oparte na sztucznej inteligencji, zaprojektowane do analizy i generowania odpowiedzi na pytania związane z treścią załadowanych dokumentów PDF. Działa w trybie wiersza poleceń.\n",
        "    **Technologie**:\n",
        "    - Model: Gemma 1.1-7b-it (Lokalnie)\n",
        "    - Stworzony przez: Rafał Dembski\n",
        "    - Technologie: LlamaIndex, PyTorch, Transformers\n",
        "    \"\"\",\n",
        "    \"en\": \"\"\"\n",
        "    # ChatPDF (Non-UI)\n",
        "    **ChatPDF** is an advanced AI-powered tool designed to analyze and generate answers to questions related to the content of uploaded PDF documents. Runs in command-line mode.\n",
        "    **Technologies**:\n",
        "    - Model: Gemma 1.1-7b-it (Local)\n",
        "    - Developed by: Rafał Dembski\n",
        "    - Technologies: LlamaIndex, PyTorch, Transformers\n",
        "    \"\"\",\n",
        "    \"de\": \"\"\"\n",
        "    # ChatPDF (Non-UI)\n",
        "    **ChatPDF** ist ein fortschrittliches, KI-gesteuertes Tool, das entwickelt wurde, um Fragen zur Analyse und Beantwortung von Fragen im Zusammenhang mit dem Inhalt hochgeladener PDF-Dokumente zu generieren. Läuft im Kommandozeilenmodus.\n",
        "    **Technologien**:\n",
        "    - Modell: Gemma 1.1-7b-it (Lokal)\n",
        "    - Entwickelt von: Rafał Dembski\n",
        "    - Technologien: LlamaIndex, PyTorch, Transformers\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "def data_ingestion():\n",
        "    documents = SimpleDirectoryReader(DATA_DIR).load_data()\n",
        "    storage_context = StorageContext.from_defaults()\n",
        "    index = VectorStoreIndex.from_documents(documents)\n",
        "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
        "    print(\"PDF data ingested and vector store created/updated.\")\n",
        "\n",
        "def handle_query(query):\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
        "    index = load_index_from_storage(storage_context)\n",
        "    chat_text_qa_msgs = [\n",
        "    (\n",
        "        \"user\",\n",
        "        \"\"\"You are a Q&A assistant named ChatPDF. You have a specific response programmed for when users specifically ask about your creator, Suriya. The response is: \"I was created by Suriya, an enthusiast in Artificial Intelligence. He is dedicated to solving complex problems and delivering innovative solutions. With a strong focus on machine learning, deep learning, Python, generative AI, NLP, and computer vision, Suriya is passionate about pushing the boundaries of AI to explore new possibilities.\" For all other inquiries, your main goal is to provide answers as accurately as possible, based on the instructions and context you have been given. If a question does not match the provided context or is outside the scope of the document, kindly advise the user to ask questions within the context of the document.\n",
        "        Context:\n",
        "        {context_str}\n",
        "        Question:\n",
        "        {query_str}\n",
        "        \"\"\"\n",
        "    )\n",
        "    ]\n",
        "    text_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)\n",
        "\n",
        "    query_engine = index.as_query_engine(text_qa_template=text_qa_template)\n",
        "    answer = query_engine.query(query)\n",
        "\n",
        "    if hasattr(answer, 'response'):\n",
        "        return answer.response\n",
        "    elif isinstance(answer, dict) and 'response' in answer:\n",
        "        return answer['response']\n",
        "    else:\n",
        "        return \"Sorry, I couldn't find an answer.\"\n",
        "\n",
        "# Main function to run without UI\n",
        "def main():\n",
        "    pdf_file_path = \"/content/sample.pdf\" # Path to your PDF file\n",
        "    user_question = \"What is the main topic of this document?\" # Your question\n",
        "\n",
        "    # Check if PDF data directory is empty, or vector store doesn't exist, then ingest data\n",
        "    if not os.listdir(DATA_DIR) or not os.path.exists(PERSIST_DIR):\n",
        "        print(\"No PDF data or vector store found. Processing PDF...\")\n",
        "        data_ingestion()\n",
        "    else:\n",
        "        print(\"Vector store already exists. Loading existing store.\")\n",
        "\n",
        "    print(f\"Question: {user_question}\")\n",
        "    response = handle_query(user_question)\n",
        "    print(f\"Answer: {response}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "kXcpcwYm3Lef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "import torch\n",
        "from transformers import pipeline # Make sure to keep this import\n",
        "\n",
        "# ... other imports and code ...\n",
        "\n",
        "# Configure the Llama index settings\n",
        "# Initialize HuggingFaceLLM directly with model_name\n",
        "Settings.llm = HuggingFaceLLM(\n",
        "    model_name=\"google/gemma-1.1-7b-it\",\n",
        "    tokenizer_name=\"google/gemma-1.1-7b-it\", # You can explicitly set tokenizer_name, though often it's inferred from model_name\n",
        "    tokenizer_kwargs={\"torch_dtype\": torch.float16 if torch.cuda.is_available() else torch.float32}, # Tokenizer kwargs\n",
        "    model_kwargs={\"torch_dtype\": torch.float16 if torch.cuda.is_available() else torch.float32, \"device_map\": \"auto\" if torch.cuda.is_available() else \"cpu\"}, # Model kwargs\n",
        "    context_window=3000, # Keep other parameters if you need them, adjust as necessary\n",
        "    max_new_tokens=512,\n",
        "    generate_kwargs={\"temperature\": 0.1},\n",
        ")\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
        ")\n",
        "\n",
        "# ... rest of your code ...\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import base64\n",
        "from llama_index.core import StorageContext, load_index_from_storage, VectorStoreIndex, SimpleDirectoryReader, ChatPromptTemplate\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from transformers import pipeline\n",
        "from dotenv import load_dotenv\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "import torch # Import torch if you are using torch_dtype\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "# Load environment variables (optional, if you still need dotenv for other things)\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "Settings.llm = HuggingFaceLLM(\n",
        "    model_name=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    tokenizer_name=\"meta-llama/Llama-3.2-1B-Instruct\", # You can explicitly set tokenizer_name, though often it's inferred from model_name\n",
        "    tokenizer_kwargs={\"torch_dtype\": torch.float16 if torch.cuda.is_available() else torch.float32}, # Tokenizer kwargs\n",
        "    model_kwargs={\"torch_dtype\": torch.float16 if torch.cuda.is_available() else torch.float32}, # Model kwargs - REMOVED device_map HERE\n",
        "    context_window=3000, # Keep other parameters if you need them, adjust as necessary\n",
        "    max_new_tokens=5,\n",
        "    generate_kwargs={\"temperature\": 0.1},\n",
        ")\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the directory for persistent storage and data\n",
        "PERSIST_DIR = \"./db\"\n",
        "DATA_DIR = \"data\"\n",
        "\n",
        "# Ensure data directory exists\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
        "\n",
        "# Language descriptions (not used in non-UI version, but kept for reference)\n",
        "descriptions = {\n",
        "    \"pl\": \"\"\"\n",
        "    # ChatPDF (Non-UI)\n",
        "    **ChatPDF** to zaawansowane narzędzie oparte na sztucznej inteligencji, zaprojektowane do analizy i generowania odpowiedzi na pytania związane z treścią załadowanych dokumentów PDF. Działa w trybie wiersza poleceń.\n",
        "    **Technologie**:\n",
        "    - Model: Gemma 1.1-7b-it (Lokalnie)\n",
        "    - Stworzony przez: Rafał Dembski\n",
        "    - Technologie: LlamaIndex, PyTorch, Transformers\n",
        "    \"\"\",\n",
        "    \"en\": \"\"\"\n",
        "    # ChatPDF (Non-UI)\n",
        "    **ChatPDF** is an advanced AI-powered tool designed to analyze and generate answers to questions related to the content of uploaded PDF documents. Runs in command-line mode.\n",
        "    **Technologies**:\n",
        "    - Model: Gemma 1.1-7b-it (Local)\n",
        "    - Developed by: Rafał Dembski\n",
        "    - Technologies: LlamaIndex, PyTorch, Transformers\n",
        "    \"\"\",\n",
        "    \"de\": \"\"\"\n",
        "    # ChatPDF (Non-UI)\n",
        "    **ChatPDF** ist ein fortschrittliches, KI-gesteuertes Tool, das entwickelt wurde, um Fragen zur Analyse und Beantwortung von Fragen im Zusammenhang mit dem Inhalt hochgeladener PDF-Dokumente zu generieren. Läuft im Kommandozeilenmodus.\n",
        "    **Technologien**:\n",
        "    - Modell: Gemma 1.1-7b-it (Lokal)\n",
        "    - Entwickelt von: Rafał Dembski\n",
        "    - Technologien: LlamaIndex, PyTorch, Transformers\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "def data_ingestion():\n",
        "    documents = SimpleDirectoryReader(DATA_DIR).load_data()\n",
        "    storage_context = StorageContext.from_defaults()\n",
        "    index = VectorStoreIndex.from_documents(documents)\n",
        "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
        "    print(\"PDF data ingested and vector store created/updated.\")\n",
        "\n",
        "def handle_query(query):\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
        "    index = load_index_from_storage(storage_context)\n",
        "    chat_text_qa_msgs = [\n",
        "    (\n",
        "        \"user\",\n",
        "        \"\"\"You are a Q&A assistant named ChatPDF. You have a specific response programmed for when users specifically ask about your creator, Suriya. The response is: \"I was created by Suriya, an enthusiast in Artificial Intelligence. He is dedicated to solving complex problems and delivering innovative solutions. With a strong focus on machine learning, deep learning, Python, generative AI, NLP, and computer vision, Suriya is passionate about pushing the boundaries of AI to explore new possibilities.\" For all other inquiries, your main goal is to provide answers as accurately as possible, based on the instructions and context you have been given. If a question does not match the provided context or is outside the scope of the document, kindly advise the user to ask questions within the context of the document.\n",
        "        Context:\n",
        "        {context_str}\n",
        "        Question:\n",
        "        {query_str}\n",
        "        \"\"\"\n",
        "    )\n",
        "    ]\n",
        "    text_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)\n",
        "\n",
        "    query_engine = index.as_query_engine(text_qa_template=text_qa_template)\n",
        "    answer = query_engine.query(query)\n",
        "\n",
        "    if hasattr(answer, 'response'):\n",
        "        return answer.response\n",
        "    elif isinstance(answer, dict) and 'response' in answer:\n",
        "        return answer['response']\n",
        "    else:\n",
        "        return \"Sorry, I couldn't find an answer.\"\n",
        "\n",
        "# Main function to run without UI\n",
        "def main():\n",
        "    pdf_file_path = \"/content/sample.pdf\" # Path to your PDF file\n",
        "    user_question = \"What is the main topic of this document?\" # Your question\n",
        "\n",
        "    # Check if PDF data directory is empty, or vector store doesn't exist, then ingest data\n",
        "    if not os.listdir(DATA_DIR) or not os.path.exists(PERSIST_DIR):\n",
        "        print(\"No PDF data or vector store found. Processing PDF...\")\n",
        "        data_ingestion()\n",
        "    else:\n",
        "        print(\"Vector store already exists. Loading existing store.\")\n",
        "\n",
        "    print(f\"Question: {user_question}\")\n",
        "    response = handle_query(user_question)\n",
        "    print(f\"Answer: {response}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "scsIzEAh6xs5",
        "outputId": "a7136d82-fd9f-4d83-a8ee-b2a0bec93d96"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained() got multiple values for keyword argument 'device_map'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-137246e34bab>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Configure the Llama index settings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Initialize HuggingFaceLLM directly with model_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m Settings.llm = HuggingFaceLLM(\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"google/gemma-1.1-7b-it\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtokenizer_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"google/gemma-1.1-7b-it\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# You can explicitly set tokenizer_name, though often it's inferred from model_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/llms/huggingface/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, context_window, max_new_tokens, query_wrapper_prompt, tokenizer_name, model_name, model, tokenizer, device_map, stopping_ids, tokenizer_kwargs, tokenizer_outputs_to_remove, model_kwargs, generate_kwargs, is_chat_model, callback_manager, system_prompt, messages_to_prompt, completion_to_prompt, pydantic_program_mode, output_parser)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;34m\"\"\"Initialize params.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mmodel_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         model = model or AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m    237\u001b[0m             \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         )\n",
            "\u001b[0;31mTypeError\u001b[0m: transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained() got multiple values for keyword argument 'device_map'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Settings.llm = HuggingFaceLLM(\n",
        "    model_name=\"google/gemma-1.1-7b-it\",\n",
        "    tokenizer_name=\"google/gemma-1.1-7b-it\", # You can explicitly set tokenizer_name, though often it's inferred from model_name\n",
        "    tokenizer_kwargs={\"torch_dtype\": torch.float16 if torch.cuda.is_available() else torch.float32}, # Tokenizer kwargs\n",
        "    model_kwargs={\"torch_dtype\": torch.float16 if torch.cuda.is_available() else torch.float32}, # Model kwargs - REMOVED device_map HERE\n",
        "    context_window=3000, # Keep other parameters if you need them, adjust as necessary\n",
        "    max_new_tokens=512,\n",
        "    generate_kwargs={\"temperature\": 0.1},\n",
        ")"
      ],
      "metadata": {
        "id": "7cQlUHYj76XU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import base64\n",
        "from llama_index.core import StorageContext, load_index_from_storage, VectorStoreIndex, SimpleDirectoryReader, ChatPromptTemplate\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from transformers import pipeline\n",
        "from dotenv import load_dotenv\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "import torch # Import torch if you are using torch_dtype\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "# Load environment variables (optional, if you still need dotenv for other things)\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "Settings.llm = HuggingFaceLLM(\n",
        "    model_name=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    tokenizer_name=\"meta-llama/Llama-3.2-1B-Instruct\", # You can explicitly set tokenizer_name, though often it's inferred from model_name\n",
        "    tokenizer_kwargs={\"torch_dtype\": torch.float16 if torch.cuda.is_available() else torch.float32}, # Tokenizer kwargs\n",
        "    model_kwargs={\"torch_dtype\": torch.float16 if torch.cuda.is_available() else torch.float32}, # Model kwargs - REMOVED device_map HERE\n",
        "    context_window=3000, # Keep other parameters if you need them, adjust as necessary\n",
        "    max_new_tokens=5,\n",
        "    generate_kwargs={\"temperature\": 0.1},\n",
        ")\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the directory for persistent storage and data\n",
        "PERSIST_DIR = \"./db\"\n",
        "DATA_DIR = \"data\"\n",
        "\n",
        "# Ensure data directory exists\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
        "\n",
        "# Language descriptions (not used in non-UI version, but kept for reference)\n",
        "descriptions = {\n",
        "    \"pl\": \"\"\"\n",
        "    # ChatPDF (Non-UI)\n",
        "    **ChatPDF** to zaawansowane narzędzie oparte na sztucznej inteligencji, zaprojektowane do analizy i generowania odpowiedzi na pytania związane z treścią załadowanych dokumentów PDF. Działa w trybie wiersza poleceń.\n",
        "    **Technologie**:\n",
        "    - Model: Gemma 1.1-7b-it (Lokalnie)\n",
        "    - Stworzony przez: Rafał Dembski\n",
        "    - Technologie: LlamaIndex, PyTorch, Transformers\n",
        "    \"\"\",\n",
        "    \"en\": \"\"\"\n",
        "    # ChatPDF (Non-UI)\n",
        "    **ChatPDF** is an advanced AI-powered tool designed to analyze and generate answers to questions related to the content of uploaded PDF documents. Runs in command-line mode.\n",
        "    **Technologies**:\n",
        "    - Model: Gemma 1.1-7b-it (Local)\n",
        "    - Developed by: Rafał Dembski\n",
        "    - Technologies: LlamaIndex, PyTorch, Transformers\n",
        "    \"\"\",\n",
        "    \"de\": \"\"\"\n",
        "    # ChatPDF (Non-UI)\n",
        "    **ChatPDF** ist ein fortschrittliches, KI-gesteuertes Tool, das entwickelt wurde, um Fragen zur Analyse und Beantwortung von Fragen im Zusammenhang mit dem Inhalt hochgeladener PDF-Dokumente zu generieren. Läuft im Kommandozeilenmodus.\n",
        "    **Technologien**:\n",
        "    - Modell: Gemma 1.1-7b-it (Lokal)\n",
        "    - Entwickelt von: Rafał Dembski\n",
        "    - Technologien: LlamaIndex, PyTorch, Transformers\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "def data_ingestion():\n",
        "    documents = SimpleDirectoryReader(DATA_DIR).load_data()\n",
        "    storage_context = StorageContext.from_defaults()\n",
        "    index = VectorStoreIndex.from_documents(documents)\n",
        "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
        "    print(\"PDF data ingested and vector store created/updated.\")\n",
        "\n",
        "def handle_query(query):\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
        "    index = load_index_from_storage(storage_context)\n",
        "    chat_text_qa_msgs = [\n",
        "    (\n",
        "        \"user\",\n",
        "        \"\"\"You are a Q&A assistant named ChatPDF. You have a specific response programmed for when users specifically ask about your creator, Suriya. The response is: \"I was created by Suriya, an enthusiast in Artificial Intelligence. He is dedicated to solving complex problems and delivering innovative solutions. With a strong focus on machine learning, deep learning, Python, generative AI, NLP, and computer vision, Suriya is passionate about pushing the boundaries of AI to explore new possibilities.\" For all other inquiries, your main goal is to provide answers as accurately as possible, based on the instructions and context you have been given. If a question does not match the provided context or is outside the scope of the document, kindly advise the user to ask questions within the context of the document.\n",
        "        Context:\n",
        "        {context_str}\n",
        "        Question:\n",
        "        {query_str}\n",
        "        \"\"\"\n",
        "    )\n",
        "    ]\n",
        "    text_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)\n",
        "\n",
        "    query_engine = index.as_query_engine(text_qa_template=text_qa_template)\n",
        "    answer = query_engine.query(query)\n",
        "\n",
        "    if hasattr(answer, 'response'):\n",
        "        return answer.response\n",
        "    elif isinstance(answer, dict) and 'response' in answer:\n",
        "        return answer['response']\n",
        "    else:\n",
        "        return \"Sorry, I couldn't find an answer.\"\n",
        "\n",
        "# Main function to run without UI\n",
        "def main():\n",
        "    pdf_file_path = \"/content/data/sample.pdf\" # Path to your PDF file\n",
        "    user_question = \"What is the main topic of this document?\" # Your question\n",
        "\n",
        "    # Check if PDF data directory is empty, or vector store doesn't exist, then ingest data\n",
        "    if not os.listdir(DATA_DIR) or not os.path.exists(PERSIST_DIR):\n",
        "        print(\"No PDF data or vector store found. Processing PDF...\")\n",
        "        data_ingestion()\n",
        "    else:\n",
        "        print(\"Vector store already exists. Loading existing store.\")\n",
        "\n",
        "    print(f\"Question: {user_question}\")\n",
        "    response = handle_query(user_question)\n",
        "    print(f\"Answer: {response}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "yht6dZ748gN9",
        "outputId": "69340852-09e3-4859-d74d-a867e28318bf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector store already exists. Loading existing store.\n",
            "Question: What is the main topic of this document?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/db/docstore.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-88c027b7e031>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-88c027b7e031>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Question: {user_question}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_question\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Answer: {response}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-88c027b7e031>\u001b[0m in \u001b[0;36mhandle_query\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhandle_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mstorage_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStorageContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersist_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPERSIST_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_index_from_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     chat_text_qa_msgs = [\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/storage/storage_context.py\u001b[0m in \u001b[0;36mfrom_defaults\u001b[0;34m(cls, docstore, index_store, vector_store, image_store, vector_stores, graph_store, property_graph_store, persist_dir, fs)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mvector_stores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIMAGE_VECTOR_STORE_NAMESPACE\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             docstore = docstore or SimpleDocumentStore.from_persist_dir(\n\u001b[0m\u001b[1;32m    112\u001b[0m                 \u001b[0mpersist_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/storage/docstore/simple_docstore.py\u001b[0m in \u001b[0;36mfrom_persist_dir\u001b[0;34m(cls, persist_dir, namespace, fs)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mpersist_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersist_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEFAULT_PERSIST_FNAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_persist_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersist_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/storage/docstore/simple_docstore.py\u001b[0m in \u001b[0;36mfrom_persist_path\u001b[0;34m(cls, persist_path, namespace, fs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \"\"\"\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0msimple_kvstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleKVStore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_persist_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersist_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple_kvstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/storage/kvstore/simple_kvstore.py\u001b[0m in \u001b[0;36mfrom_persist_path\u001b[0;34m(cls, persist_path, fs)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfsspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilesystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading {__name__} from {persist_path}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersist_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/spec.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0mac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"autocommit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_intrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m             f = self._open(\n\u001b[0m\u001b[1;32m   1302\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/implementations/local.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_mkdir\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mLocalFileOpener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtouch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/implementations/local.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_compression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocksize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_BUFFER_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/implementations/local.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocommit\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m                     \u001b[0mcompress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/db/docstore.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Settings.llm = HuggingFaceLLM(\n",
        "    model_name=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    tokenizer_name=\"meta-llama/Llama-3.2-1B-Instruct\", # You can explicitly set tokenizer_name, though often it's inferred from model_name\n",
        "    tokenizer_kwargs={\"torch_dtype\": torch.float16 if torch.cuda.is_available() else torch.float32}, # Tokenizer kwargs\n",
        "    model_kwargs={\"torch_dtype\": torch.float16 if torch.cuda.is_available() else torch.float32}, # Model kwargs - REMOVED device_map HERE\n",
        "    context_window=3000, # Keep other parameters if you need them, adjust as necessary\n",
        "    max_new_tokens=5,\n",
        "    generate_kwargs={\"temperature\": 0.1},\n",
        ")"
      ],
      "metadata": {
        "id": "H9KYb74t9teK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import base64\n",
        "from llama_index.core import StorageContext, load_index_from_storage, VectorStoreIndex, SimpleDirectoryReader, ChatPromptTemplate\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from transformers import pipeline\n",
        "from dotenv import load_dotenv\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "import torch # Import torch if you are using torch_dtype\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "# Load environment variables (optional, if you still need dotenv for other things)\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "Settings.llm = HuggingFaceLLM(\n",
        "    model_name=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    tokenizer_name=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    context_window=3000,\n",
        "    max_new_tokens=5,\n",
        "    generate_kwargs={\"temperature\": 0.1},\n",
        ")\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
        ")\n",
        "\n",
        "\n",
        "# Define the directory for persistent storage and data\n",
        "PERSIST_DIR = os.path.abspath(\"./db\") # Use absolute path\n",
        "DATA_DIR = \"data\" # Keep as relative for now, adjust if needed\n",
        "\n",
        "# Ensure data directory exists\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
        "\n",
        "# ... (descriptions dictionary - unchanged) ...\n",
        "\n",
        "def data_ingestion():\n",
        "    print(\"Starting data ingestion...\") # Debug print\n",
        "    try:\n",
        "        documents = SimpleDirectoryReader(DATA_DIR).load_data()\n",
        "        print(f\"Loaded {len(documents)} documents.\") # Debug print\n",
        "        storage_context = StorageContext.from_defaults()\n",
        "        index = VectorStoreIndex.from_documents(documents)\n",
        "        index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
        "        print(f\"Vector store persisted to: {PERSIST_DIR}\") # Debug print\n",
        "        print(\"PDF data ingested and vector store created/updated.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during data ingestion: {e}\") # Error handling\n",
        "        raise # Re-raise the exception for debugging\n",
        "\n",
        "\n",
        "def handle_query(query):\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
        "    print(f\"Loading vector store from: {PERSIST_DIR}\") # Debug print\n",
        "    index = load_index_from_storage(storage_context) # This is where FileNotFoundError occurs\n",
        "    # ... (rest of handle_query - unchanged) ...\n",
        "\n",
        "\n",
        "# Main function to run without UI\n",
        "def main():\n",
        "    pdf_file_path = \"/content/data/sample.pdf\" # Path to your PDF file\n",
        "    user_question = \"What is the main topic of this document?\" # Your question\n",
        "\n",
        "    # Force data ingestion for debugging\n",
        "    print(\"Forcing data ingestion for debugging...\") # Debug print\n",
        "    data_ingestion() # Always run data ingestion\n",
        "\n",
        "    print(\"Vector store should be created/updated now.\") # Debug print\n",
        "\n",
        "    print(f\"Question: {user_question}\")\n",
        "    response = handle_query(user_question)\n",
        "    print(f\"Answer: {response}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "QlaUeyT1_XTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WPXg4YSX_tS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S8VXG5G0_tQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cTrtF_KJ_tNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import base64\n",
        "from llama_index.core import StorageContext, load_index_from_storage, VectorStoreIndex, SimpleDirectoryReader, ChatPromptTemplate\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from transformers import pipeline\n",
        "from dotenv import load_dotenv\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "import torch  # Import torch if you are using torch_dtype\n",
        "\n",
        "# Load environment variables (optional, if you still need dotenv for other things)\n",
        "load_dotenv()\n",
        "\n",
        "# Configure the Llama index settings\n",
        "Settings.llm = HuggingFaceLLM(\n",
        "    model_name=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    tokenizer_name=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    context_window=3000,\n",
        "    max_new_tokens=5,  # Reduced max_new_tokens for testing, adjust as needed\n",
        "    generate_kwargs={\"temperature\": 0.1},\n",
        ")\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
        ")\n",
        "\n",
        "\n",
        "# Define the directory for persistent storage and data\n",
        "PERSIST_DIR = os.path.abspath(\"./db\")  # Use absolute path\n",
        "DATA_DIR = \"data\"  # Keep as relative for now, adjust if needed\n",
        "\n",
        "# Ensure data directory exists\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
        "\n",
        "# Language descriptions (not used in non-UI version, but kept for reference)\n",
        "descriptions = {\n",
        "    \"pl\": \"\"\"\n",
        "    # ChatPDF (Non-UI)\n",
        "    **ChatPDF** to zaawansowane narzędzie oparte na sztucznej inteligencji, zaprojektowane do analizy i generowania odpowiedzi na pytania związane z treścią załadowanych dokumentów PDF. Działa w trybie wiersza poleceń.\n",
        "    **Technologie**:\n",
        "    - Model: Llama-3.2-1B-Instruct (Lokalnie)\n",
        "    - Stworzony przez: Rafał Dembski\n",
        "    - Technologie: LlamaIndex, PyTorch, Transformers\n",
        "    \"\"\",\n",
        "    \"en\": \"\"\"\n",
        "    # ChatPDF (Non-UI)\n",
        "    **ChatPDF** is an advanced AI-powered tool designed to analyze and generate answers to questions related to the content of uploaded PDF documents. Runs in command-line mode.\n",
        "    **Technologies**:\n",
        "    - Model: Llama-3.2-1B-Instruct (Local)\n",
        "    - Developed by: Rafał Dembski\n",
        "    - Technologies: LlamaIndex, PyTorch, Transformers\n",
        "    \"\"\",\n",
        "    \"de\": \"\"\"\n",
        "    # ChatPDF (Non-UI)\n",
        "    **ChatPDF** ist ein fortschrittliches, KI-gesteuertes Tool, das entwickelt wurde, um Fragen zur Analyse und Beantwortung von Fragen im Zusammenhang mit dem Inhalt hochgeladener PDF-Dokumente zu generieren. Läuft im Kommandozeilenmodus.\n",
        "    **Technologien**:\n",
        "    - Modell: Llama-3.2-1B-Instruct (Lokal)\n",
        "    - Entwickelt von: Rafał Dembski\n",
        "    - Technologien: LlamaIndex, PyTorch, Transformers\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "def data_ingestion():\n",
        "    print(\"Starting data ingestion...\")  # Debug print\n",
        "    try:\n",
        "        print(f\"DATA_DIR is: {DATA_DIR}\")  # Debug print\n",
        "        print(f\"Contents of DATA_DIR: {os.listdir(DATA_DIR)}\")  # Debug print\n",
        "        pdf_file_path = os.path.abspath(os.path.join(DATA_DIR, \"sample.pdf\"))  # Assuming \"sample.pdf\" in DATA_DIR\n",
        "        print(f\"Attempting to load PDF from path: {pdf_file_path}\")  # Debug print\n",
        "        documents = SimpleDirectoryReader(input_files=[pdf_file_path]).load_data()  # Load specific file\n",
        "        print(f\"Loaded {len(documents)} documents.\")  # Debug print\n",
        "        storage_context = StorageContext.from_defaults()\n",
        "        index = VectorStoreIndex.from_documents(documents)\n",
        "        index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
        "        print(f\"Vector store persisted to: {PERSIST_DIR}\")  # Debug print\n",
        "        print(\"PDF data ingested and vector store created/updated.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during data ingestion: {e}\")  # Error handling\n",
        "        raise  # Re-raise the exception for debugging\n",
        "\n",
        "\n",
        "def handle_query(query):\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
        "    print(f\"Loading vector store from: {PERSIST_DIR}\")  # Debug print\n",
        "    index = load_index_from_storage(storage_context)  # This is where FileNotFoundError occurred\n",
        "    chat_text_qa_msgs = [\n",
        "        (\n",
        "            \"user\",\n",
        "            \"\"\"You are a Q&A assistant named ChatPDF. You have a specific response programmed for when users specifically ask about your creator, Suriya. The response is: \"I was created by Suriya, an enthusiast in Artificial Intelligence. He is dedicated to solving complex problems and delivering innovative solutions. With a strong focus on machine learning, deep learning, Python, generative AI, NLP, and computer vision, Suriya is passionate about pushing the boundaries of AI to explore new possibilities.\" For all other inquiries, your main goal is to provide answers as accurately as possible, based on the instructions and context you have been given. If a question does not match the provided context or is outside the scope of the document, kindly advise the user to ask questions within the context of the document.\n",
        "        Context:\n",
        "        {context_str}\n",
        "        Question:\n",
        "        {query_str}\n",
        "        \"\"\"\n",
        "        )\n",
        "    ]\n",
        "    text_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)\n",
        "\n",
        "    query_engine = index.as_query_engine(text_qa_template=text_qa_template)\n",
        "    answer = query_engine.query(query)\n",
        "\n",
        "    if hasattr(answer, 'response'):\n",
        "        return answer.response\n",
        "    elif isinstance(answer, dict) and 'response' in answer:\n",
        "        return answer['response']\n",
        "    else:\n",
        "        return \"Sorry, I couldn't find an answer.\"\n",
        "\n",
        "\n",
        "# Main function to run without UI\n",
        "def main():\n",
        "    pdf_file_path = \"/content/data/sample.pdf\"  # Path to your PDF file\n",
        "    user_question = \"What is the main topic of this document?\"  # Your question\n",
        "\n",
        "    # Force data ingestion for debugging - always run it\n",
        "    print(\"Forcing data ingestion for debugging...\")  # Debug print\n",
        "    data_ingestion()  # Always run data ingestion\n",
        "    print(\"Vector store should be created/updated now.\")  # Debug print\n",
        "\n",
        "    print(f\"Question: {user_question}\")\n",
        "    response = handle_query(user_question)\n",
        "    print(f\"Answer: {response}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iToNGta_tKa",
        "outputId": "d1fd4c98-6e73-43ec-f787-1b7c2c77fe6e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forcing data ingestion for debugging...\n",
            "Starting data ingestion...\n",
            "DATA_DIR is: data\n",
            "Contents of DATA_DIR: ['sample.pdf']\n",
            "Attempting to load PDF from path: /content/data/sample.pdf\n",
            "Loaded 6 documents.\n",
            "Vector store persisted to: /content/db\n",
            "PDF data ingested and vector store created/updated.\n",
            "Vector store should be created/updated now.\n",
            "Question: What is the main topic of this document?\n",
            "Loading vector store from: /content/db\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: The main topic of this\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import base64\n",
        "from llama_index.core import StorageContext, load_index_from_storage, VectorStoreIndex, SimpleDirectoryReader, ChatPromptTemplate\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from transformers import pipeline\n",
        "from dotenv import load_dotenv\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "import torch  # Import torch if you are using torch_dtype\n",
        "\n",
        "# Load environment variables (optional, if you still need dotenv for other things)\n",
        "load_dotenv()\n",
        "\n",
        "# Configure the Llama index settings\n",
        "Settings.llm = HuggingFaceLLM(\n",
        "    model_name=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    tokenizer_name=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    context_window=3000,\n",
        "    max_new_tokens=55,  # Reduced max_new_tokens for testing, adjust as needed\n",
        "    generate_kwargs={\"temperature\": 0.1},\n",
        ")\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
        ")\n",
        "\n",
        "\n",
        "# Define the directory for persistent storage and data\n",
        "PERSIST_DIR = os.path.abspath(\"./db\")  # Use absolute path\n",
        "DATA_DIR = \"data\"  # Keep as relative for now, adjust if needed\n",
        "\n",
        "# Ensure data directory exists\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
        "\n",
        "# Language descriptions (not used in non-UI version, but kept for reference)\n",
        "descriptions = {\n",
        "    \"pl\": \"\"\"\n",
        "    # ChatPDF (Non-UI)\n",
        "    **ChatPDF** to zaawansowane narzędzie oparte na sztucznej inteligencji, zaprojektowane do analizy i generowania odpowiedzi na pytania związane z treścią załadowanych dokumentów PDF. Działa w trybie wiersza poleceń.\n",
        "    **Technologie**:\n",
        "    - Model: Llama-3.2-1B-Instruct (Lokalnie)\n",
        "    - Stworzony przez: Rafał Dembski\n",
        "    - Technologie: LlamaIndex, PyTorch, Transformers\n",
        "    \"\"\",\n",
        "    \"en\": \"\"\"\n",
        "    # ChatPDF (Non-UI)\n",
        "    **ChatPDF** is an advanced AI-powered tool designed to analyze and generate answers to questions related to the content of uploaded PDF documents. Runs in command-line mode.\n",
        "    **Technologies**:\n",
        "    - Model: Llama-3.2-1B-Instruct (Local)\n",
        "    - Developed by: Rafał Dembski\n",
        "    - Technologies: LlamaIndex, PyTorch, Transformers\n",
        "    \"\"\",\n",
        "    \"de\": \"\"\"\n",
        "    # ChatPDF (Non-UI)\n",
        "    **ChatPDF** ist ein fortschrittliches, KI-gesteuertes Tool, das entwickelt wurde, um Fragen zur Analyse und Beantwortung von Fragen im Zusammenhang mit dem Inhalt hochgeladener PDF-Dokumente zu generieren. Läuft im Kommandozeilenmodus.\n",
        "    **Technologien**:\n",
        "    - Modell: Llama-3.2-1B-Instruct (Lokal)\n",
        "    - Entwickelt von: Rafał Dembski\n",
        "    - Technologien: LlamaIndex, PyTorch, Transformers\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "def data_ingestion():\n",
        "    print(\"Starting data ingestion...\")  # Debug print\n",
        "    try:\n",
        "        print(f\"DATA_DIR is: {DATA_DIR}\")  # Debug print\n",
        "        print(f\"Contents of DATA_DIR: {os.listdir(DATA_DIR)}\")  # Debug print\n",
        "        pdf_file_path = os.path.abspath(os.path.join(DATA_DIR, \"sample.pdf\"))  # Assuming \"sample.pdf\" in DATA_DIR\n",
        "        print(f\"Attempting to load PDF from path: {pdf_file_path}\")  # Debug print\n",
        "        documents = SimpleDirectoryReader(input_files=[pdf_file_path]).load_data()  # Load specific file\n",
        "        print(f\"Loaded {len(documents)} documents.\")  # Debug print\n",
        "        storage_context = StorageContext.from_defaults()\n",
        "        index = VectorStoreIndex.from_documents(documents)\n",
        "        index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
        "        print(f\"Vector store persisted to: {PERSIST_DIR}\")  # Debug print\n",
        "        print(\"PDF data ingested and vector store created/updated.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during data ingestion: {e}\")  # Error handling\n",
        "        raise  # Re-raise the exception for debugging\n",
        "\n",
        "\n",
        "def handle_query(query):\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
        "    print(f\"Loading vector store from: {PERSIST_DIR}\")  # Debug print\n",
        "    index = load_index_from_storage(storage_context)  # This is where FileNotFoundError occurred\n",
        "    chat_text_qa_msgs = [\n",
        "        (\n",
        "            \"user\",\n",
        "            \"\"\"You are a Q&A assistant named ChatPDF. You have a specific response programmed for when users specifically ask about your creator, Suriya. The response is: \"I was created by Suriya, an enthusiast in Artificial Intelligence. He is dedicated to solving complex problems and delivering innovative solutions. With a strong focus on machine learning, deep learning, Python, generative AI, NLP, and computer vision, Suriya is passionate about pushing the boundaries of AI to explore new possibilities.\" For all other inquiries, your main goal is to provide answers as accurately as possible, based on the instructions and context you have been given. If a question does not match the provided context or is outside the scope of the document, kindly advise the user to ask questions within the context of the document.\n",
        "        Context:\n",
        "        {context_str}\n",
        "        Question:\n",
        "        {query_str}\n",
        "        \"\"\"\n",
        "        )\n",
        "    ]\n",
        "    text_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)\n",
        "\n",
        "    query_engine = index.as_query_engine(text_qa_template=text_qa_template)\n",
        "    answer = query_engine.query(query)\n",
        "\n",
        "    if hasattr(answer, 'response'):\n",
        "        return answer.response\n",
        "    elif isinstance(answer, dict) and 'response' in answer:\n",
        "        return answer['response']\n",
        "    else:\n",
        "        return \"Sorry, I couldn't find an answer.\"\n",
        "\n",
        "\n",
        "# Main function to run without UI\n",
        "def main():\n",
        "    pdf_file_path = \"/content/data/sample.pdf\"  # Path to your PDF file\n",
        "    user_question = \"What is the main topic of this document?\"  # Your question\n",
        "\n",
        "    # Force data ingestion for debugging - always run it\n",
        "    print(\"Forcing data ingestion for debugging...\")  # Debug print\n",
        "    data_ingestion()  # Always run data ingestion\n",
        "    print(\"Vector store should be created/updated now.\")  # Debug print\n",
        "\n",
        "    print(f\"Question: {user_question}\")\n",
        "    response = handle_query(user_question)\n",
        "    print(f\"Answer: {response}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heqPPc_2AbUr",
        "outputId": "c5a6c3ae-0cb7-4f43-dc91-f8a96577239d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forcing data ingestion for debugging...\n",
            "Starting data ingestion...\n",
            "DATA_DIR is: data\n",
            "Contents of DATA_DIR: ['sample.pdf']\n",
            "Attempting to load PDF from path: /content/data/sample.pdf\n",
            "Loaded 6 documents.\n",
            "Vector store persisted to: /content/db\n",
            "PDF data ingested and vector store created/updated.\n",
            "Vector store should be created/updated now.\n",
            "Question: What is the main topic of this document?\n",
            "Loading vector store from: /content/db\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: The main topic of this document is the German economy and its challenges, specifically focusing on the need for reform, the impact of demographic aging, climate change, and digitalization on the economy.\n"
          ]
        }
      ]
    }
  ]
}