{
  "nbformat": 4,
  "nbformat_minor": 1,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a2a5d623f2d04cb98f108a22e260eb9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d90556caee14415aee840e76fd84e39",
              "IPY_MODEL_5983f55e352d46a58ee3291c7c44b0f3",
              "IPY_MODEL_4eb42efb6dee4ff99643760a4736268c"
            ],
            "layout": "IPY_MODEL_4ad31049643e48618063d122c8256249"
          }
        },
        "0d90556caee14415aee840e76fd84e39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15be2fd4915148e998788cbbb3736a76",
            "placeholder": "​",
            "style": "IPY_MODEL_8d9b0c9ad4c641c58ce6b1f4e752b855",
            "value": "modules.json: 100%"
          }
        },
        "5983f55e352d46a58ee3291c7c44b0f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21621144110b4cacab55a99c23c68fe7",
            "max": 229,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e09cb023b43741c7bbfadbe949623491",
            "value": 229
          }
        },
        "4eb42efb6dee4ff99643760a4736268c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c617300af8734248a9bd3252ea39ce12",
            "placeholder": "​",
            "style": "IPY_MODEL_07436878dae54f96b124dc8acef1d7d1",
            "value": " 229/229 [00:00&lt;00:00, 6.54kB/s]"
          }
        },
        "4ad31049643e48618063d122c8256249": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15be2fd4915148e998788cbbb3736a76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d9b0c9ad4c641c58ce6b1f4e752b855": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21621144110b4cacab55a99c23c68fe7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e09cb023b43741c7bbfadbe949623491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c617300af8734248a9bd3252ea39ce12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07436878dae54f96b124dc8acef1d7d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "187f9932a78844fe81afe040dfce24c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_56d511db88854c93a0d87ad09d90f0fc",
              "IPY_MODEL_833da7761cc4484bb47883e393ecc832",
              "IPY_MODEL_a5bd7973e2b14ef2908c8265aa32c79d"
            ],
            "layout": "IPY_MODEL_8e2a909c9c8e47c9acddac4e4ba02045"
          }
        },
        "56d511db88854c93a0d87ad09d90f0fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3451da3333bb4b7eb8432fb4b1f4e85a",
            "placeholder": "​",
            "style": "IPY_MODEL_94ad60880d4f422299c5dc74659d9626",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "833da7761cc4484bb47883e393ecc832": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7163798d089b49709c61610db28f1aac",
            "max": 122,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83143ff3d67846f0b125ff9d08900708",
            "value": 122
          }
        },
        "a5bd7973e2b14ef2908c8265aa32c79d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5690235b7b3c431b88314540d670246e",
            "placeholder": "​",
            "style": "IPY_MODEL_1594b4bfe0114c979cc80608dafeb6c5",
            "value": " 122/122 [00:00&lt;00:00, 2.06kB/s]"
          }
        },
        "8e2a909c9c8e47c9acddac4e4ba02045": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3451da3333bb4b7eb8432fb4b1f4e85a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94ad60880d4f422299c5dc74659d9626": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7163798d089b49709c61610db28f1aac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83143ff3d67846f0b125ff9d08900708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5690235b7b3c431b88314540d670246e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1594b4bfe0114c979cc80608dafeb6c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8764062ab16478f92fad69b212048c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ed4037da5384235a4f0dc6e3349254f",
              "IPY_MODEL_1f509632e4154474aa5239445ae5f683",
              "IPY_MODEL_a84a763e6e204464b515fc7a82b2f1c2"
            ],
            "layout": "IPY_MODEL_b2dc3bd6c03843b1bb819b418341a24e"
          }
        },
        "4ed4037da5384235a4f0dc6e3349254f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f95bb34ee45e47e1b898883a201127a4",
            "placeholder": "​",
            "style": "IPY_MODEL_999df7a52c4f48638198a19eb79cb821",
            "value": "README.md: 100%"
          }
        },
        "1f509632e4154474aa5239445ae5f683": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be5de5124efa4ca587de4fbc87939f3a",
            "max": 4044,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dc37e3d714bf47c19aebcaeb3ac1f146",
            "value": 4044
          }
        },
        "a84a763e6e204464b515fc7a82b2f1c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04619164b6dd45a8a41a42083ef7c98f",
            "placeholder": "​",
            "style": "IPY_MODEL_7581ae830f1b48b59c39b3bb279c3e47",
            "value": " 4.04k/4.04k [00:00&lt;00:00, 231kB/s]"
          }
        },
        "b2dc3bd6c03843b1bb819b418341a24e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f95bb34ee45e47e1b898883a201127a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "999df7a52c4f48638198a19eb79cb821": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be5de5124efa4ca587de4fbc87939f3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc37e3d714bf47c19aebcaeb3ac1f146": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "04619164b6dd45a8a41a42083ef7c98f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7581ae830f1b48b59c39b3bb279c3e47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "517a6c8278f54d6b93b5880377aedc85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3e9843afed1947e1af4736eeaf4bef78",
              "IPY_MODEL_3f3ca5a02a40486cb786e160247e2f80",
              "IPY_MODEL_56e46bee73494f45820a0437932d667b"
            ],
            "layout": "IPY_MODEL_f9dcbbd0a6f84772a527a548af8e9be2"
          }
        },
        "3e9843afed1947e1af4736eeaf4bef78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a0a3d989ac04104bd3a232b6428e698",
            "placeholder": "​",
            "style": "IPY_MODEL_d7b747bdeb2940548847b42d49180b70",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "3f3ca5a02a40486cb786e160247e2f80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5efaf426386431ab6d3538b6552b36c",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36b8bda492084777882cae239a69ca8a",
            "value": 53
          }
        },
        "56e46bee73494f45820a0437932d667b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5baa21f73ffa4714b53386546632e764",
            "placeholder": "​",
            "style": "IPY_MODEL_a620de02eafd4f3eb40dca6cb421e1e4",
            "value": " 53.0/53.0 [00:00&lt;00:00, 1.34kB/s]"
          }
        },
        "f9dcbbd0a6f84772a527a548af8e9be2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a0a3d989ac04104bd3a232b6428e698": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7b747bdeb2940548847b42d49180b70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5efaf426386431ab6d3538b6552b36c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36b8bda492084777882cae239a69ca8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5baa21f73ffa4714b53386546632e764": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a620de02eafd4f3eb40dca6cb421e1e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd75571210984274a9942c5efc069da0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22659eedccb84250b9aaa33fe6014515",
              "IPY_MODEL_2fc92ccc271246c5bc0e4dd932e2c541",
              "IPY_MODEL_ab4e4736fcc24ec1ac7ea2ed1bde840b"
            ],
            "layout": "IPY_MODEL_db292e82268a457db79fb67b32da3c87"
          }
        },
        "22659eedccb84250b9aaa33fe6014515": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5d5eddc89cc4f06a6ef236108116122",
            "placeholder": "​",
            "style": "IPY_MODEL_1811bc1d664f474ca2fc364042a0da1b",
            "value": "config.json: 100%"
          }
        },
        "2fc92ccc271246c5bc0e4dd932e2c541": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a24d2ded9e514754be142e2803576150",
            "max": 629,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e87da19aff4b4defabfc9c001983c72f",
            "value": 629
          }
        },
        "ab4e4736fcc24ec1ac7ea2ed1bde840b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_910e32b207734761a69fff42bf369c6b",
            "placeholder": "​",
            "style": "IPY_MODEL_28ff875304b04b3a87215a741873f1f7",
            "value": " 629/629 [00:00&lt;00:00, 5.45kB/s]"
          }
        },
        "db292e82268a457db79fb67b32da3c87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5d5eddc89cc4f06a6ef236108116122": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1811bc1d664f474ca2fc364042a0da1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a24d2ded9e514754be142e2803576150": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e87da19aff4b4defabfc9c001983c72f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "910e32b207734761a69fff42bf369c6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28ff875304b04b3a87215a741873f1f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1679d54dbdcf4c3e8de6ea470e4b4966": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_00771a8a12a04fab957399d0e5537632",
              "IPY_MODEL_67a02f45b755403794d60de9fef735fc",
              "IPY_MODEL_4a2c4a7b196645c5ba87f2f1646c08de"
            ],
            "layout": "IPY_MODEL_e41ee03713844fef9c85bb048ce20f47"
          }
        },
        "00771a8a12a04fab957399d0e5537632": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6055a662d3404b3f941492810e1e44b0",
            "placeholder": "​",
            "style": "IPY_MODEL_bc1b533a136d461ab14a3ab1cf1a707b",
            "value": "model.safetensors: 100%"
          }
        },
        "67a02f45b755403794d60de9fef735fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecf175c9de944d2ab3a17a6ce1976667",
            "max": 69569488,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b6a327c8fce0454a85a94acdfc73c214",
            "value": 69569488
          }
        },
        "4a2c4a7b196645c5ba87f2f1646c08de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68622489fec5415d8a8b0ef0f960cb46",
            "placeholder": "​",
            "style": "IPY_MODEL_b619a44515d043f987db689fb36fbdf2",
            "value": " 69.6M/69.6M [00:01&lt;00:00, 86.1MB/s]"
          }
        },
        "e41ee03713844fef9c85bb048ce20f47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6055a662d3404b3f941492810e1e44b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc1b533a136d461ab14a3ab1cf1a707b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ecf175c9de944d2ab3a17a6ce1976667": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6a327c8fce0454a85a94acdfc73c214": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "68622489fec5415d8a8b0ef0f960cb46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b619a44515d043f987db689fb36fbdf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75f7fa409f044955b7f91e1145913876": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_73f360df3b18499f81c20e227d7ebd82",
              "IPY_MODEL_c51c1096a9cc4280b3c39a9ce56204b0",
              "IPY_MODEL_1628676a86d848f99e850ef571b8c13a"
            ],
            "layout": "IPY_MODEL_5d1900b885d4401aabd115ed38b6f52a"
          }
        },
        "73f360df3b18499f81c20e227d7ebd82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a857faa554ec495f81b71f1312c8f7f0",
            "placeholder": "​",
            "style": "IPY_MODEL_f7dba94c62f04ce08d185a071bd612b3",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "c51c1096a9cc4280b3c39a9ce56204b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b860685a6c9482dbd25804ce68576d3",
            "max": 314,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23348cbdbaa64b06966b91b0921428fa",
            "value": 314
          }
        },
        "1628676a86d848f99e850ef571b8c13a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32a8be39628140c39d3a99208eb3abd7",
            "placeholder": "​",
            "style": "IPY_MODEL_fa9ce9f9eb6c464ab4b6841fbaeb150b",
            "value": " 314/314 [00:00&lt;00:00, 10.2kB/s]"
          }
        },
        "5d1900b885d4401aabd115ed38b6f52a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a857faa554ec495f81b71f1312c8f7f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7dba94c62f04ce08d185a071bd612b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b860685a6c9482dbd25804ce68576d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23348cbdbaa64b06966b91b0921428fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "32a8be39628140c39d3a99208eb3abd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa9ce9f9eb6c464ab4b6841fbaeb150b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "114cddc5b9fc4852857a4d2463ab42d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fe5605e339c640deb214c785a35f1b4c",
              "IPY_MODEL_6a2f0ffb4858468590625500a1549b1c",
              "IPY_MODEL_d322e4035cd34999b2043b26a4cfc3ba"
            ],
            "layout": "IPY_MODEL_adc87313a2f9496ea437beb6fefc640d"
          }
        },
        "fe5605e339c640deb214c785a35f1b4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb3f7462ba8a4b1e86b641991f63474a",
            "placeholder": "​",
            "style": "IPY_MODEL_bcebd3664da14d82bdb1edc9a2e6b369",
            "value": "vocab.txt: 100%"
          }
        },
        "6a2f0ffb4858468590625500a1549b1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5c6e75c210649899969f9eb91b553cc",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_db4a572155034d5bbd4a8da3ebbcd13f",
            "value": 231508
          }
        },
        "d322e4035cd34999b2043b26a4cfc3ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8d7f63cffa94ba2a291241ee7251d13",
            "placeholder": "​",
            "style": "IPY_MODEL_756ebb62dbcf4d76b47ca6a6ba1f09a1",
            "value": " 232k/232k [00:00&lt;00:00, 2.30MB/s]"
          }
        },
        "adc87313a2f9496ea437beb6fefc640d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb3f7462ba8a4b1e86b641991f63474a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcebd3664da14d82bdb1edc9a2e6b369": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5c6e75c210649899969f9eb91b553cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db4a572155034d5bbd4a8da3ebbcd13f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8d7f63cffa94ba2a291241ee7251d13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "756ebb62dbcf4d76b47ca6a6ba1f09a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f00a614e9904bfbb75dc80a718b6955": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9637b3c2e3fd49f491c73ca3e8636eec",
              "IPY_MODEL_fffc08fbc36340ef8190409d98830743",
              "IPY_MODEL_5a59ed39d5e4491da5b9d49cfd30626a"
            ],
            "layout": "IPY_MODEL_902327e36cd04cc4ade30dad2a4e6727"
          }
        },
        "9637b3c2e3fd49f491c73ca3e8636eec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7f708bd66ef49e28d01b5b805c0e06c",
            "placeholder": "​",
            "style": "IPY_MODEL_0cd1769c81cd47b78a5e38f259c7b307",
            "value": "tokenizer.json: 100%"
          }
        },
        "fffc08fbc36340ef8190409d98830743": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_caa5c2029f154296b786ab81df317375",
            "max": 466248,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e94e4f37d4924e78be7005a19124b6e3",
            "value": 466248
          }
        },
        "5a59ed39d5e4491da5b9d49cfd30626a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3aa67259a4674db1a870577a0533c582",
            "placeholder": "​",
            "style": "IPY_MODEL_d07633b8913647bbbe203ba9b0b1c8cb",
            "value": " 466k/466k [00:00&lt;00:00, 8.41MB/s]"
          }
        },
        "902327e36cd04cc4ade30dad2a4e6727": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7f708bd66ef49e28d01b5b805c0e06c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cd1769c81cd47b78a5e38f259c7b307": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "caa5c2029f154296b786ab81df317375": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e94e4f37d4924e78be7005a19124b6e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3aa67259a4674db1a870577a0533c582": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d07633b8913647bbbe203ba9b0b1c8cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe5d8ff24ab443ccae676474237e6e5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ed3e8a9b34144b6d876eceb249fb5138",
              "IPY_MODEL_ad1e44a6f4f44d69967498fd4ffd0028",
              "IPY_MODEL_660b7f01bfcc447c99e0575e6871c04b"
            ],
            "layout": "IPY_MODEL_bf6d8b30749c4bdea1007ac60806e24c"
          }
        },
        "ed3e8a9b34144b6d876eceb249fb5138": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c656200085284d799735404cbbdfa67d",
            "placeholder": "​",
            "style": "IPY_MODEL_75f776ccecf448e2ad3db6ad7bb05e80",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "ad1e44a6f4f44d69967498fd4ffd0028": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2345ca5d6ba44ae9a97f08009c22587",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_353c82591d6f4667b90fd4036808054a",
            "value": 112
          }
        },
        "660b7f01bfcc447c99e0575e6871c04b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db25cda9c60d4f1ca2ccaf3e49f55c33",
            "placeholder": "​",
            "style": "IPY_MODEL_803391198f1a41319e6af9c9e2aba54f",
            "value": " 112/112 [00:00&lt;00:00, 4.62kB/s]"
          }
        },
        "bf6d8b30749c4bdea1007ac60806e24c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c656200085284d799735404cbbdfa67d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75f776ccecf448e2ad3db6ad7bb05e80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2345ca5d6ba44ae9a97f08009c22587": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "353c82591d6f4667b90fd4036808054a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db25cda9c60d4f1ca2ccaf3e49f55c33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "803391198f1a41319e6af9c9e2aba54f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "225fd347d0c54319b5e970ba648dc534": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_993e0e6ab3fe4adca5006e6108f17616",
              "IPY_MODEL_7958c06bc3144c11a99875a7665a6197",
              "IPY_MODEL_377d9fdf49cb42218603a05f522e40cc"
            ],
            "layout": "IPY_MODEL_30477e8f22774dfcb95c21f161b718b8"
          }
        },
        "993e0e6ab3fe4adca5006e6108f17616": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c579d9d5296410f8c1bfbbfbf73103a",
            "placeholder": "​",
            "style": "IPY_MODEL_554f446a003f49b28ea476c9c6fe815a",
            "value": "1_Pooling%2Fconfig.json: 100%"
          }
        },
        "7958c06bc3144c11a99875a7665a6197": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c684913a79b4f4c83c646f7b5683698",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_339d1188e42b4d88b05f07fdf7611761",
            "value": 190
          }
        },
        "377d9fdf49cb42218603a05f522e40cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11c31f27f64a4a2f89bc357ff5fcb27a",
            "placeholder": "​",
            "style": "IPY_MODEL_1fc82cda351a42e38c1c2f460087ad25",
            "value": " 190/190 [00:00&lt;00:00, 3.42kB/s]"
          }
        },
        "30477e8f22774dfcb95c21f161b718b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c579d9d5296410f8c1bfbbfbf73103a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "554f446a003f49b28ea476c9c6fe815a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c684913a79b4f4c83c646f7b5683698": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "339d1188e42b4d88b05f07fdf7611761": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "11c31f27f64a4a2f89bc357ff5fcb27a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fc82cda351a42e38c1c2f460087ad25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6gCOZY82kqR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3zVF7MDR3CBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w235tglJ3CEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/spaces/rafaldembski/PDF-CHATBOT/blob/main/app.py\n"
      ],
      "metadata": {
        "id": "1WtI92j13S1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import streamlit as st\n",
        "import os\n",
        "import base64\n",
        "from llama_index.core import StorageContext, load_index_from_storage, VectorStoreIndex, SimpleDirectoryReader, ChatPromptTemplate\n",
        "from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
        "from dotenv import load_dotenv\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "from PIL import Image\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Configure the Llama index settings\n",
        "Settings.llm = HuggingFaceInferenceAPI(\n",
        "    model_name=\"google/gemma-1.1-7b-it\",\n",
        "    tokenizer_name=\"google/gemma-1.1-7b-it\",\n",
        "    context_window=3000,\n",
        "    token=os.getenv(\"HF_TOKEN\"),\n",
        "    max_new_tokens=512,\n",
        "    generate_kwargs={\"temperature\": 0.1},\n",
        ")\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
        ")\n",
        "\n",
        "# Define the directory for persistent storage and data\n",
        "PERSIST_DIR = \"./db\"\n",
        "DATA_DIR = \"data\"\n",
        "\n",
        "# Ensure data directory exists\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
        "\n",
        "# Language descriptions\n",
        "descriptions = {\n",
        "    \"pl\": \"\"\"\n",
        "    # ChatPDF\n",
        "    **ChatPDF** to zaawansowane narzędzie oparte na sztucznej inteligencji, zaprojektowane do analizy i generowania odpowiedzi na pytania związane z treścią załadowanych dokumentów PDF. Aplikacja umożliwia użytkownikom wprowadzanie zapytań dotyczących zawartości dokumentów i otrzymywanie precyzyjnych odpowiedzi w oparciu o zaawansowane algorytmy uczenia maszynowego.\n",
        "    **Jak korzystać z aplikacji**:\n",
        "    1. Wgraj plik PDF, korzystając z przycisku **Submit & Process**.\n",
        "    2. Poczekaj, aż plik PDF zostanie przetworzony.\n",
        "    3. Zadawaj pytania dotyczące zawartości pliku, określając język, w jakim ma być wygenerowana odpowiedź.\n",
        "    **Technologie**:\n",
        "    - Model: Gemma 1.1-7b-it\n",
        "    - Stworzony przez: Rafał Dembski\n",
        "    - Technologie: LlamaIndex, PyTorch, Streamlit\n",
        "    \"\"\",\n",
        "    \"en\": \"\"\"\n",
        "    # ChatPDF\n",
        "    **ChatPDF** is an advanced AI-powered tool designed to analyze and generate answers to questions related to the content of uploaded PDF documents. The application allows users to input queries about document contents and receive precise responses based on advanced machine learning algorithms.\n",
        "    **How to use the application**:\n",
        "    1. Upload a PDF file using the **Submit & Process** button.\n",
        "    2. Wait for the PDF file to be processed.\n",
        "    3. Ask questions about the content of the file, specifying the language in which you want the response to be generated.\n",
        "    **Technologies**:\n",
        "    - Model: Gemma 1.1-7b-it\n",
        "    - Developed by: Rafał Dembski\n",
        "    - Technologies: LlamaIndex, PyTorch, Streamlit\n",
        "    \"\"\",\n",
        "    \"de\": \"\"\"\n",
        "    # ChatPDF\n",
        "    **ChatPDF** ist ein fortschrittliches, KI-gesteuertes Tool, das entwickelt wurde, um Fragen zur Analyse und Beantwortung von Fragen im Zusammenhang mit dem Inhalt hochgeladener PDF-Dokumente zu generieren. Die Anwendung ermöglicht es Benutzern, Anfragen bezüglich des Dokumenteninhalts einzugeben und präzise Antworten basierend auf fortschrittlichen maschinellen Lernalgorithmen zu erhalten.\n",
        "    **So verwenden Sie die Anwendung**:\n",
        "    1. Laden Sie eine PDF-Datei über die Schaltfläche **Submit & Process** hoch.\n",
        "    2. Warten Sie, bis die PDF-Datei verarbeitet wurde.\n",
        "    3. Stellen Sie Fragen zum Inhalt der Datei und geben Sie an, in welcher Sprache die Antwort generiert werden soll.\n",
        "    **Technologien**:\n",
        "    - Modell: Gemma 1.1-7b-it\n",
        "    - Entwickelt von: Rafał Dembski\n",
        "    - Technologien: LlamaIndex, PyTorch, Streamlit\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "def displayPDF(file):\n",
        "    with open(file, \"rb\") as f:\n",
        "        base64_pdf = base64.b64encode(f.read()).decode('utf-8')\n",
        "    pdf_display = f'<iframe src=\"data:application/pdf;base64,{base64_pdf}\" width=\"100%\" height=\"600\" type=\"application/pdf\"></iframe>'\n",
        "    st.markdown(pdf_display, unsafe_allow_html=True)\n",
        "\n",
        "def data_ingestion():\n",
        "    documents = SimpleDirectoryReader(DATA_DIR).load_data()\n",
        "    storage_context = StorageContext.from_defaults()\n",
        "    index = VectorStoreIndex.from_documents(documents)\n",
        "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
        "\n",
        "def handle_query(query):\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
        "    index = load_index_from_storage(storage_context)\n",
        "    chat_text_qa_msgs = [\n",
        "    (\n",
        "        \"user\",\n",
        "        \"\"\"You are a Q&A assistant named ChatPDF. You have a specific response programmed for when users specifically ask about your creator, Suriya. The response is: \"I was created by Suriya, an enthusiast in Artificial Intelligence. He is dedicated to solving complex problems and delivering innovative solutions. With a strong focus on machine learning, deep learning, Python, generative AI, NLP, and computer vision, Suriya is passionate about pushing the boundaries of AI to explore new possibilities.\" For all other inquiries, your main goal is to provide answers as accurately as possible, based on the instructions and context you have been given. If a question does not match the provided context or is outside the scope of the document, kindly advise the user to ask questions within the context of the document.\n",
        "        Context:\n",
        "        {context_str}\n",
        "        Question:\n",
        "        {query_str}\n",
        "        \"\"\"\n",
        "    )\n",
        "    ]\n",
        "    text_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)\n",
        "\n",
        "    query_engine = index.as_query_engine(text_qa_template=text_qa_template)\n",
        "    answer = query_engine.query(query)\n",
        "\n",
        "    if hasattr(answer, 'response'):\n",
        "        return answer.response\n",
        "    elif isinstance(answer, dict) and 'response' in answer:\n",
        "        return answer['response']\n",
        "    else:\n",
        "        return \"Sorry, I couldn't find an answer.\"\n",
        "\n",
        "# Streamlit app initialization\n",
        "# Language selection\n",
        "selected_language = st.sidebar.selectbox(\"Wybierz język / Select Language / Sprache auswählen\", (\"pl\", \"en\", \"de\"))\n",
        "\n",
        "# Display description based on selected language\n",
        "st.markdown(descriptions[selected_language])\n",
        "\n",
        "if 'messages' not in st.session_state:\n",
        "    st.session_state.messages = [{'role': 'assistant', \"content\": 'Hello! Upload a PDF and ask me anything about its content.'}]\n",
        "\n",
        "with st.sidebar:\n",
        "    st.title(\"Menu:\")\n",
        "    uploaded_file = st.file_uploader(\"Upload your PDF Files and Click on the Submit & Process Button\")\n",
        "    if st.button(\"Submit & Process\"):\n",
        "        with st.spinner(\"Processing...\"):\n",
        "            filepath = \"data/saved_pdf.pdf\"\n",
        "            with open(filepath, \"wb\") as f:\n",
        "                f.write(uploaded_file.getbuffer())\n",
        "            data_ingestion()  # Process PDF every time new file is uploaded\n",
        "            st.success(\"Done\")\n",
        "\n",
        "user_prompt = st.chat_input(\"Ask me anything about the content of the PDF:\")\n",
        "if user_prompt:\n",
        "    st.session_state.messages.append({'role': 'user', \"content\": user_prompt})\n",
        "    response = handle_query(user_prompt)\n",
        "    st.session_state.messages.append({'role': 'assistant', \"content\": response})\n",
        "\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message['role']):\n",
        "        st.write(message['content'])\n"
      ],
      "metadata": {
        "id": "qct18eJj3CH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "streamlit\n",
        "python-dotenv\n",
        "llama-index\n",
        "llama-index-embeddings-huggingface\n",
        "llama-index-llms-huggingface"
      ],
      "metadata": {
        "id": "5SEarGX_3CKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import streamlit as st\n",
        "import os\n",
        "import base64\n",
        "from llama_index.core import StorageContext, load_index_from_storage, VectorStoreIndex, SimpleDirectoryReader, ChatPromptTemplate\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM # Changed import here\n",
        "from transformers import pipeline # Import pipeline from transformers\n",
        "from dotenv import load_dotenv\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "from PIL import Image\n",
        "\n",
        "# Load environment variables (optional, if you still need dotenv for other things)\n",
        "load_dotenv()\n",
        "\n",
        "# Configure the Llama index settings\n",
        "# Load the Gemma model using Hugging Face pipeline\n",
        "hf_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"google/gemma-1.1-7b-it\",\n",
        "    tokenizer=\"google/gemma-1.1-7b-it\",\n",
        "    torch_dtype=torch.float16, # Optional: Use float16 for less memory if you have CUDA and torch >= 2.0.1\n",
        "    device_map=\"auto\", # or \"cuda:0\" if you have specific GPU\n",
        ")\n",
        "\n",
        "Settings.llm = HuggingFaceLLM(\n",
        "    pipeline=hf_pipeline,\n",
        ")\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
        ")\n",
        "\n",
        "# Define the directory for persistent storage and data\n",
        "PERSIST_DIR = \"./db\"\n",
        "DATA_DIR = \"data\"\n",
        "\n",
        "# Ensure data directory exists\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
        "\n",
        "# Language descriptions\n",
        "descriptions = {\n",
        "    \"pl\": \"\"\"\n",
        "    # ChatPDF\n",
        "    **ChatPDF** to zaawansowane narzędzie oparte na sztucznej inteligencji, zaprojektowane do analizy i generowania odpowiedzi na pytania związane z treścią załadowanych dokumentów PDF. Aplikacja umożliwia użytkownikom wprowadzanie zapytań dotyczących zawartości dokumentów i otrzymywanie precyzyjnych odpowiedzi w oparciu o zaawansowane algorytmy uczenia maszynowego.\n",
        "    **Jak korzystać z aplikacji**:\n",
        "    1. Wgraj plik PDF, korzystając z przycisku **Submit & Process**.\n",
        "    2. Poczekaj, aż plik PDF zostanie przetworzony.\n",
        "    3. Zadawaj pytania dotyczące zawartości pliku, określając język, w jakim ma być wygenerowana odpowiedź.\n",
        "    **Technologie**:\n",
        "    - Model: Gemma 1.1-7b-it (Lokalnie)\n",
        "    - Stworzony przez: Rafał Dembski\n",
        "    - Technologie: LlamaIndex, PyTorch, Streamlit\n",
        "    \"\"\",\n",
        "    \"en\": \"\"\"\n",
        "    # ChatPDF\n",
        "    **ChatPDF** is an advanced AI-powered tool designed to analyze and generate answers to questions related to the content of uploaded PDF documents. The application allows users to input queries about document contents and receive precise responses based on advanced machine learning algorithms.\n",
        "    **How to use the application**:\n",
        "    1. Upload a PDF file using the **Submit & Process** button.\n",
        "    2. Wait for the PDF file to be processed.\n",
        "    3. Ask questions about the content of the file, specifying the language in which you want the response to be generated.\n",
        "    **Technologies**:\n",
        "    - Model: Gemma 1.1-7b-it (Local)\n",
        "    - Developed by: Rafał Dembski\n",
        "    - Technologies: LlamaIndex, PyTorch, Streamlit\n",
        "    \"\"\",\n",
        "    \"de\": \"\"\"\n",
        "    # ChatPDF\n",
        "    **ChatPDF** ist ein fortschrittliches, KI-gesteuertes Tool, das entwickelt wurde, um Fragen zur Analyse und Beantwortung von Fragen im Zusammenhang mit dem Inhalt hochgeladener PDF-Dokumente zu generieren. Die Anwendung ermöglicht es Benutzern, Anfragen bezüglich des Dokumenteninhalts einzugeben und präzise Antworten basierend auf fortschrittlichen maschinellen Lernalgorithmen zu erhalten.\n",
        "    **So verwenden Sie die Anwendung**:\n",
        "    1. Laden Sie eine PDF-Datei über die Schaltfläche **Submit & Process** hoch.\n",
        "    2. Warten Sie, bis die PDF-Datei verarbeitet wurde.\n",
        "    3. Stellen Sie Fragen zum Inhalt der Datei und geben Sie an, in welcher Sprache die Antwort generiert werden soll.\n",
        "    **Technologien**:\n",
        "    - Modell: Gemma 1.1-7b-it (Lokal)\n",
        "    - Entwickelt von: Rafał Dembski\n",
        "    - Technologien: LlamaIndex, PyTorch, Streamlit\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "def displayPDF(file):\n",
        "    with open(file, \"rb\") as f:\n",
        "        base64_pdf = base64.b64encode(f.read()).decode('utf-8')\n",
        "    pdf_display = f'<iframe src=\"data:application/pdf;base64,{base64_pdf}\" width=\"100%\" height=\"600\" type=\"application/pdf\"></iframe>'\n",
        "    st.markdown(pdf_display, unsafe_allow_html=True)\n",
        "\n",
        "def data_ingestion():\n",
        "    documents = SimpleDirectoryReader(DATA_DIR).load_data()\n",
        "    storage_context = StorageContext.from_defaults()\n",
        "    index = VectorStoreIndex.from_documents(documents)\n",
        "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
        "\n",
        "def handle_query(query):\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
        "    index = load_index_from_storage(storage_context)\n",
        "    chat_text_qa_msgs = [\n",
        "    (\n",
        "        \"user\",\n",
        "        \"\"\"You are a Q&A assistant named ChatPDF. You have a specific response programmed for when users specifically ask about your creator, Suriya. The response is: \"I was created by Suriya, an enthusiast in Artificial Intelligence. He is dedicated to solving complex problems and delivering innovative solutions. With a strong focus on machine learning, deep learning, Python, generative AI, NLP, and computer vision, Suriya is passionate about pushing the boundaries of AI to explore new possibilities.\" For all other inquiries, your main goal is to provide answers as accurately as possible, based on the instructions and context you have been given. If a question does not match the provided context or is outside the scope of the document, kindly advise the user to ask questions within the context of the document.\n",
        "        Context:\n",
        "        {context_str}\n",
        "        Question:\n",
        "        {query_str}\n",
        "        \"\"\"\n",
        "    )\n",
        "    ]\n",
        "    text_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)\n",
        "\n",
        "    query_engine = index.as_query_engine(text_qa_template=text_qa_template)\n",
        "    answer = query_engine.query(query)\n",
        "\n",
        "    if hasattr(answer, 'response'):\n",
        "        return answer.response\n",
        "    elif isinstance(answer, dict) and 'response' in answer:\n",
        "        return answer['response']\n",
        "    else:\n",
        "        return \"Sorry, I couldn't find an answer.\"\n",
        "\n",
        "# Streamlit app initialization\n",
        "# Language selection\n",
        "selected_language = st.sidebar.selectbox(\"Wybierz język / Select Language / Sprache auswählen\", (\"pl\", \"en\", \"de\"))\n",
        "\n",
        "# Display description based on selected language\n",
        "st.markdown(descriptions[selected_language])\n",
        "\n",
        "if 'messages' not in st.session_state:\n",
        "    st.session_state.messages = [{'role': 'assistant', \"content\": 'Hello! Upload a PDF and ask me anything about its content.'}]\n",
        "\n",
        "with st.sidebar:\n",
        "    st.title(\"Menu:\")\n",
        "    uploaded_file = st.file_uploader(\"Upload your PDF Files and Click on the Submit & Process Button\")\n",
        "    if st.button(\"Submit & Process\"):\n",
        "        with st.spinner(\"Processing...\"):\n",
        "            filepath = \"data/saved_pdf.pdf\"\n",
        "            with open(filepath, \"wb\") as f:\n",
        "                f.write(uploaded_file.getbuffer())\n",
        "            data_ingestion()  # Process PDF every time new file is uploaded\n",
        "            st.success(\"Done\")\n",
        "\n",
        "user_prompt = st.chat_input(\"Ask me anything about the content of the PDF:\")\n",
        "if user_prompt:\n",
        "    st.session_state.messages.append({'role': 'user', \"content\": user_prompt})\n",
        "    response = handle_query(user_prompt)\n",
        "    st.session_state.messages.append({'role': 'assistant', \"content\": response})\n",
        "\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message['role']):\n",
        "        st.write(message['content'])"
      ],
      "metadata": {
        "id": "ZlNRvQDn2-7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "streamlit\n",
        "python-dotenv\n",
        "llama-index\n",
        "llama-index-embeddings-huggingface\n",
        "llama-index-llms-huggingface\n",
        "numpy\n",
        "llama-index\n",
        "transformers\n",
        "python-dotenv\n",
        "torch"
      ],
      "metadata": {
        "id": "QyJZIHIn5dOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r a.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyHcBouO3w1A",
        "outputId": "101332a5-4547-4ce2-b5b3-4ee3aadff6e5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (from -r a.txt (line 1)) (1.42.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from -r a.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.11/dist-packages (from -r a.txt (line 3)) (0.12.22)\n",
            "Requirement already satisfied: llama-index-embeddings-huggingface in /usr/local/lib/python3.11/dist-packages (from -r a.txt (line 4)) (0.5.2)\n",
            "Requirement already satisfied: llama-index-llms-huggingface in /usr/local/lib/python3.11/dist-packages (from -r a.txt (line 5)) (0.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r a.txt (line 6)) (1.26.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from -r a.txt (line 8)) (4.48.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from -r a.txt (line 10)) (2.5.1+cu124)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (8.1.8)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (4.25.6)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r a.txt (line 1)) (6.4.2)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->-r a.txt (line 3)) (0.4.6)\n",
            "Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from llama-index->-r a.txt (line 3)) (0.4.1)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.22 in /usr/local/lib/python3.11/dist-packages (from llama-index->-r a.txt (line 3)) (0.12.22)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->-r a.txt (line 3)) (0.3.1)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->-r a.txt (line 3)) (0.6.8)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->-r a.txt (line 3)) (0.3.25)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->-r a.txt (line 3)) (0.4.3)\n",
            "Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->-r a.txt (line 3)) (0.3.1)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->-r a.txt (line 3)) (0.3.0)\n",
            "Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->-r a.txt (line 3)) (0.4.5)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index->-r a.txt (line 3)) (0.4.0)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index->-r a.txt (line 3)) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r a.txt (line 4)) (0.28.1)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-huggingface->-r a.txt (line 4)) (3.4.1)\n",
            "Requirement already satisfied: text-generation<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-huggingface->-r a.txt (line 5)) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers->-r a.txt (line 8)) (3.17.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers->-r a.txt (line 8)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->-r a.txt (line 8)) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->-r a.txt (line 8)) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers->-r a.txt (line 8)) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers->-r a.txt (line 8)) (4.67.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->-r a.txt (line 10)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->-r a.txt (line 10)) (1.3.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit->-r a.txt (line 1)) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit->-r a.txt (line 1)) (1.28.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r a.txt (line 1)) (4.0.12)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r a.txt (line 4)) (3.11.13)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index->-r a.txt (line 3)) (1.61.1)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (2.0.38)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (1.6.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (2.10.6)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (0.9.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (1.17.2)\n",
            "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.13 in /usr/local/lib/python3.11/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index->-r a.txt (line 3)) (0.1.13)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index->-r a.txt (line 3)) (4.13.3)\n",
            "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index->-r a.txt (line 3)) (5.3.1)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index->-r a.txt (line 3)) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r a.txt (line 3)) (0.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index->-r a.txt (line 3)) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit->-r a.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit->-r a.txt (line 1)) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit->-r a.txt (line 1)) (2025.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->-r a.txt (line 10)) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit->-r a.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit->-r a.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit->-r a.txt (line 1)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit->-r a.txt (line 1)) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit->-r a.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit->-r a.txt (line 1)) (2.18.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface->-r a.txt (line 4)) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface->-r a.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface->-r a.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface->-r a.txt (line 5)) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r a.txt (line 4)) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r a.txt (line 4)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r a.txt (line 4)) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r a.txt (line 4)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r a.txt (line 4)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r a.txt (line 4)) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r a.txt (line 4)) (1.18.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index->-r a.txt (line 3)) (2.6)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r a.txt (line 1)) (5.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r a.txt (line 1)) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r a.txt (line 1)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r a.txt (line 1)) (0.23.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (0.14.0)\n",
            "Requirement already satisfied: llama-cloud-services>=0.6.2 in /usr/local/lib/python3.11/dist-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r a.txt (line 3)) (0.6.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->-r a.txt (line 1)) (0.1.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index->-r a.txt (line 3)) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index->-r a.txt (line 3)) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index->-r a.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit->-r a.txt (line 1)) (1.17.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.22->llama-index->-r a.txt (line 3)) (3.26.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface->-r a.txt (line 4)) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token ْْْْْْْْْْXXXXXXX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttBqGadC6OvL",
        "outputId": "08074709-7fba-4292-9078-9d145cb40141"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `read` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `read`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "google/gemma-1.1-7b-it"
      ],
      "metadata": {
        "id": "-uci-udW5uHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import base64\n",
        "from llama_index.core import StorageContext, load_index_from_storage, VectorStoreIndex, SimpleDirectoryReader, ChatPromptTemplate\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from transformers import pipeline\n",
        "from dotenv import load_dotenv\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "import torch # Import torch if you are using torch_dtype\n",
        "\n",
        "# Load environment variables (optional, if you still need dotenv for other things)\n",
        "load_dotenv()\n",
        "\n",
        "# Configure the Llama index settings\n",
        "# Load the Gemma model using Hugging Face pipeline\n",
        "hf_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    tokenizer=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32, # Use float16 if CUDA is available, otherwise float32\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else \"cpu\", # Use GPU if available, otherwise CPU\n",
        ")\n",
        "\n",
        "Settings.llm = HuggingFaceLLM(\n",
        "    pipeline=hf_pipeline,\n",
        ")\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
        ")\n",
        "\n",
        "# Define the directory for persistent storage and data\n",
        "PERSIST_DIR = \"./db\"\n",
        "DATA_DIR = \"data\"\n",
        "\n",
        "# Ensure data directory exists\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
        "\n",
        "# Language descriptions (not used in non-UI version, but kept for reference)\n",
        "descriptions = {\n",
        "    \"pl\": \"\"\"\n",
        "    # ChatPDF (Non-UI)\n",
        "    **ChatPDF** to zaawansowane narzędzie oparte na sztucznej inteligencji, zaprojektowane do analizy i generowania odpowiedzi na pytania związane z treścią załadowanych dokumentów PDF. Działa w trybie wiersza poleceń.\n",
        "    **Technologie**:\n",
        "    - Model: Gemma 1.1-7b-it (Lokalnie)\n",
        "    - Stworzony przez: Rafał Dembski\n",
        "    - Technologie: LlamaIndex, PyTorch, Transformers\n",
        "    \"\"\",\n",
        "    \"en\": \"\"\"\n",
        "    # ChatPDF (Non-UI)\n",
        "    **ChatPDF** is an advanced AI-powered tool designed to analyze and generate answers to questions related to the content of uploaded PDF documents. Runs in command-line mode.\n",
        "    **Technologies**:\n",
        "    - Model: Gemma 1.1-7b-it (Local)\n",
        "    - Developed by: Rafał Dembski\n",
        "    - Technologies: LlamaIndex, PyTorch, Transformers\n",
        "    \"\"\",\n",
        "    \"de\": \"\"\"\n",
        "    # ChatPDF (Non-UI)\n",
        "    **ChatPDF** ist ein fortschrittliches, KI-gesteuertes Tool, das entwickelt wurde, um Fragen zur Analyse und Beantwortung von Fragen im Zusammenhang mit dem Inhalt hochgeladener PDF-Dokumente zu generieren. Läuft im Kommandozeilenmodus.\n",
        "    **Technologien**:\n",
        "    - Modell: Gemma 1.1-7b-it (Lokal)\n",
        "    - Entwickelt von: Rafał Dembski\n",
        "    - Technologien: LlamaIndex, PyTorch, Transformers\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "def data_ingestion():\n",
        "    documents = SimpleDirectoryReader(DATA_DIR).load_data()\n",
        "    storage_context = StorageContext.from_defaults()\n",
        "    index = VectorStoreIndex.from_documents(documents)\n",
        "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
        "    print(\"PDF data ingested and vector store created/updated.\")\n",
        "\n",
        "def handle_query(query):\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
        "    index = load_index_from_storage(storage_context)\n",
        "    chat_text_qa_msgs = [\n",
        "    (\n",
        "        \"user\",\n",
        "        \"\"\"You are a Q&A assistant named ChatPDF. You have a specific response programmed for when users specifically ask about your creator, Suriya. The response is: \"I was created by Suriya, an enthusiast in Artificial Intelligence. He is dedicated to solving complex problems and delivering innovative solutions. With a strong focus on machine learning, deep learning, Python, generative AI, NLP, and computer vision, Suriya is passionate about pushing the boundaries of AI to explore new possibilities.\" For all other inquiries, your main goal is to provide answers as accurately as possible, based on the instructions and context you have been given. If a question does not match the provided context or is outside the scope of the document, kindly advise the user to ask questions within the context of the document.\n",
        "        Context:\n",
        "        {context_str}\n",
        "        Question:\n",
        "        {query_str}\n",
        "        \"\"\"\n",
        "    )\n",
        "    ]\n",
        "    text_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)\n",
        "\n",
        "    query_engine = index.as_query_engine(text_qa_template=text_qa_template)\n",
        "    answer = query_engine.query(query)\n",
        "\n",
        "    if hasattr(answer, 'response'):\n",
        "        return answer.response\n",
        "    elif isinstance(answer, dict) and 'response' in answer:\n",
        "        return answer['response']\n",
        "    else:\n",
        "        return \"Sorry, I couldn't find an answer.\"\n",
        "\n",
        "# Main function to run without UI\n",
        "def main():\n",
        "    pdf_file_path = \"/content/sample.pdf\" # Path to your PDF file\n",
        "    user_question = \"What is the main topic of this document?\" # Your question\n",
        "\n",
        "    # Check if PDF data directory is empty, or vector store doesn't exist, then ingest data\n",
        "    if not os.listdir(DATA_DIR) or not os.path.exists(PERSIST_DIR):\n",
        "        print(\"No PDF data or vector store found. Processing PDF...\")\n",
        "        data_ingestion()\n",
        "    else:\n",
        "        print(\"Vector store already exists. Loading existing store.\")\n",
        "\n",
        "    print(f\"Question: {user_question}\")\n",
        "    response = handle_query(user_question)\n",
        "    print(f\"Answer: {response}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "kXcpcwYm3Lef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "import torch\n",
        "from transformers import pipeline # Make sure to keep this import\n",
        "\n",
        "# ... other imports and code ...\n",
        "\n",
        "# Configure the Llama index settings\n",
        "# Initialize HuggingFaceLLM directly with model_name\n",
        "Settings.llm = HuggingFaceLLM(\n",
        "    model_name=\"google/gemma-1.1-7b-it\",\n",
        "    tokenizer_name=\"google/gemma-1.1-7b-it\", # You can explicitly set tokenizer_name, though often it's inferred from model_name\n",
        "    tokenizer_kwargs={\"torch_dtype\": torch.float16 if torch.cuda.is_available() else torch.float32}, # Tokenizer kwargs\n",
        "    model_kwargs={\"torch_dtype\": torch.float16 if torch.cuda.is_available() else torch.float32, \"device_map\": \"auto\" if torch.cuda.is_available() else \"cpu\"}, # Model kwargs\n",
        "    context_window=3000, # Keep other parameters if you need them, adjust as necessary\n",
        "    max_new_tokens=512,\n",
        "    generate_kwargs={\"temperature\": 0.1},\n",
        ")\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
        ")\n",
        "\n",
        "# ... rest of your code ...\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import base64\n",
        "from llama_index.core import StorageContext, load_index_from_storage, VectorStoreIndex, SimpleDirectoryReader, ChatPromptTemplate\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from transformers import pipeline\n",
        "from dotenv import load_dotenv\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "import torch # Import torch if you are using torch_dtype\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "# Load environment variables (optional, if you still need dotenv for other things)\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "Settings.llm = HuggingFaceLLM(\n",
        "    model_name=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    tokenizer_name=\"meta-llama/Llama-3.2-1B-Instruct\", # You can explicitly set tokenizer_name, though often it's inferred from model_name\n",
        "    tokenizer_kwargs={\"torch_dtype\": torch.float16 if torch.cuda.is_available() else torch.float32}, # Tokenizer kwargs\n",
        "    model_kwargs={\"torch_dtype\": torch.float16 if torch.cuda.is_available() else torch.float32}, # Model kwargs - REMOVED device_map HERE\n",
        "    context_window=3000, # Keep other parameters if you need them, adjust as necessary\n",
        "    max_new_tokens=5,\n",
        "    generate_kwargs={\"temperature\": 0.1},\n",
        ")\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the directory for persistent storage and data\n",
        "PERSIST_DIR = \"./db\"\n",
        "DATA_DIR = \"data\"\n",
        "\n",
        "# Ensure data directory exists\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
        "\n",
        "# Language descriptions (not used in non-UI version, but kept for reference)\n",
        "descriptions = {\n",
        "    \"pl\": \"\"\"\n",
        "    # ChatPDF (Non-UI)\n",
        "    **ChatPDF** to zaawansowane narzędzie oparte na sztucznej inteligencji, zaprojektowane do analizy i generowania odpowiedzi na pytania związane z treścią załadowanych dokumentów PDF. Działa w trybie wiersza poleceń.\n",
        "    **Technologie**:\n",
        "    - Model: Gemma 1.1-7b-it (Lokalnie)\n",
        "    - Stworzony przez: Rafał Dembski\n",
        "    - Technologie: LlamaIndex, PyTorch, Transformers\n",
        "    \"\"\",\n",
        "    \"en\": \"\"\"\n",
        "    # ChatPDF (Non-UI)\n",
        "    **ChatPDF** is an advanced AI-powered tool designed to analyze and generate answers to questions related to the content of uploaded PDF documents. Runs in command-line mode.\n",
        "    **Technologies**:\n",
        "    - Model: Gemma 1.1-7b-it (Local)\n",
        "    - Developed by: Rafał Dembski\n",
        "    - Technologies: LlamaIndex, PyTorch, Transformers\n",
        "    \"\"\",\n",
        "    \"de\": \"\"\"\n",
        "    # ChatPDF (Non-UI)\n",
        "    **ChatPDF** ist ein fortschrittliches, KI-gesteuertes Tool, das entwickelt wurde, um Fragen zur Analyse und Beantwortung von Fragen im Zusammenhang mit dem Inhalt hochgeladener PDF-Dokumente zu generieren. Läuft im Kommandozeilenmodus.\n",
        "    **Technologien**:\n",
        "    - Modell: Gemma 1.1-7b-it (Lokal)\n",
        "    - Entwickelt von: Rafał Dembski\n",
        "    - Technologien: LlamaIndex, PyTorch, Transformers\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "def data_ingestion():\n",
        "    documents = SimpleDirectoryReader(DATA_DIR).load_data()\n",
        "    storage_context = StorageContext.from_defaults()\n",
        "    index = VectorStoreIndex.from_documents(documents)\n",
        "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
        "    print(\"PDF data ingested and vector store created/updated.\")\n",
        "\n",
        "def handle_query(query):\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
        "    index = load_index_from_storage(storage_context)\n",
        "    chat_text_qa_msgs = [\n",
        "    (\n",
        "        \"user\",\n",
        "        \"\"\"You are a Q&A assistant named ChatPDF. You have a specific response programmed for when users specifically ask about your creator, Suriya. The response is: \"I was created by Suriya, an enthusiast in Artificial Intelligence. He is dedicated to solving complex problems and delivering innovative solutions. With a strong focus on machine learning, deep learning, Python, generative AI, NLP, and computer vision, Suriya is passionate about pushing the boundaries of AI to explore new possibilities.\" For all other inquiries, your main goal is to provide answers as accurately as possible, based on the instructions and context you have been given. If a question does not match the provided context or is outside the scope of the document, kindly advise the user to ask questions within the context of the document.\n",
        "        Context:\n",
        "        {context_str}\n",
        "        Question:\n",
        "        {query_str}\n",
        "        \"\"\"\n",
        "    )\n",
        "    ]\n",
        "    text_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)\n",
        "\n",
        "    query_engine = index.as_query_engine(text_qa_template=text_qa_template)\n",
        "    answer = query_engine.query(query)\n",
        "\n",
        "    if hasattr(answer, 'response'):\n",
        "        return answer.response\n",
        "    elif isinstance(answer, dict) and 'response' in answer:\n",
        "        return answer['response']\n",
        "    else:\n",
        "        return \"Sorry, I couldn't find an answer.\"\n",
        "\n",
        "# Main function to run without UI\n",
        "def main():\n",
        "    pdf_file_path = \"/content/sample.pdf\" # Path to your PDF file\n",
        "    user_question = \"What is the main topic of this document?\" # Your question\n",
        "\n",
        "    # Check if PDF data directory is empty, or vector store doesn't exist, then ingest data\n",
        "    if not os.listdir(DATA_DIR) or not os.path.exists(PERSIST_DIR):\n",
        "        print(\"No PDF data or vector store found. Processing PDF...\")\n",
        "        data_ingestion()\n",
        "    else:\n",
        "        print(\"Vector store already exists. Loading existing store.\")\n",
        "\n",
        "    print(f\"Question: {user_question}\")\n",
        "    response = handle_query(user_question)\n",
        "    print(f\"Answer: {response}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "scsIzEAh6xs5",
        "outputId": "a7136d82-fd9f-4d83-a8ee-b2a0bec93d96"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained() got multiple values for keyword argument 'device_map'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-137246e34bab>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Configure the Llama index settings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Initialize HuggingFaceLLM directly with model_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m Settings.llm = HuggingFaceLLM(\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"google/gemma-1.1-7b-it\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtokenizer_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"google/gemma-1.1-7b-it\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# You can explicitly set tokenizer_name, though often it's inferred from model_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/llms/huggingface/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, context_window, max_new_tokens, query_wrapper_prompt, tokenizer_name, model_name, model, tokenizer, device_map, stopping_ids, tokenizer_kwargs, tokenizer_outputs_to_remove, model_kwargs, generate_kwargs, is_chat_model, callback_manager, system_prompt, messages_to_prompt, completion_to_prompt, pydantic_program_mode, output_parser)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;34m\"\"\"Initialize params.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mmodel_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         model = model or AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m    237\u001b[0m             \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         )\n",
            "\u001b[0;31mTypeError\u001b[0m: transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained() got multiple values for keyword argument 'device_map'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Settings.llm = HuggingFaceLLM(\n",
        "    model_name=\"google/gemma-1.1-7b-it\",\n",
        "    tokenizer_name=\"google/gemma-1.1-7b-it\", # You can explicitly set tokenizer_name, though often it's inferred from model_name\n",
        "    tokenizer_kwargs={\"torch_dtype\": torch.float16 if torch.cuda.is_available() else torch.float32}, # Tokenizer kwargs\n",
        "    model_kwargs={\"torch_dtype\": torch.float16 if torch.cuda.is_available() else torch.float32}, # Model kwargs - REMOVED device_map HERE\n",
        "    context_window=3000, # Keep other parameters if you need them, adjust as necessary\n",
        "    max_new_tokens=512,\n",
        "    generate_kwargs={\"temperature\": 0.1},\n",
        ")"
      ],
      "metadata": {
        "id": "7cQlUHYj76XU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import base64\n",
        "from llama_index.core import StorageContext, load_index_from_storage, VectorStoreIndex, SimpleDirectoryReader, ChatPromptTemplate\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from transformers import pipeline\n",
        "from dotenv import load_dotenv\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "import torch # Import torch if you are using torch_dtype\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "# Load environment variables (optional, if you still need dotenv for other things)\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "Settings.llm = HuggingFaceLLM(\n",
        "    model_name=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    tokenizer_name=\"meta-llama/Llama-3.2-1B-Instruct\", # You can explicitly set tokenizer_name, though often it's inferred from model_name\n",
        "    tokenizer_kwargs={\"torch_dtype\": torch.float16 if torch.cuda.is_available() else torch.float32}, # Tokenizer kwargs\n",
        "    model_kwargs={\"torch_dtype\": torch.float16 if torch.cuda.is_available() else torch.float32}, # Model kwargs - REMOVED device_map HERE\n",
        "    context_window=3000, # Keep other parameters if you need them, adjust as necessary\n",
        "    max_new_tokens=5,\n",
        "    generate_kwargs={\"temperature\": 0.1},\n",
        ")\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the directory for persistent storage and data\n",
        "PERSIST_DIR = \"./db\"\n",
        "DATA_DIR = \"data\"\n",
        "\n",
        "# Ensure data directory exists\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
        "\n",
        "# Language descriptions (not used in non-UI version, but kept for reference)\n",
        "descriptions = {\n",
        "    \"pl\": \"\"\"\n",
        "    # ChatPDF (Non-UI)\n",
        "    **ChatPDF** to zaawansowane narzędzie oparte na sztucznej inteligencji, zaprojektowane do analizy i generowania odpowiedzi na pytania związane z treścią załadowanych dokumentów PDF. Działa w trybie wiersza poleceń.\n",
        "    **Technologie**:\n",
        "    - Model: Gemma 1.1-7b-it (Lokalnie)\n",
        "    - Stworzony przez: Rafał Dembski\n",
        "    - Technologie: LlamaIndex, PyTorch, Transformers\n",
        "    \"\"\",\n",
        "    \"en\": \"\"\"\n",
        "    # ChatPDF (Non-UI)\n",
        "    **ChatPDF** is an advanced AI-powered tool designed to analyze and generate answers to questions related to the content of uploaded PDF documents. Runs in command-line mode.\n",
        "    **Technologies**:\n",
        "    - Model: Gemma 1.1-7b-it (Local)\n",
        "    - Developed by: Rafał Dembski\n",
        "    - Technologies: LlamaIndex, PyTorch, Transformers\n",
        "    \"\"\",\n",
        "    \"de\": \"\"\"\n",
        "    # ChatPDF (Non-UI)\n",
        "    **ChatPDF** ist ein fortschrittliches, KI-gesteuertes Tool, das entwickelt wurde, um Fragen zur Analyse und Beantwortung von Fragen im Zusammenhang mit dem Inhalt hochgeladener PDF-Dokumente zu generieren. Läuft im Kommandozeilenmodus.\n",
        "    **Technologien**:\n",
        "    - Modell: Gemma 1.1-7b-it (Lokal)\n",
        "    - Entwickelt von: Rafał Dembski\n",
        "    - Technologien: LlamaIndex, PyTorch, Transformers\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "def data_ingestion():\n",
        "    documents = SimpleDirectoryReader(DATA_DIR).load_data()\n",
        "    storage_context = StorageContext.from_defaults()\n",
        "    index = VectorStoreIndex.from_documents(documents)\n",
        "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
        "    print(\"PDF data ingested and vector store created/updated.\")\n",
        "\n",
        "def handle_query(query):\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
        "    index = load_index_from_storage(storage_context)\n",
        "    chat_text_qa_msgs = [\n",
        "    (\n",
        "        \"user\",\n",
        "        \"\"\"You are a Q&A assistant named ChatPDF. You have a specific response programmed for when users specifically ask about your creator, Suriya. The response is: \"I was created by Suriya, an enthusiast in Artificial Intelligence. He is dedicated to solving complex problems and delivering innovative solutions. With a strong focus on machine learning, deep learning, Python, generative AI, NLP, and computer vision, Suriya is passionate about pushing the boundaries of AI to explore new possibilities.\" For all other inquiries, your main goal is to provide answers as accurately as possible, based on the instructions and context you have been given. If a question does not match the provided context or is outside the scope of the document, kindly advise the user to ask questions within the context of the document.\n",
        "        Context:\n",
        "        {context_str}\n",
        "        Question:\n",
        "        {query_str}\n",
        "        \"\"\"\n",
        "    )\n",
        "    ]\n",
        "    text_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)\n",
        "\n",
        "    query_engine = index.as_query_engine(text_qa_template=text_qa_template)\n",
        "    answer = query_engine.query(query)\n",
        "\n",
        "    if hasattr(answer, 'response'):\n",
        "        return answer.response\n",
        "    elif isinstance(answer, dict) and 'response' in answer:\n",
        "        return answer['response']\n",
        "    else:\n",
        "        return \"Sorry, I couldn't find an answer.\"\n",
        "\n",
        "# Main function to run without UI\n",
        "def main():\n",
        "    pdf_file_path = \"/content/data/sample.pdf\" # Path to your PDF file\n",
        "    user_question = \"What is the main topic of this document?\" # Your question\n",
        "\n",
        "    # Check if PDF data directory is empty, or vector store doesn't exist, then ingest data\n",
        "    if not os.listdir(DATA_DIR) or not os.path.exists(PERSIST_DIR):\n",
        "        print(\"No PDF data or vector store found. Processing PDF...\")\n",
        "        data_ingestion()\n",
        "    else:\n",
        "        print(\"Vector store already exists. Loading existing store.\")\n",
        "\n",
        "    print(f\"Question: {user_question}\")\n",
        "    response = handle_query(user_question)\n",
        "    print(f\"Answer: {response}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "yht6dZ748gN9",
        "outputId": "69340852-09e3-4859-d74d-a867e28318bf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector store already exists. Loading existing store.\n",
            "Question: What is the main topic of this document?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/db/docstore.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-88c027b7e031>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-88c027b7e031>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Question: {user_question}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_question\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Answer: {response}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-88c027b7e031>\u001b[0m in \u001b[0;36mhandle_query\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhandle_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mstorage_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStorageContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersist_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPERSIST_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_index_from_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     chat_text_qa_msgs = [\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/storage/storage_context.py\u001b[0m in \u001b[0;36mfrom_defaults\u001b[0;34m(cls, docstore, index_store, vector_store, image_store, vector_stores, graph_store, property_graph_store, persist_dir, fs)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mvector_stores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIMAGE_VECTOR_STORE_NAMESPACE\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             docstore = docstore or SimpleDocumentStore.from_persist_dir(\n\u001b[0m\u001b[1;32m    112\u001b[0m                 \u001b[0mpersist_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/storage/docstore/simple_docstore.py\u001b[0m in \u001b[0;36mfrom_persist_dir\u001b[0;34m(cls, persist_dir, namespace, fs)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mpersist_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersist_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEFAULT_PERSIST_FNAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_persist_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersist_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/storage/docstore/simple_docstore.py\u001b[0m in \u001b[0;36mfrom_persist_path\u001b[0;34m(cls, persist_path, namespace, fs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \"\"\"\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0msimple_kvstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleKVStore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_persist_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersist_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple_kvstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/storage/kvstore/simple_kvstore.py\u001b[0m in \u001b[0;36mfrom_persist_path\u001b[0;34m(cls, persist_path, fs)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfsspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilesystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading {__name__} from {persist_path}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersist_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/spec.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0mac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"autocommit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_intrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m             f = self._open(\n\u001b[0m\u001b[1;32m   1302\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/implementations/local.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_mkdir\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mLocalFileOpener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtouch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/implementations/local.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_compression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocksize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_BUFFER_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/implementations/local.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocommit\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m                     \u001b[0mcompress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/db/docstore.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Settings.llm = HuggingFaceLLM(\n",
        "    model_name=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    tokenizer_name=\"meta-llama/Llama-3.2-1B-Instruct\", # You can explicitly set tokenizer_name, though often it's inferred from model_name\n",
        "    tokenizer_kwargs={\"torch_dtype\": torch.float16 if torch.cuda.is_available() else torch.float32}, # Tokenizer kwargs\n",
        "    model_kwargs={\"torch_dtype\": torch.float16 if torch.cuda.is_available() else torch.float32}, # Model kwargs - REMOVED device_map HERE\n",
        "    context_window=3000, # Keep other parameters if you need them, adjust as necessary\n",
        "    max_new_tokens=5,\n",
        "    generate_kwargs={\"temperature\": 0.1},\n",
        ")"
      ],
      "metadata": {
        "id": "H9KYb74t9teK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import base64\n",
        "from llama_index.core import StorageContext, load_index_from_storage, VectorStoreIndex, SimpleDirectoryReader, ChatPromptTemplate\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from transformers import pipeline\n",
        "from dotenv import load_dotenv\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "import torch # Import torch if you are using torch_dtype\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "# Load environment variables (optional, if you still need dotenv for other things)\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "Settings.llm = HuggingFaceLLM(\n",
        "    model_name=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    tokenizer_name=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    context_window=3000,\n",
        "    max_new_tokens=5,\n",
        "    generate_kwargs={\"temperature\": 0.1},\n",
        ")\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
        ")\n",
        "\n",
        "\n",
        "# Define the directory for persistent storage and data\n",
        "PERSIST_DIR = os.path.abspath(\"./db\") # Use absolute path\n",
        "DATA_DIR = \"data\" # Keep as relative for now, adjust if needed\n",
        "\n",
        "# Ensure data directory exists\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
        "\n",
        "# ... (descriptions dictionary - unchanged) ...\n",
        "\n",
        "def data_ingestion():\n",
        "    print(\"Starting data ingestion...\") # Debug print\n",
        "    try:\n",
        "        documents = SimpleDirectoryReader(DATA_DIR).load_data()\n",
        "        print(f\"Loaded {len(documents)} documents.\") # Debug print\n",
        "        storage_context = StorageContext.from_defaults()\n",
        "        index = VectorStoreIndex.from_documents(documents)\n",
        "        index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
        "        print(f\"Vector store persisted to: {PERSIST_DIR}\") # Debug print\n",
        "        print(\"PDF data ingested and vector store created/updated.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during data ingestion: {e}\") # Error handling\n",
        "        raise # Re-raise the exception for debugging\n",
        "\n",
        "\n",
        "def handle_query(query):\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
        "    print(f\"Loading vector store from: {PERSIST_DIR}\") # Debug print\n",
        "    index = load_index_from_storage(storage_context) # This is where FileNotFoundError occurs\n",
        "    # ... (rest of handle_query - unchanged) ...\n",
        "\n",
        "\n",
        "# Main function to run without UI\n",
        "def main():\n",
        "    pdf_file_path = \"/content/data/sample.pdf\" # Path to your PDF file\n",
        "    user_question = \"What is the main topic of this document?\" # Your question\n",
        "\n",
        "    # Force data ingestion for debugging\n",
        "    print(\"Forcing data ingestion for debugging...\") # Debug print\n",
        "    data_ingestion() # Always run data ingestion\n",
        "\n",
        "    print(\"Vector store should be created/updated now.\") # Debug print\n",
        "\n",
        "    print(f\"Question: {user_question}\")\n",
        "    response = handle_query(user_question)\n",
        "    print(f\"Answer: {response}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "QlaUeyT1_XTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WPXg4YSX_tS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S8VXG5G0_tQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cTrtF_KJ_tNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import base64\n",
        "from llama_index.core import StorageContext, load_index_from_storage, VectorStoreIndex, SimpleDirectoryReader, ChatPromptTemplate\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from transformers import pipeline\n",
        "from dotenv import load_dotenv\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "import torch  # Import torch if you are using torch_dtype\n",
        "\n",
        "# Load environment variables (optional, if you still need dotenv for other things)\n",
        "load_dotenv()\n",
        "\n",
        "# Configure the Llama index settings\n",
        "Settings.llm = HuggingFaceLLM(\n",
        "    model_name=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    tokenizer_name=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    context_window=3000,\n",
        "    max_new_tokens=5,  # Reduced max_new_tokens for testing, adjust as needed\n",
        "    generate_kwargs={\"temperature\": 0.1},\n",
        ")\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
        ")\n",
        "\n",
        "\n",
        "# Define the directory for persistent storage and data\n",
        "PERSIST_DIR = os.path.abspath(\"./db\")  # Use absolute path\n",
        "DATA_DIR = \"data\"  # Keep as relative for now, adjust if needed\n",
        "\n",
        "# Ensure data directory exists\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
        "\n",
        "# Language descriptions (not used in non-UI version, but kept for reference)\n",
        "descriptions = {\n",
        "    \"pl\": \"\"\"\n",
        "    # ChatPDF (Non-UI)\n",
        "    **ChatPDF** to zaawansowane narzędzie oparte na sztucznej inteligencji, zaprojektowane do analizy i generowania odpowiedzi na pytania związane z treścią załadowanych dokumentów PDF. Działa w trybie wiersza poleceń.\n",
        "    **Technologie**:\n",
        "    - Model: Llama-3.2-1B-Instruct (Lokalnie)\n",
        "    - Stworzony przez: Rafał Dembski\n",
        "    - Technologie: LlamaIndex, PyTorch, Transformers\n",
        "    \"\"\",\n",
        "    \"en\": \"\"\"\n",
        "    # ChatPDF (Non-UI)\n",
        "    **ChatPDF** is an advanced AI-powered tool designed to analyze and generate answers to questions related to the content of uploaded PDF documents. Runs in command-line mode.\n",
        "    **Technologies**:\n",
        "    - Model: Llama-3.2-1B-Instruct (Local)\n",
        "    - Developed by: Rafał Dembski\n",
        "    - Technologies: LlamaIndex, PyTorch, Transformers\n",
        "    \"\"\",\n",
        "    \"de\": \"\"\"\n",
        "    # ChatPDF (Non-UI)\n",
        "    **ChatPDF** ist ein fortschrittliches, KI-gesteuertes Tool, das entwickelt wurde, um Fragen zur Analyse und Beantwortung von Fragen im Zusammenhang mit dem Inhalt hochgeladener PDF-Dokumente zu generieren. Läuft im Kommandozeilenmodus.\n",
        "    **Technologien**:\n",
        "    - Modell: Llama-3.2-1B-Instruct (Lokal)\n",
        "    - Entwickelt von: Rafał Dembski\n",
        "    - Technologien: LlamaIndex, PyTorch, Transformers\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "def data_ingestion():\n",
        "    print(\"Starting data ingestion...\")  # Debug print\n",
        "    try:\n",
        "        print(f\"DATA_DIR is: {DATA_DIR}\")  # Debug print\n",
        "        print(f\"Contents of DATA_DIR: {os.listdir(DATA_DIR)}\")  # Debug print\n",
        "        pdf_file_path = os.path.abspath(os.path.join(DATA_DIR, \"sample.pdf\"))  # Assuming \"sample.pdf\" in DATA_DIR\n",
        "        print(f\"Attempting to load PDF from path: {pdf_file_path}\")  # Debug print\n",
        "        documents = SimpleDirectoryReader(input_files=[pdf_file_path]).load_data()  # Load specific file\n",
        "        print(f\"Loaded {len(documents)} documents.\")  # Debug print\n",
        "        storage_context = StorageContext.from_defaults()\n",
        "        index = VectorStoreIndex.from_documents(documents)\n",
        "        index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
        "        print(f\"Vector store persisted to: {PERSIST_DIR}\")  # Debug print\n",
        "        print(\"PDF data ingested and vector store created/updated.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during data ingestion: {e}\")  # Error handling\n",
        "        raise  # Re-raise the exception for debugging\n",
        "\n",
        "\n",
        "def handle_query(query):\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
        "    print(f\"Loading vector store from: {PERSIST_DIR}\")  # Debug print\n",
        "    index = load_index_from_storage(storage_context)  # This is where FileNotFoundError occurred\n",
        "    chat_text_qa_msgs = [\n",
        "        (\n",
        "            \"user\",\n",
        "            \"\"\"You are a Q&A assistant named ChatPDF. You have a specific response programmed for when users specifically ask about your creator, Suriya. The response is: \"I was created by Suriya, an enthusiast in Artificial Intelligence. He is dedicated to solving complex problems and delivering innovative solutions. With a strong focus on machine learning, deep learning, Python, generative AI, NLP, and computer vision, Suriya is passionate about pushing the boundaries of AI to explore new possibilities.\" For all other inquiries, your main goal is to provide answers as accurately as possible, based on the instructions and context you have been given. If a question does not match the provided context or is outside the scope of the document, kindly advise the user to ask questions within the context of the document.\n",
        "        Context:\n",
        "        {context_str}\n",
        "        Question:\n",
        "        {query_str}\n",
        "        \"\"\"\n",
        "        )\n",
        "    ]\n",
        "    text_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)\n",
        "\n",
        "    query_engine = index.as_query_engine(text_qa_template=text_qa_template)\n",
        "    answer = query_engine.query(query)\n",
        "\n",
        "    if hasattr(answer, 'response'):\n",
        "        return answer.response\n",
        "    elif isinstance(answer, dict) and 'response' in answer:\n",
        "        return answer['response']\n",
        "    else:\n",
        "        return \"Sorry, I couldn't find an answer.\"\n",
        "\n",
        "\n",
        "# Main function to run without UI\n",
        "def main():\n",
        "    pdf_file_path = \"/content/data/sample.pdf\"  # Path to your PDF file\n",
        "    user_question = \"What is the main topic of this document?\"  # Your question\n",
        "\n",
        "    # Force data ingestion for debugging - always run it\n",
        "    print(\"Forcing data ingestion for debugging...\")  # Debug print\n",
        "    data_ingestion()  # Always run data ingestion\n",
        "    print(\"Vector store should be created/updated now.\")  # Debug print\n",
        "\n",
        "    print(f\"Question: {user_question}\")\n",
        "    response = handle_query(user_question)\n",
        "    print(f\"Answer: {response}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iToNGta_tKa",
        "outputId": "d1fd4c98-6e73-43ec-f787-1b7c2c77fe6e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forcing data ingestion for debugging...\n",
            "Starting data ingestion...\n",
            "DATA_DIR is: data\n",
            "Contents of DATA_DIR: ['sample.pdf']\n",
            "Attempting to load PDF from path: /content/data/sample.pdf\n",
            "Loaded 6 documents.\n",
            "Vector store persisted to: /content/db\n",
            "PDF data ingested and vector store created/updated.\n",
            "Vector store should be created/updated now.\n",
            "Question: What is the main topic of this document?\n",
            "Loading vector store from: /content/db\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: The main topic of this\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import base64\n",
        "from llama_index.core import StorageContext, load_index_from_storage, VectorStoreIndex, SimpleDirectoryReader, ChatPromptTemplate\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from transformers import pipeline\n",
        "from dotenv import load_dotenv\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "import torch  # Import torch if you are using torch_dtype\n",
        "\n",
        "# Load environment variables (optional, if you still need dotenv for other things)\n",
        "load_dotenv()\n",
        "\n",
        "# Configure the Llama index settings\n",
        "Settings.llm = HuggingFaceLLM(\n",
        "    model_name=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    tokenizer_name=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    context_window=3000,\n",
        "    max_new_tokens=55,  # Reduced max_new_tokens for testing, adjust as needed\n",
        "    generate_kwargs={\"temperature\": 0.1},\n",
        ")\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
        ")\n",
        "\n",
        "\n",
        "# Define the directory for persistent storage and data\n",
        "PERSIST_DIR = os.path.abspath(\"./db\")  # Use absolute path\n",
        "DATA_DIR = \"data\"  # Keep as relative for now, adjust if needed\n",
        "\n",
        "# Ensure data directory exists\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
        "\n",
        "# Language descriptions (not used in non-UI version, but kept for reference)\n",
        "descriptions = {\n",
        "    \"pl\": \"\"\"\n",
        "    # ChatPDF (Non-UI)\n",
        "    **ChatPDF** to zaawansowane narzędzie oparte na sztucznej inteligencji, zaprojektowane do analizy i generowania odpowiedzi na pytania związane z treścią załadowanych dokumentów PDF. Działa w trybie wiersza poleceń.\n",
        "    **Technologie**:\n",
        "    - Model: Llama-3.2-1B-Instruct (Lokalnie)\n",
        "    - Stworzony przez: Rafał Dembski\n",
        "    - Technologie: LlamaIndex, PyTorch, Transformers\n",
        "    \"\"\",\n",
        "    \"en\": \"\"\"\n",
        "    # ChatPDF (Non-UI)\n",
        "    **ChatPDF** is an advanced AI-powered tool designed to analyze and generate answers to questions related to the content of uploaded PDF documents. Runs in command-line mode.\n",
        "    **Technologies**:\n",
        "    - Model: Llama-3.2-1B-Instruct (Local)\n",
        "    - Developed by: Rafał Dembski\n",
        "    - Technologies: LlamaIndex, PyTorch, Transformers\n",
        "    \"\"\",\n",
        "    \"de\": \"\"\"\n",
        "    # ChatPDF (Non-UI)\n",
        "    **ChatPDF** ist ein fortschrittliches, KI-gesteuertes Tool, das entwickelt wurde, um Fragen zur Analyse und Beantwortung von Fragen im Zusammenhang mit dem Inhalt hochgeladener PDF-Dokumente zu generieren. Läuft im Kommandozeilenmodus.\n",
        "    **Technologien**:\n",
        "    - Modell: Llama-3.2-1B-Instruct (Lokal)\n",
        "    - Entwickelt von: Rafał Dembski\n",
        "    - Technologien: LlamaIndex, PyTorch, Transformers\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "def data_ingestion():\n",
        "    print(\"Starting data ingestion...\")  # Debug print\n",
        "    try:\n",
        "        print(f\"DATA_DIR is: {DATA_DIR}\")  # Debug print\n",
        "        print(f\"Contents of DATA_DIR: {os.listdir(DATA_DIR)}\")  # Debug print\n",
        "        pdf_file_path = os.path.abspath(os.path.join(DATA_DIR, \"sample.pdf\"))  # Assuming \"sample.pdf\" in DATA_DIR\n",
        "        print(f\"Attempting to load PDF from path: {pdf_file_path}\")  # Debug print\n",
        "        documents = SimpleDirectoryReader(input_files=[pdf_file_path]).load_data()  # Load specific file\n",
        "        print(f\"Loaded {len(documents)} documents.\")  # Debug print\n",
        "        storage_context = StorageContext.from_defaults()\n",
        "        index = VectorStoreIndex.from_documents(documents)\n",
        "        index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
        "        print(f\"Vector store persisted to: {PERSIST_DIR}\")  # Debug print\n",
        "        print(\"PDF data ingested and vector store created/updated.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during data ingestion: {e}\")  # Error handling\n",
        "        raise  # Re-raise the exception for debugging\n",
        "\n",
        "\n",
        "def handle_query(query):\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
        "    print(f\"Loading vector store from: {PERSIST_DIR}\")  # Debug print\n",
        "    index = load_index_from_storage(storage_context)  # This is where FileNotFoundError occurred\n",
        "    chat_text_qa_msgs = [\n",
        "        (\n",
        "            \"user\",\n",
        "            \"\"\"You are a Q&A assistant named ChatPDF. You have a specific response programmed for when users specifically ask about your creator, Suriya. The response is: \"I was created by Suriya, an enthusiast in Artificial Intelligence. He is dedicated to solving complex problems and delivering innovative solutions. With a strong focus on machine learning, deep learning, Python, generative AI, NLP, and computer vision, Suriya is passionate about pushing the boundaries of AI to explore new possibilities.\" For all other inquiries, your main goal is to provide answers as accurately as possible, based on the instructions and context you have been given. If a question does not match the provided context or is outside the scope of the document, kindly advise the user to ask questions within the context of the document.\n",
        "        Context:\n",
        "        {context_str}\n",
        "        Question:\n",
        "        {query_str}\n",
        "        \"\"\"\n",
        "        )\n",
        "    ]\n",
        "    text_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)\n",
        "\n",
        "    query_engine = index.as_query_engine(text_qa_template=text_qa_template)\n",
        "    answer = query_engine.query(query)\n",
        "\n",
        "    if hasattr(answer, 'response'):\n",
        "        return answer.response\n",
        "    elif isinstance(answer, dict) and 'response' in answer:\n",
        "        return answer['response']\n",
        "    else:\n",
        "        return \"Sorry, I couldn't find an answer.\"\n",
        "\n",
        "\n",
        "# Main function to run without UI\n",
        "def main():\n",
        "    pdf_file_path = \"/content/data/sample.pdf\"  # Path to your PDF file\n",
        "    user_question = \"What is the main topic of this document?\"  # Your question\n",
        "\n",
        "    # Force data ingestion for debugging - always run it\n",
        "    print(\"Forcing data ingestion for debugging...\")  # Debug print\n",
        "    data_ingestion()  # Always run data ingestion\n",
        "    print(\"Vector store should be created/updated now.\")  # Debug print\n",
        "\n",
        "    print(f\"Question: {user_question}\")\n",
        "    response = handle_query(user_question)\n",
        "    print(f\"Answer: {response}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heqPPc_2AbUr",
        "outputId": "c5a6c3ae-0cb7-4f43-dc91-f8a96577239d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forcing data ingestion for debugging...\n",
            "Starting data ingestion...\n",
            "DATA_DIR is: data\n",
            "Contents of DATA_DIR: ['sample.pdf']\n",
            "Attempting to load PDF from path: /content/data/sample.pdf\n",
            "Loaded 6 documents.\n",
            "Vector store persisted to: /content/db\n",
            "PDF data ingested and vector store created/updated.\n",
            "Vector store should be created/updated now.\n",
            "Question: What is the main topic of this document?\n",
            "Loading vector store from: /content/db\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: The main topic of this document is the German economy and its challenges, specifically focusing on the need for reform, the impact of demographic aging, climate change, and digitalization on the economy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCVbbOfUFQl-",
        "outputId": "a5e66843-5029-44f2-8b93-870ee1a0087a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDFPro"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrOs-aiiFcIO",
        "outputId": "3c099e01-2003-44c5-9e4e-0cf7d94f3ebe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDFPro\n",
            "  Downloading pymupdfpro-1.25.3-cp39-abi3-manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting PyMuPDF==1.25.3 (from PyMuPDFPro)\n",
            "  Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from PyMuPDFPro) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->PyMuPDFPro) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->PyMuPDFPro) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->PyMuPDFPro) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->PyMuPDFPro) (2025.1.31)\n",
            "Downloading pymupdfpro-1.25.3-cp39-abi3-manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF, PyMuPDFPro\n",
            "Successfully installed PyMuPDF-1.25.3 PyMuPDFPro-1.25.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6qSHM_9FmD9",
        "outputId": "8e77b58c-20a8-4bd7-f3f6-b9a1246794af"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.18-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.37 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.40)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.19 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.19)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.11)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.19->langchain-community) (0.3.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.19->langchain-community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.37->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.37->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.37->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.37->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.19->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.19->langchain-community) (2.27.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.18-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Installing collected packages: httpx-sse, pydantic-settings, langchain-community\n",
            "Successfully installed httpx-sse-0.4.0 langchain-community-0.3.18 pydantic-settings-2.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF - أسرع من PyPDF2\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import os\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS  # استخدام faiss-cpu\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# Function to extract text from PDFs (using PyMuPDF for speed)\n",
        "def extract_pdf_text(pdf_file_paths):\n",
        "    text = \"\"\n",
        "    for pdf_path in pdf_file_paths:\n",
        "        try:\n",
        "            doc = fitz.open(pdf_path)\n",
        "            for page in doc:\n",
        "                text += page.get_text(\"text\")  # Faster than PyPDF2\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing PDF file at {pdf_path}: {e}\")\n",
        "            continue\n",
        "    return text\n",
        "\n",
        "# Function to split text into optimized chunks\n",
        "def split_text_into_chunks(text):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=200)\n",
        "    return text_splitter.split_text(text)\n",
        "\n",
        "# Function to create vector store using optimized HuggingFace embeddings\n",
        "def create_and_save_vector_store(text_chunks):\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L3-v2\")  # Lighter model\n",
        "    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n",
        "    vector_store.save_local(\"faiss_index\")\n",
        "    print(\"Vector store created and saved locally.\")\n",
        "\n",
        "# Function to create the conversational prompt template\n",
        "def create_prompt_template():\n",
        "    prompt_template = \"\"\"\n",
        "    Answer the question as detailed as possible from the provided context.\n",
        "    If the answer contains any structured data like tables or lists, respond in the same format.\n",
        "    If the answer is not in the provided context, say: \"The answer is not available in the context.\" Do not make up an answer.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "    \"\"\"\n",
        "    return PromptTemplate(template=prompt_template, input_variables=['context', 'question'])\n",
        "\n",
        "# Function to handle user queries efficiently\n",
        "def handle_user_query(user_question):\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L3-v2\")  # Lighter model\n",
        "    try:\n",
        "        new_db = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading vector store: {e}. Ensure 'faiss_index' exists and is correctly created.\")\n",
        "        return\n",
        "\n",
        "    docs = new_db.similarity_search(user_question)\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "    prompt = create_prompt_template()\n",
        "    formatted_prompt = prompt.format(context=context, question=user_question)\n",
        "\n",
        "    # Use a smaller and optimized model for text generation\n",
        "    generator = pipeline(\n",
        "        'text-generation',\n",
        "        model='meta-llama/Llama-3.2-1B-Instruct',  # Faster model for CPU\n",
        "        torch_dtype=torch.float32,\n",
        "        device='cpu'\n",
        "    )\n",
        "\n",
        "    response = generator(formatted_prompt, max_new_tokens=5, num_return_sequences=1)\n",
        "    reply_text = response[0]['generated_text'] if response else \"No response generated.\"\n",
        "\n",
        "    print(\"Question:\", user_question)\n",
        "    print(\"Reply:\", reply_text)\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    pdf_file_paths = [\n",
        "        \"/content/data/sample.pdf\",  # Replace with actual PDF file paths\n",
        "    ]\n",
        "    user_question = \"What is the main topic of these documents?\"\n",
        "\n",
        "    # Process PDF and create vector store if needed\n",
        "    if not os.path.exists(\"faiss_index\"):\n",
        "        print(\"Processing PDF and creating vector store...\")\n",
        "        raw_text = extract_pdf_text(pdf_file_paths)\n",
        "        if raw_text:\n",
        "            text_chunks = split_text_into_chunks(raw_text)\n",
        "            create_and_save_vector_store(text_chunks)\n",
        "        else:\n",
        "            print(\"No text extracted from PDFs. Skipping vector store creation.\")\n",
        "            return\n",
        "    else:\n",
        "        print(\"Vector store already exists. Loading existing store.\")\n",
        "\n",
        "    handle_user_query(user_question)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a2a5d623f2d04cb98f108a22e260eb9e",
            "0d90556caee14415aee840e76fd84e39",
            "5983f55e352d46a58ee3291c7c44b0f3",
            "4eb42efb6dee4ff99643760a4736268c",
            "4ad31049643e48618063d122c8256249",
            "15be2fd4915148e998788cbbb3736a76",
            "8d9b0c9ad4c641c58ce6b1f4e752b855",
            "21621144110b4cacab55a99c23c68fe7",
            "e09cb023b43741c7bbfadbe949623491",
            "c617300af8734248a9bd3252ea39ce12",
            "07436878dae54f96b124dc8acef1d7d1",
            "187f9932a78844fe81afe040dfce24c0",
            "56d511db88854c93a0d87ad09d90f0fc",
            "833da7761cc4484bb47883e393ecc832",
            "a5bd7973e2b14ef2908c8265aa32c79d",
            "8e2a909c9c8e47c9acddac4e4ba02045",
            "3451da3333bb4b7eb8432fb4b1f4e85a",
            "94ad60880d4f422299c5dc74659d9626",
            "7163798d089b49709c61610db28f1aac",
            "83143ff3d67846f0b125ff9d08900708",
            "5690235b7b3c431b88314540d670246e",
            "1594b4bfe0114c979cc80608dafeb6c5",
            "e8764062ab16478f92fad69b212048c9",
            "4ed4037da5384235a4f0dc6e3349254f",
            "1f509632e4154474aa5239445ae5f683",
            "a84a763e6e204464b515fc7a82b2f1c2",
            "b2dc3bd6c03843b1bb819b418341a24e",
            "f95bb34ee45e47e1b898883a201127a4",
            "999df7a52c4f48638198a19eb79cb821",
            "be5de5124efa4ca587de4fbc87939f3a",
            "dc37e3d714bf47c19aebcaeb3ac1f146",
            "04619164b6dd45a8a41a42083ef7c98f",
            "7581ae830f1b48b59c39b3bb279c3e47",
            "517a6c8278f54d6b93b5880377aedc85",
            "3e9843afed1947e1af4736eeaf4bef78",
            "3f3ca5a02a40486cb786e160247e2f80",
            "56e46bee73494f45820a0437932d667b",
            "f9dcbbd0a6f84772a527a548af8e9be2",
            "0a0a3d989ac04104bd3a232b6428e698",
            "d7b747bdeb2940548847b42d49180b70",
            "a5efaf426386431ab6d3538b6552b36c",
            "36b8bda492084777882cae239a69ca8a",
            "5baa21f73ffa4714b53386546632e764",
            "a620de02eafd4f3eb40dca6cb421e1e4",
            "fd75571210984274a9942c5efc069da0",
            "22659eedccb84250b9aaa33fe6014515",
            "2fc92ccc271246c5bc0e4dd932e2c541",
            "ab4e4736fcc24ec1ac7ea2ed1bde840b",
            "db292e82268a457db79fb67b32da3c87",
            "d5d5eddc89cc4f06a6ef236108116122",
            "1811bc1d664f474ca2fc364042a0da1b",
            "a24d2ded9e514754be142e2803576150",
            "e87da19aff4b4defabfc9c001983c72f",
            "910e32b207734761a69fff42bf369c6b",
            "28ff875304b04b3a87215a741873f1f7",
            "1679d54dbdcf4c3e8de6ea470e4b4966",
            "00771a8a12a04fab957399d0e5537632",
            "67a02f45b755403794d60de9fef735fc",
            "4a2c4a7b196645c5ba87f2f1646c08de",
            "e41ee03713844fef9c85bb048ce20f47",
            "6055a662d3404b3f941492810e1e44b0",
            "bc1b533a136d461ab14a3ab1cf1a707b",
            "ecf175c9de944d2ab3a17a6ce1976667",
            "b6a327c8fce0454a85a94acdfc73c214",
            "68622489fec5415d8a8b0ef0f960cb46",
            "b619a44515d043f987db689fb36fbdf2",
            "75f7fa409f044955b7f91e1145913876",
            "73f360df3b18499f81c20e227d7ebd82",
            "c51c1096a9cc4280b3c39a9ce56204b0",
            "1628676a86d848f99e850ef571b8c13a",
            "5d1900b885d4401aabd115ed38b6f52a",
            "a857faa554ec495f81b71f1312c8f7f0",
            "f7dba94c62f04ce08d185a071bd612b3",
            "8b860685a6c9482dbd25804ce68576d3",
            "23348cbdbaa64b06966b91b0921428fa",
            "32a8be39628140c39d3a99208eb3abd7",
            "fa9ce9f9eb6c464ab4b6841fbaeb150b",
            "114cddc5b9fc4852857a4d2463ab42d8",
            "fe5605e339c640deb214c785a35f1b4c",
            "6a2f0ffb4858468590625500a1549b1c",
            "d322e4035cd34999b2043b26a4cfc3ba",
            "adc87313a2f9496ea437beb6fefc640d",
            "cb3f7462ba8a4b1e86b641991f63474a",
            "bcebd3664da14d82bdb1edc9a2e6b369",
            "c5c6e75c210649899969f9eb91b553cc",
            "db4a572155034d5bbd4a8da3ebbcd13f",
            "c8d7f63cffa94ba2a291241ee7251d13",
            "756ebb62dbcf4d76b47ca6a6ba1f09a1",
            "1f00a614e9904bfbb75dc80a718b6955",
            "9637b3c2e3fd49f491c73ca3e8636eec",
            "fffc08fbc36340ef8190409d98830743",
            "5a59ed39d5e4491da5b9d49cfd30626a",
            "902327e36cd04cc4ade30dad2a4e6727",
            "c7f708bd66ef49e28d01b5b805c0e06c",
            "0cd1769c81cd47b78a5e38f259c7b307",
            "caa5c2029f154296b786ab81df317375",
            "e94e4f37d4924e78be7005a19124b6e3",
            "3aa67259a4674db1a870577a0533c582",
            "d07633b8913647bbbe203ba9b0b1c8cb",
            "fe5d8ff24ab443ccae676474237e6e5b",
            "ed3e8a9b34144b6d876eceb249fb5138",
            "ad1e44a6f4f44d69967498fd4ffd0028",
            "660b7f01bfcc447c99e0575e6871c04b",
            "bf6d8b30749c4bdea1007ac60806e24c",
            "c656200085284d799735404cbbdfa67d",
            "75f776ccecf448e2ad3db6ad7bb05e80",
            "e2345ca5d6ba44ae9a97f08009c22587",
            "353c82591d6f4667b90fd4036808054a",
            "db25cda9c60d4f1ca2ccaf3e49f55c33",
            "803391198f1a41319e6af9c9e2aba54f",
            "225fd347d0c54319b5e970ba648dc534",
            "993e0e6ab3fe4adca5006e6108f17616",
            "7958c06bc3144c11a99875a7665a6197",
            "377d9fdf49cb42218603a05f522e40cc",
            "30477e8f22774dfcb95c21f161b718b8",
            "8c579d9d5296410f8c1bfbbfbf73103a",
            "554f446a003f49b28ea476c9c6fe815a",
            "6c684913a79b4f4c83c646f7b5683698",
            "339d1188e42b4d88b05f07fdf7611761",
            "11c31f27f64a4a2f89bc357ff5fcb27a",
            "1fc82cda351a42e38c1c2f460087ad25"
          ]
        },
        "id": "L6WAz1TeEwZe",
        "outputId": "f763c2e2-ae59-4303-fed3-1108e74d6c68"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing PDF and creating vector store...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-d183d3dc967d>:30: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L3-v2\")  # Lighter model\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2a5d623f2d04cb98f108a22e260eb9e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "187f9932a78844fe81afe040dfce24c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/4.04k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8764062ab16478f92fad69b212048c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "517a6c8278f54d6b93b5880377aedc85"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd75571210984274a9942c5efc069da0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/69.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1679d54dbdcf4c3e8de6ea470e4b4966"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75f7fa409f044955b7f91e1145913876"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "114cddc5b9fc4852857a4d2463ab42d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f00a614e9904bfbb75dc80a718b6955"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe5d8ff24ab443ccae676474237e6e5b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling%2Fconfig.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "225fd347d0c54319b5e970ba648dc534"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector store created and saved locally.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the main topic of these documents?\n",
            "Reply: \n",
            "    Answer the question as detailed as possible from the provided context.\n",
            "    If the answer contains any structured data like tables or lists, respond in the same format.\n",
            "    If the answer is not in the provided context, say: \"The answer is not available in the context.\" Do not make up an answer.\n",
            "\n",
            "    Context:\n",
            "    von einer Stagnation der Erwerbstätigkeit aus. Einem Be-\n",
            "schäftigungsaufbau im Bereich der sozialen Dienstleistun-\n",
            "gen dürfte dabei zunächst ein weiterer Abbau der Beschäf-\n",
            "tigung in den produzierenden Branchen gegenüberstehen. \n",
            "Angesichts eines zunehmenden qualifikatorischen Mis-\n",
            "matches zwischen Arbeitsangebot und -nachfrage dürften \n",
            "die Beschäftigungsperspektiven für Arbeitslose jedoch \n",
            "weiterhin schwierig bleiben, so dass die Arbeitslosigkeit im \n",
            "Jahresdurchschnitt voraussichtlich um 120.000 Personen \n",
            "steigt. \n",
            "Bei den Verbraucherpreisen rechnet die Bundesregierung \n",
            "für dieses Jahr mit einer Inflationsrate von 2,2 Prozent. \n",
            "Einem leicht überdurchschnittlichen Zuwachs bei den \n",
            "Dienstleistungspreisen, die wesentlich durch die zuvor hö-\n",
            "heren Lohnabschlüsse geprägt waren, stehen die anhalten-\n",
            "den Wirkungen der vorangegangenen geldpolitischen \n",
            "Straffungen, gesunkener Energiekosten sowie entlastende \n",
            "Preisentwicklungen auf den vorgelagerten Preisstufen \n",
            "gegenüber. Temporär inflationserhöhende Effekte ergeben \n",
            "sich zu Jahresbeginn 2025 durch administrative Preiserhö-\n",
            "hungen wie die Anhebung der CO2-Abgabe, gestiegene \n",
            "Portogebühren, die Anhebung des Preises für das Deutsch-\n",
            "landticket sowie erhöhte Beitragssätze bei Kranken- und \n",
            "Pflegeversicherungen.\n",
            "STRUKTURREFORMEN FORTFÜHREN, WETTBE-\n",
            "WERBSFÄHIGKEIT NACHHALTIG STÄRKEN\n",
            "Jenseits der kurzfristigen Hürden für eine neue wirtschaft-\n",
            "liche Dynamik steht Deutschland aus Sicht der Bundesre-\n",
            "gierung vor vier strukturellen Herausforderungen, denen \n",
            "sich die Wirtschafts- und Finanzpolitik weiterhin widmen \n",
            "muss. \n",
            "1.\t Die geopolitische Zeitenwende erfordert zusätzliche \n",
            "Ausgaben für Landes- und Bündnisverteidigung wie \n",
            "auch die Unterstützung der Ukraine. Hinzu kommt eine \n",
            "immer konfrontativere Handelspolitik, die zu zuneh-\n",
            "mender Fragmentierung der Weltwirtschaft führt. Dies \n",
            "trifft die deutsche Wirtschaft mit ihrer traditionell star-\n",
            "ken Exportorientierung in besonderem Maße.\n",
            "2.\t Durch die demografische Alterung ist mit einer Ver-\n",
            "schärfung bereits bestehender Fachkräfteengpässe und \n",
            "deutlich erhöhtem Druck auf die Finanzierung der So-\n",
            "zialversicherungssysteme zu rechnen.\n",
            "3.\t Der aufgrund des Klimawandels notwendige Umbau \n",
            "der Wirtschaft auf dem Weg zur Treibhausgasneutrali-\n",
            "tät geht mit entsprechenden Kosten einher und erfor-\n",
            "dert umfassende private, aber auch öffentliche Inves-\n",
            "titionen.\n",
            "4.\t In den vergangenen Jahrzehnten vernachlässigte Stand-\n",
            "ortfaktoren hemmen Produktivität und Wachstums-\n",
            "potenzial. Hier sind immenser öffentlicher Investitions-\n",
            "stau bei Digitalisierung, Infrastruktur und Verteidigung, \n",
            "aber auch eine teils übermäßige Bürokratie unter an-\n",
            "derem in Folge europäischer Vorgaben, Berichts- und \n",
            "Nachweispflichten zu nennen. Darüber hinaus gibt es \n",
            "große Herausforderungen im Bildungsbereich, ange-\n",
            "fangen bei der frühkindlichen Bildung bis hin zur Be-\n",
            "rufsbildung.\n",
            "Um das zuletzt schwache Wachstumspotenzial der deut-\n",
            "schen Wirtschaft zu stärken und den oben genannten He-\n",
            "rausforderungen zu begegnen, hat die Bundesregierung seit \n",
            "Beginn dieser Legislaturperiode verstärkt auf angebotssei-\n",
            "tige Reformen gesetzt. Sie hat dazu das Thema Bürokratie-\n",
            "abbau einschließlich Planungs- und Genehmigungsbe-\n",
            "schleunigung in den vergangenen zwei Jahren stark \n",
            "priorisiert und zahlreiche Weichenstellungen vorgenom-\n",
            "men. So wurden unter anderem durch das vierte Bürokra-\n",
            "tieentlastungsgesetz diverse Regeln und Abläufe verein-\n",
            "facht. Mit dem „Pakt für Planungs-, Genehmigungs- und \n",
            "Umsetzungsbeschleunigung“ hat die Bundesregierung ge-\n",
            "meinsam mit den Ländern Fristen verkürzt und einzelne \n",
            "Prüfschritte in Genehmigungsverfahren reduziert. \n",
            "Mit den Praxischecks hat sie ein neues Instrument einge-\n",
            "führt, welches gezielt Wechselwirkungen zwischen den \n",
            "verschiedenen Regelungen in den Blick nimmt. Dabei wer-\n",
            "den im Austausch zwischen Unternehmen, Verwaltungen \n",
            "sowie weiteren Expertinnen und Experten anhand konkre-\n",
            "ter Fallkonstellationen gemeinsam bürokratische Hemm-\n",
            "nisse und Lösungsansätze identifiziert. Das Ver-\n",
            "fahren wurde erfolgreich mit dem Praxischeck \n",
            "„Errichtung und Betrieb von Photovoltaik-An-\n",
            "SCHL AGL ICHTER _02|25_WIRTSCHAFTSPOLITIK 10\n",
            "lagen“ pilotiert; der Großteil der identifizierten Hemmnis-\n",
            "se ist bereits aus dem Weg geräumt, u.a. im Solarpaket. Die \n",
            "Ressorts setzen einen Beschluss der Wachstumsinitiative \n",
            "dazu um und führen nunmehr vermehrt Praxischecks \n",
            "durch. Die Anstrengungen der Bundesregierung wirken sich \n",
            "bereits jetzt positiv aus: Wie der Nationale Normenkontroll-\n",
            "rat feststellt, wurde die Wirtschaft zwischen Juli 2023 und \n",
            "Juni 2024 um mehr als 400 Millionen Euro Erfüllungsauf-\n",
            "wand entlastet.\n",
            "Um das Arbeitsangebot trotz der Auswirkungen der demo-\n",
            "grafischen Alterung zu stabilisieren oder sogar auszuweiten, \n",
            "hat die Bundesregierung Reformen zur besseren Nutzung \n",
            "des Erwerbspersonenpotenzials umgesetzt bzw. in die Wege \n",
            "geleitet. So hat sie umfassende Maßnahmen für eine be-\n",
            "schleunigte und bürokratieärmere qualifizierte Einwande-\n",
            "\n",
            "des Erwerbspersonenpotenzials umgesetzt bzw. in die Wege \n",
            "geleitet. So hat sie umfassende Maßnahmen für eine be-\n",
            "schleunigte und bürokratieärmere qualifizierte Einwande-\n",
            "rung umgesetzt. Gleichzeitig hat sie zahlreiche Hürden bei \n",
            "der Arbeitsmarktintegration von bereits Zugewanderten \n",
            "abgebaut. Dies dürfte dazu beigetragen haben, dass die Er-\n",
            "werbstätigkeit im Durchschnitt des vergangenen Jahres mit \n",
            "rund 46 Millionen Personen einen Höchststand erreichte.\n",
            "Auch stellt die Bundesregierung weiterhin finanzielle Mit-\n",
            "tel für den Ausbau und die Qualität der Kindertagesbetreu-\n",
            "ung zur Verfügung, die einen Beitrag zu einer umfangrei-\n",
            "cheren Erwerbstätigkeit von Müttern leisten kann. Um die \n",
            "Erwerbsfähigkeit im Zuge der Transformation und sich \n",
            "verändernder Qualifikationsanforderungen aufrechtzu-\n",
            "erhalten, wurde die Aus- und Weiterbildung beispielsweise \n",
            "durch das Qualifizierungsgeld oder die Ausbildungsgaran-\n",
            "tie gestärkt. Diese Maßnahmen tragen auch zu einer besse-\n",
            "ren Entwicklung der Arbeitsproduktivität bei.\n",
            "Im Zuge der durch den russischen Angriffskrieg ausgelösten \n",
            "Energiekrise 2022/2023 ist es in einem gesamtgesellschaft-\n",
            "lichen Kraftakt gelungen, die Energieversorgung über alle \n",
            "Energieträger hinweg zu stabilisieren und sich von der star-\n",
            "ken Abhängigkeit von russischen Energieimporten zu lösen. \n",
            "Auch dank der Maßnahmen der Bundesregierung, wie z. B. \n",
            "den temporär eingeführten Strom-, Gas- und Wärmepreis-\n",
            "bremsen, konnten die unmittelbaren Krisenfolgen abgemil-\n",
            "dert werden. Jenseits dieser akuten Stabilisierungsmaßnah-\n",
            "men hat die Bundesregierung die Grundlagen für eine \n",
            "deutlich stärkere Dynamik beim Ausbau von erneuerbaren \n",
            "Energien sowie der Netzinfrastruktur gelegt und zügig eine \n",
            "Infrastruktur für Flüssigerdgas aufgebaut. Dies hat einen \n",
            "Beitrag dazu geleistet, dass in den beiden vergangenen Jah-\n",
            "ren jeweils Rekorde beim Ausbau und den Genehmigungen \n",
            "erneuerbarer Energien erreicht wurden. Die Strom- und \n",
            "Gaspreise an den Kurzfristmärkten sind gegenüber den Kri-\n",
            "senjahren mittlerweile deutlich gesunken. So lag der durch-\n",
            "schnittliche Strompreis für Neuabschlüsse in der Industrie \n",
            "im Jahr 2024 durch Abschaffung der EEG-Umlage unter dem \n",
            "Niveau von 2017 bis 2020. Allerdings hat sich mit der Ener-\n",
            "giekrise der Abstand der Endverbraucherpreise der deut-\n",
            "schen Industrie für Strom und Gas im Vergleich zu wichtigen \n",
            "internationalen Wettbewerbern wie den USA vergrößert. Vor \n",
            "diesem Hintergrund ist wesentliches Ziel der Bun-\n",
            "desregierung, die Energiepreise wieder rückzu-\n",
            "führen.\n",
            "ABBILDUNG 2: GENEHMIGUNGSBESCHLEUNIGUNG IM AUSBAU DER ERNEUERBAREN ENERGIEN UND \n",
            "NETZINFRASTRUKTUR\n",
            "Quelle: Eigene Darstellung, Bundesnetzagentur. *Stand Q3 2024 mit Prognose für Q4 2024. \n",
            "Megawatt\n",
            "Neugenehmigungen Wind an Land\n",
            "2021\n",
            "2022\n",
            "2023\n",
            "2024*\n",
            "Kilometer\n",
            "Netzausbau\n",
            "0\n",
            "400\n",
            "800\n",
            "1.200\n",
            "1.600\n",
            "2.000\n",
            "1.379\n",
            "1.739\n",
            "607\n",
            "1.628\n",
            "304\n",
            "481\n",
            "630\n",
            "321\n",
            "0\n",
            "1.000\n",
            "2.000\n",
            "3.000\n",
            "4.000\n",
            "5.000\n",
            "6.000\n",
            "2024\n",
            "2023\n",
            "2022\n",
            "2021\n",
            "2020\n",
            "2019\n",
            "2018\n",
            "2017\n",
            "Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4\n",
            "Q1 Q2 Q3 Q4\n",
            "Genehmigte Trassenkilometer\n",
            "Neugenehmigungen Wind an Land\n",
            "In Bau gegangene Trassenkilometer\n",
            "SCHL AGL ICHTER _02|25_WIRTSCHAFTSPOLITIK 11\n",
            "Um die Transformation der Industrie zu unterstützen und \n",
            "die Resilienz des Standorts zu erhöhen, hat die Bundesre-\n",
            "gierung den Hochlauf neuer Technologien ermöglicht. Sie \n",
            "hat dazu u. a. das Konzept der grünen Leitmärkte zur Stär-\n",
            "kung der Nachfrage nach grünen Grundstoffen, Produkten \n",
            "und Technologien entwickelt. Zudem hat sie die Ansiedlung \n",
            "in Schlüsseltechnologien wie Wasserstoff, Batteriezellfer-\n",
            "tigung, Mikroelektronik, digitalen Technologien und Bio-\n",
            "technologie unterstützt.\n",
            "Die Bundesregierung hat den deutschen Wirtschaftsstand-\n",
            "ort durch Maßnahmen im Bereich Investitionen, For-\n",
            "schung- und Entwicklung und Handel nachhaltig gestärkt. \n",
            "So hat sie die übergreifenden Rahmenbedingungen für \n",
            "Investitionen insbesondere durch attraktivere Abschrei-\n",
            "bungsbedingungen, weitergehende Möglichkeiten zur  \n",
            "Verlustverrechnung sowie eine mehrfach erheblich aus-\n",
            "geweitete steuerliche Förderung von Forschungs- und Ent-\n",
            "wicklungsausgaben verbessert. Mit dem Zukunftsfonds und \n",
            "der WIN-Initiative („Wachstums- und Innovationskapital \n",
            "für Deutschland“) soll das Ökosystem für Wagniskapital \n",
            "gestärkt und somit der Zugang zu Kapital für innovative \n",
            "Start-ups erleichtert werden. Die Investitionen in die Ver-\n",
            "kehrsinfrastruktur wurden signifikant gestärkt, insbeson-\n",
            "dere im Bereich der Schiene. Gleichzeitig konnten zuletzt \n",
            "deutliche Fortschritte bei der digitalen Infrastruktur er-\n",
            "reicht werden. Die Diversifikation der Außenhandelsbezie-\n",
            "hungen ist vorangeschritten und neue Abkommen wie jenes \n",
            "mit den Mercosur-Staaten wurden ausverhandelt oder sind \n",
            "in Arbeit.\n",
            "Angesichts der Größe der Herausforderungen ist es umso \n",
            "wichtiger, sich die immensen Stärken Deutschlands bewusst \n",
            "zu machen. Kaum ein anderes Land verfügt über eine solche \n",
            "Vielfalt an exzellenten, gerade auch mittelständischen\n",
            "\n",
            "2021 und 2023 die ausgestoßenen THG-Emissionen je  \n",
            "eine Million Euro Bruttoinlandsprodukt (preisbereinigt) um  \n",
            "34 Tonnen reduziert werden. Dies entspricht einer Entkopp-\n",
            "lungsrate von 13 Prozent in nur zwei Jahren. Das Jahr 2023 \n",
            "markiert damit die stärkste Entkopplung seit der Wieder-\n",
            "vereinigung.\n",
            "WOHLFAHRTSMESSUNG: ERWEITERTE PERSPEKTIVE \n",
            "AUF DAS WOHLERGEHEN UND KONVERGENZ­\n",
            "PROZESSE IN DER GESELLSCHAFT\n",
            "Das im Jahr 2022 eingeführte und inzwischen etablierte \n",
            "Kapitel zur Wohlfahrtsmessung blickt auch im diesjährigen \n",
            "Jahreswirtschaftsbericht auf die Entwicklung wohlfahrts-\n",
            "relevanter Aspekte. Mehr als 30 ausgewählte Indikatoren \n",
            "ergänzen das BIP gezielt und bilden neben ökonomischen \n",
            "auch wesentliche ökologische, soziale und gesellschaftliche \n",
            "Dimensionen ab. Der Blick richtet sich damit sowohl auf \n",
            "das quantitative als auch das qualitative Wachstum, welches \n",
            "die Lebensqualität und Zukunftsaussichten aller Bürgerin-\n",
            "nen und Bürger berücksichtigt.\n",
            "Die Entwicklung der Indikatoren spiegelt am aktuellen \n",
            "Rand die strukturellen Herausforderungen der deutschen \n",
            "Wirtschaft wider. So sind in den nächsten Jahren insbeson-\n",
            "dere in den Bereichen der Produktivitätsentwicklung, der \n",
            "Bildung oder des Innovationspotenzials zusätzliche An-\n",
            "strengungen notwendig. Gleichzeitig sind Erfolge wie eine \n",
            "insgesamt steigende Erwerbsbeteiligung, mehr Frauen in \n",
            "Führungspositionen oder das Voranschreiten der Energie-\n",
            "wende für eine treibhausgasneutrale Zukunft sichtbar. An-\n",
            "dere Indikatoren im Bereich des Umwelt- und Naturschut-\n",
            "zes weisen hingegen noch zu geringe bzw. abflachende \n",
            "Trendverläufe in die gewünschte Richtung auf. Weiterhin \n",
            "wird die Wohlfahrtsindikatorik stetig überprüft und weiter-\n",
            "entwickelt. So sind im diesjährigen Jahreswirtschaftsbericht \n",
            "erstmals Indikatoren zur Vermögensverteilung auf Basis \n",
            "neuer experimenteller Daten der EZB sowie zur Kreislauf-\n",
            "wirtschaft im Zuge der Nationalen Kreislaufwirtschafts-\n",
            "strategie enthalten. Auch in den kommenden Jahren wird \n",
            "es vor dem Hintergrund einer evidenzbasierten Politik \n",
            "wichtig sein, mithilfe eines solchen Indikatorensets die \n",
            "Fortschritte auf einem nachhaltigen und zukunftsfähigen \n",
            "Wachstumspfad zu verfolgen.\n",
            "Referat: IA1 – Grundsatzfragen der Wirtschaftspolitik,  \n",
            "IC-WA – Wirtschaftspolitische Analyse, Wohlfahrtsindikatorik, \n",
            "IC1 – Beobachtung, Analyse und Projektion der gesamt­\n",
            "wirtschaftlichen Entwicklung \n",
            "schlaglichter@bmwk.bund.de\n",
            "Jahreswirtschaftsbericht 2025: Für eine neue wirtschaftliche \n",
            "Dynamik\n",
            "Pressemitteilung des BMWK\n",
            "KONTAKT & MEHR ZUM THEMA\n",
            "\n",
            "wichtiger, sich die immensen Stärken Deutschlands bewusst \n",
            "zu machen. Kaum ein anderes Land verfügt über eine solche \n",
            "Vielfalt an exzellenten, gerade auch mittelständischen \n",
            "Unternehmen. Nach wie vor wird die hohe Fachkompetenz \n",
            "in den deutschen Betrieben und die institutionelle For-\n",
            "schungslandschaft und das damit verbundene Innovations-\n",
            "potenzial in Deutschland weltweit geschätzt. Diese Stärken \n",
            "bilden auch unter Berücksichtigung der im Vergleich zu \n",
            "anderen führenden Wirtschaftsnationen deutlich geringe-\n",
            "ren öffentlichen und privaten Verschuldung eine gute  \n",
            "Ausgangsposition, um die großen gesamtwirtschaftlichen \n",
            "Investitionsbedarfe zu adressieren und die Wettbewerbs-\n",
            "fähigkeit wieder zu verbessern.\n",
            "INVESTIVE IMPULSE SETZEN, UNSICHERHEITEN \n",
            "ABBAUEN\n",
            "Die Transformation zur Klimaneutralität, die digitale Trans-\n",
            "formation und die erforderliche Steigerung der Wettbe-\n",
            "werbsfähigkeit verlangen der Wirtschaft erhebliche Anpas-\n",
            "sungen ab. In kurzer Zeit müssen erhebliche – vorrangig \n",
            "private – finanzielle Mittel mobilisiert werden, um in neue \n",
            "Technologien, Infrastrukturen und Prozesse zu investieren \n",
            "sowie neue Wachstumsmärkte zu erschließen. Dafür \n",
            "braucht es verlässliche Rahmenbedingungen, aber auch zu-\n",
            "sätzliche Investitionsanreize. \n",
            "Der finanzpolitische Kurs der Bundesregierung ist seit 2023 \n",
            "als Ergebnis der Rückführung krisenbedingter Entlastungs- \n",
            "und Stabilisierungsmaßnahmen, die zur Bewältigung der \n",
            "Corona- und der Energiekrise notwendig waren, moderat \n",
            "restriktiv ausgerichtet, wie auch die übereinstimmenden \n",
            "Einschätzungen von IWF, OECD und der EU-Kommission \n",
            "zeigen. Dieser Kurs ist auch im Kontext der zeitweise hohen \n",
            "Preissteigerungsraten zu sehen, die sich seit Ende 2023 deut-\n",
            "lich verringert haben, aber noch immer auf erhöhtem Ni-\n",
            "veau lagen. Im Zuge der Verringerung des fiskalischen Spiel-\n",
            "raums im Klima- und Transformationsfonds um insgesamt \n",
            "60 Milliarden Euro als Konsequenz des Urteils des Bundes-\n",
            "verfassungsgerichts vom November 2023 mussten wirt-\n",
            "schaftspolitische Vorhaben verstärkt priorisiert werden. Das \n",
            "betrifft insbesondere finanzielle Anreize für Investitionen \n",
            "von Unternehmen und privaten Haushalten und dürfte \n",
            "zwischenzeitlich die Unsicherheit bei Wirtschaftsakteuren \n",
            "erhöht haben.\n",
            "Daher ist die Wirtschafts- und Finanzpolitik künftig doppelt \n",
            "gefordert: Es gilt, den in den vergangenen Jahren einge-\n",
            "schlagenen Weg struktureller Reformen konsequent fort-\n",
            "zusetzen und zugleich das Vertrauen der Wirtschaftsakteu-\n",
            "re zu stärken. Für die mittelfristigen Wachstumsaussichten \n",
            "sind die Bereiche unternehmerischer Investitionen und \n",
            "Innovationen sowie Forschung und Entwicklung von be-\n",
            "sonderer Bedeutung. Beispiele für mögliche Impulse zur \n",
            "Stärkung der Investitionstätigkeit wären weitere Verbesse-\n",
            "rungen im Sinne eines innovationsfreundlichen steuerli-\n",
            "chen Umfeldes oder die Einführung einer unbürokratischen \n",
            "und unkomplizierten Investitionsprämie. Maßnahmen zur \n",
            "Stärkung der Investitionstätigkeit würden den Standort \n",
            "Deutschland auch für ausländische Investoren attraktiver \n",
            "machen. Für gewerblich genutzte E-Autos, den klima-\n",
            "freundlichen Wohnungsneubau oder den sozialen Woh-\n",
            "nungsbau wären weitere, auch nachfrageseitige Impulse \n",
            "denkbar. \n",
            "KLIMAPOLITIK: TREIBHAUSGAS (THG)-­\n",
            "EMISSIONEN GEHEN WEITER ZURÜCK\n",
            "Es bleibt im Interesse Deutschlands, national, EU-weit und \n",
            "international dazu beizutragen, die globale Treibhausgas-\n",
            "entwicklung umzukehren und gleichzeitig Maßnahmen zur \n",
            "Anpassung an den Klimawandel zu ergreifen. Denn mit fort-\n",
            "schreitendem Klimawandel steigen die Risiken irreversibler \n",
            "Umweltveränderungen mit nicht zuletzt erheblichen Folgen \n",
            "für die Grundlagen wirtschaftlicher Wertschöpfung. Mit \n",
            "dem europäischen Fit-for-55-Paket und der na-\n",
            "tionalen Umsetzung wurden weitreichende am-\n",
            "bitionierte Maßnahmen für eine wettbewerbs-\n",
            "SCHL AGL ICHTER _02|25_WIRTSCHAFTSPOLITIK 12\n",
            "fähige, ressourceneffiziente Wirtschaft im Sinne des \n",
            "European Green Deals und des europäischen Klimaschutz-\n",
            "ziels auf den Weg gebracht. Auf nationaler Ebene gibt das \n",
            "Klimaschutzgesetz seit 2019 den nationalen rechtlichen \n",
            "Rahmen für die Klimaschutzpolitik vor. \n",
            "In den letzten Jahren konnte die Bundesregierung entschei-\n",
            "dende Schritte zum Erreichen des 2030-Klimaschutzziels \n",
            "gehen: Während zu Beginn der Legislaturperiode noch eine \n",
            "Minderung der Treibhausgase bis 2030 von lediglich 49 Pro-\n",
            "zent projiziert wurde, weisen die aktuellen Projektionsdaten \n",
            "einen deutlich stärkeren Rückgang der Treibhausgasemis-\n",
            "sionen um knapp 64 Prozent aus. Das zeigt, dass das Gesamt-\n",
            "minderungsziel des Bundes-Klimaschutzgesetzes von min-\n",
            "destens 65 Prozent bis 2030 bei anhaltenden Fortschritten \n",
            "in Reichweite liegt. Insgesamt hat sich die Entkopplung von \n",
            "THG-Emissionen und ökonomischer Wertschöpfung in den \n",
            "letzten Jahren deutlich beschleunigt. So konnten zwischen \n",
            "2021 und 2023 die ausgestoßenen THG-Emissionen je  \n",
            "eine Million Euro Bruttoinlandsprodukt (preisbereinigt) um  \n",
            "34 Tonnen reduziert werden. Dies entspricht einer Entkopp-\n",
            "\n",
            "    Question:\n",
            "    What is the main topic of these documents?\n",
            "     Answer:\n",
            "    The main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF - أسرع من PyPDF2\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import os\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS  # استخدام faiss-cpu\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# Function to extract text from PDFs (using PyMuPDF for speed)\n",
        "def extract_pdf_text(pdf_file_paths):\n",
        "    text = \"\"\n",
        "    for pdf_path in pdf_file_paths:\n",
        "        try:\n",
        "            doc = fitz.open(pdf_path)\n",
        "            for page in doc:\n",
        "                text += page.get_text(\"text\")  # Faster than PyPDF2\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing PDF file at {pdf_path}: {e}\")\n",
        "            continue\n",
        "    return text\n",
        "\n",
        "# Function to split text into optimized chunks\n",
        "def split_text_into_chunks(text):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=200)\n",
        "    return text_splitter.split_text(text)\n",
        "\n",
        "# Function to create vector store using optimized HuggingFace embeddings\n",
        "def create_and_save_vector_store(text_chunks):\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L3-v2\")  # Lighter model\n",
        "    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n",
        "    vector_store.save_local(\"faiss_index\")\n",
        "    print(\"Vector store created and saved locally.\")\n",
        "\n",
        "# Function to create the conversational prompt template\n",
        "def create_prompt_template():\n",
        "    prompt_template = \"\"\"\n",
        "    Answer the question as detailed as possible from the provided context.\n",
        "    If the answer contains any structured data like tables or lists, respond in the same format.\n",
        "    If the answer is not in the provided context, say: \"The answer is not available in the context.\" Do not make up an answer.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "    \"\"\"\n",
        "    return PromptTemplate(template=prompt_template, input_variables=['context', 'question'])\n",
        "\n",
        "# Function to handle user queries efficiently\n",
        "def handle_user_query(user_question):\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L3-v2\")  # Lighter model\n",
        "    try:\n",
        "        new_db = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading vector store: {e}. Ensure 'faiss_index' exists and is correctly created.\")\n",
        "        return\n",
        "\n",
        "    docs = new_db.similarity_search(user_question)\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "    prompt = create_prompt_template()\n",
        "    formatted_prompt = prompt.format(context=context, question=user_question)\n",
        "\n",
        "    # Use a smaller and optimized model for text generation\n",
        "    generator = pipeline(\n",
        "        'text-generation',\n",
        "        model='meta-llama/Llama-3.2-1B-Instruct',  # Faster model for CPU\n",
        "        torch_dtype=torch.float32,\n",
        "        device='cpu'\n",
        "    )\n",
        "\n",
        "    response = generator(formatted_prompt, max_new_tokens=55, num_return_sequences=1)\n",
        "    reply_text = response[0]['generated_text'] if response else \"No response generated.\"\n",
        "\n",
        "    print(\"Question:\", user_question)\n",
        "    print(\"Reply:\", reply_text)\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    pdf_file_paths = [\n",
        "        \"/content/data/sample.pdf\",  # Replace with actual PDF file paths\n",
        "    ]\n",
        "    user_question = \"What is the main topic of these documents?\"\n",
        "\n",
        "    # Process PDF and create vector store if needed\n",
        "    if not os.path.exists(\"faiss_index\"):\n",
        "        print(\"Processing PDF and creating vector store...\")\n",
        "        raw_text = extract_pdf_text(pdf_file_paths)\n",
        "        if raw_text:\n",
        "            text_chunks = split_text_into_chunks(raw_text)\n",
        "            create_and_save_vector_store(text_chunks)\n",
        "        else:\n",
        "            print(\"No text extracted from PDFs. Skipping vector store creation.\")\n",
        "            return\n",
        "    else:\n",
        "        print(\"Vector store already exists. Loading existing store.\")\n",
        "\n",
        "    handle_user_query(user_question)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gkf-nQ0WHSG3",
        "outputId": "c891ddd4-12fe-49ea-c7c1-1e1ef0ac0598"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector store already exists. Loading existing store.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-46d22ccc3b17>:52: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L3-v2\")  # Lighter model\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cpu\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the main topic of these documents?\n",
            "Reply: \n",
            "    Answer the question as detailed as possible from the provided context.\n",
            "    If the answer contains any structured data like tables or lists, respond in the same format.\n",
            "    If the answer is not in the provided context, say: \"The answer is not available in the context.\" Do not make up an answer.\n",
            "\n",
            "    Context:\n",
            "    von einer Stagnation der Erwerbstätigkeit aus. Einem Be-\n",
            "schäftigungsaufbau im Bereich der sozialen Dienstleistun-\n",
            "gen dürfte dabei zunächst ein weiterer Abbau der Beschäf-\n",
            "tigung in den produzierenden Branchen gegenüberstehen. \n",
            "Angesichts eines zunehmenden qualifikatorischen Mis-\n",
            "matches zwischen Arbeitsangebot und -nachfrage dürften \n",
            "die Beschäftigungsperspektiven für Arbeitslose jedoch \n",
            "weiterhin schwierig bleiben, so dass die Arbeitslosigkeit im \n",
            "Jahresdurchschnitt voraussichtlich um 120.000 Personen \n",
            "steigt. \n",
            "Bei den Verbraucherpreisen rechnet die Bundesregierung \n",
            "für dieses Jahr mit einer Inflationsrate von 2,2 Prozent. \n",
            "Einem leicht überdurchschnittlichen Zuwachs bei den \n",
            "Dienstleistungspreisen, die wesentlich durch die zuvor hö-\n",
            "heren Lohnabschlüsse geprägt waren, stehen die anhalten-\n",
            "den Wirkungen der vorangegangenen geldpolitischen \n",
            "Straffungen, gesunkener Energiekosten sowie entlastende \n",
            "Preisentwicklungen auf den vorgelagerten Preisstufen \n",
            "gegenüber. Temporär inflationserhöhende Effekte ergeben \n",
            "sich zu Jahresbeginn 2025 durch administrative Preiserhö-\n",
            "hungen wie die Anhebung der CO2-Abgabe, gestiegene \n",
            "Portogebühren, die Anhebung des Preises für das Deutsch-\n",
            "landticket sowie erhöhte Beitragssätze bei Kranken- und \n",
            "Pflegeversicherungen.\n",
            "STRUKTURREFORMEN FORTFÜHREN, WETTBE-\n",
            "WERBSFÄHIGKEIT NACHHALTIG STÄRKEN\n",
            "Jenseits der kurzfristigen Hürden für eine neue wirtschaft-\n",
            "liche Dynamik steht Deutschland aus Sicht der Bundesre-\n",
            "gierung vor vier strukturellen Herausforderungen, denen \n",
            "sich die Wirtschafts- und Finanzpolitik weiterhin widmen \n",
            "muss. \n",
            "1.\t Die geopolitische Zeitenwende erfordert zusätzliche \n",
            "Ausgaben für Landes- und Bündnisverteidigung wie \n",
            "auch die Unterstützung der Ukraine. Hinzu kommt eine \n",
            "immer konfrontativere Handelspolitik, die zu zuneh-\n",
            "mender Fragmentierung der Weltwirtschaft führt. Dies \n",
            "trifft die deutsche Wirtschaft mit ihrer traditionell star-\n",
            "ken Exportorientierung in besonderem Maße.\n",
            "2.\t Durch die demografische Alterung ist mit einer Ver-\n",
            "schärfung bereits bestehender Fachkräfteengpässe und \n",
            "deutlich erhöhtem Druck auf die Finanzierung der So-\n",
            "zialversicherungssysteme zu rechnen.\n",
            "3.\t Der aufgrund des Klimawandels notwendige Umbau \n",
            "der Wirtschaft auf dem Weg zur Treibhausgasneutrali-\n",
            "tät geht mit entsprechenden Kosten einher und erfor-\n",
            "dert umfassende private, aber auch öffentliche Inves-\n",
            "titionen.\n",
            "4.\t In den vergangenen Jahrzehnten vernachlässigte Stand-\n",
            "ortfaktoren hemmen Produktivität und Wachstums-\n",
            "potenzial. Hier sind immenser öffentlicher Investitions-\n",
            "stau bei Digitalisierung, Infrastruktur und Verteidigung, \n",
            "aber auch eine teils übermäßige Bürokratie unter an-\n",
            "derem in Folge europäischer Vorgaben, Berichts- und \n",
            "Nachweispflichten zu nennen. Darüber hinaus gibt es \n",
            "große Herausforderungen im Bildungsbereich, ange-\n",
            "fangen bei der frühkindlichen Bildung bis hin zur Be-\n",
            "rufsbildung.\n",
            "Um das zuletzt schwache Wachstumspotenzial der deut-\n",
            "schen Wirtschaft zu stärken und den oben genannten He-\n",
            "rausforderungen zu begegnen, hat die Bundesregierung seit \n",
            "Beginn dieser Legislaturperiode verstärkt auf angebotssei-\n",
            "tige Reformen gesetzt. Sie hat dazu das Thema Bürokratie-\n",
            "abbau einschließlich Planungs- und Genehmigungsbe-\n",
            "schleunigung in den vergangenen zwei Jahren stark \n",
            "priorisiert und zahlreiche Weichenstellungen vorgenom-\n",
            "men. So wurden unter anderem durch das vierte Bürokra-\n",
            "tieentlastungsgesetz diverse Regeln und Abläufe verein-\n",
            "facht. Mit dem „Pakt für Planungs-, Genehmigungs- und \n",
            "Umsetzungsbeschleunigung“ hat die Bundesregierung ge-\n",
            "meinsam mit den Ländern Fristen verkürzt und einzelne \n",
            "Prüfschritte in Genehmigungsverfahren reduziert. \n",
            "Mit den Praxischecks hat sie ein neues Instrument einge-\n",
            "führt, welches gezielt Wechselwirkungen zwischen den \n",
            "verschiedenen Regelungen in den Blick nimmt. Dabei wer-\n",
            "den im Austausch zwischen Unternehmen, Verwaltungen \n",
            "sowie weiteren Expertinnen und Experten anhand konkre-\n",
            "ter Fallkonstellationen gemeinsam bürokratische Hemm-\n",
            "nisse und Lösungsansätze identifiziert. Das Ver-\n",
            "fahren wurde erfolgreich mit dem Praxischeck \n",
            "„Errichtung und Betrieb von Photovoltaik-An-\n",
            "SCHL AGL ICHTER _02|25_WIRTSCHAFTSPOLITIK 10\n",
            "lagen“ pilotiert; der Großteil der identifizierten Hemmnis-\n",
            "se ist bereits aus dem Weg geräumt, u.a. im Solarpaket. Die \n",
            "Ressorts setzen einen Beschluss der Wachstumsinitiative \n",
            "dazu um und führen nunmehr vermehrt Praxischecks \n",
            "durch. Die Anstrengungen der Bundesregierung wirken sich \n",
            "bereits jetzt positiv aus: Wie der Nationale Normenkontroll-\n",
            "rat feststellt, wurde die Wirtschaft zwischen Juli 2023 und \n",
            "Juni 2024 um mehr als 400 Millionen Euro Erfüllungsauf-\n",
            "wand entlastet.\n",
            "Um das Arbeitsangebot trotz der Auswirkungen der demo-\n",
            "grafischen Alterung zu stabilisieren oder sogar auszuweiten, \n",
            "hat die Bundesregierung Reformen zur besseren Nutzung \n",
            "des Erwerbspersonenpotenzials umgesetzt bzw. in die Wege \n",
            "geleitet. So hat sie umfassende Maßnahmen für eine be-\n",
            "schleunigte und bürokratieärmere qualifizierte Einwande-\n",
            "\n",
            "des Erwerbspersonenpotenzials umgesetzt bzw. in die Wege \n",
            "geleitet. So hat sie umfassende Maßnahmen für eine be-\n",
            "schleunigte und bürokratieärmere qualifizierte Einwande-\n",
            "rung umgesetzt. Gleichzeitig hat sie zahlreiche Hürden bei \n",
            "der Arbeitsmarktintegration von bereits Zugewanderten \n",
            "abgebaut. Dies dürfte dazu beigetragen haben, dass die Er-\n",
            "werbstätigkeit im Durchschnitt des vergangenen Jahres mit \n",
            "rund 46 Millionen Personen einen Höchststand erreichte.\n",
            "Auch stellt die Bundesregierung weiterhin finanzielle Mit-\n",
            "tel für den Ausbau und die Qualität der Kindertagesbetreu-\n",
            "ung zur Verfügung, die einen Beitrag zu einer umfangrei-\n",
            "cheren Erwerbstätigkeit von Müttern leisten kann. Um die \n",
            "Erwerbsfähigkeit im Zuge der Transformation und sich \n",
            "verändernder Qualifikationsanforderungen aufrechtzu-\n",
            "erhalten, wurde die Aus- und Weiterbildung beispielsweise \n",
            "durch das Qualifizierungsgeld oder die Ausbildungsgaran-\n",
            "tie gestärkt. Diese Maßnahmen tragen auch zu einer besse-\n",
            "ren Entwicklung der Arbeitsproduktivität bei.\n",
            "Im Zuge der durch den russischen Angriffskrieg ausgelösten \n",
            "Energiekrise 2022/2023 ist es in einem gesamtgesellschaft-\n",
            "lichen Kraftakt gelungen, die Energieversorgung über alle \n",
            "Energieträger hinweg zu stabilisieren und sich von der star-\n",
            "ken Abhängigkeit von russischen Energieimporten zu lösen. \n",
            "Auch dank der Maßnahmen der Bundesregierung, wie z. B. \n",
            "den temporär eingeführten Strom-, Gas- und Wärmepreis-\n",
            "bremsen, konnten die unmittelbaren Krisenfolgen abgemil-\n",
            "dert werden. Jenseits dieser akuten Stabilisierungsmaßnah-\n",
            "men hat die Bundesregierung die Grundlagen für eine \n",
            "deutlich stärkere Dynamik beim Ausbau von erneuerbaren \n",
            "Energien sowie der Netzinfrastruktur gelegt und zügig eine \n",
            "Infrastruktur für Flüssigerdgas aufgebaut. Dies hat einen \n",
            "Beitrag dazu geleistet, dass in den beiden vergangenen Jah-\n",
            "ren jeweils Rekorde beim Ausbau und den Genehmigungen \n",
            "erneuerbarer Energien erreicht wurden. Die Strom- und \n",
            "Gaspreise an den Kurzfristmärkten sind gegenüber den Kri-\n",
            "senjahren mittlerweile deutlich gesunken. So lag der durch-\n",
            "schnittliche Strompreis für Neuabschlüsse in der Industrie \n",
            "im Jahr 2024 durch Abschaffung der EEG-Umlage unter dem \n",
            "Niveau von 2017 bis 2020. Allerdings hat sich mit der Ener-\n",
            "giekrise der Abstand der Endverbraucherpreise der deut-\n",
            "schen Industrie für Strom und Gas im Vergleich zu wichtigen \n",
            "internationalen Wettbewerbern wie den USA vergrößert. Vor \n",
            "diesem Hintergrund ist wesentliches Ziel der Bun-\n",
            "desregierung, die Energiepreise wieder rückzu-\n",
            "führen.\n",
            "ABBILDUNG 2: GENEHMIGUNGSBESCHLEUNIGUNG IM AUSBAU DER ERNEUERBAREN ENERGIEN UND \n",
            "NETZINFRASTRUKTUR\n",
            "Quelle: Eigene Darstellung, Bundesnetzagentur. *Stand Q3 2024 mit Prognose für Q4 2024. \n",
            "Megawatt\n",
            "Neugenehmigungen Wind an Land\n",
            "2021\n",
            "2022\n",
            "2023\n",
            "2024*\n",
            "Kilometer\n",
            "Netzausbau\n",
            "0\n",
            "400\n",
            "800\n",
            "1.200\n",
            "1.600\n",
            "2.000\n",
            "1.379\n",
            "1.739\n",
            "607\n",
            "1.628\n",
            "304\n",
            "481\n",
            "630\n",
            "321\n",
            "0\n",
            "1.000\n",
            "2.000\n",
            "3.000\n",
            "4.000\n",
            "5.000\n",
            "6.000\n",
            "2024\n",
            "2023\n",
            "2022\n",
            "2021\n",
            "2020\n",
            "2019\n",
            "2018\n",
            "2017\n",
            "Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4\n",
            "Q1 Q2 Q3 Q4\n",
            "Genehmigte Trassenkilometer\n",
            "Neugenehmigungen Wind an Land\n",
            "In Bau gegangene Trassenkilometer\n",
            "SCHL AGL ICHTER _02|25_WIRTSCHAFTSPOLITIK 11\n",
            "Um die Transformation der Industrie zu unterstützen und \n",
            "die Resilienz des Standorts zu erhöhen, hat die Bundesre-\n",
            "gierung den Hochlauf neuer Technologien ermöglicht. Sie \n",
            "hat dazu u. a. das Konzept der grünen Leitmärkte zur Stär-\n",
            "kung der Nachfrage nach grünen Grundstoffen, Produkten \n",
            "und Technologien entwickelt. Zudem hat sie die Ansiedlung \n",
            "in Schlüsseltechnologien wie Wasserstoff, Batteriezellfer-\n",
            "tigung, Mikroelektronik, digitalen Technologien und Bio-\n",
            "technologie unterstützt.\n",
            "Die Bundesregierung hat den deutschen Wirtschaftsstand-\n",
            "ort durch Maßnahmen im Bereich Investitionen, For-\n",
            "schung- und Entwicklung und Handel nachhaltig gestärkt. \n",
            "So hat sie die übergreifenden Rahmenbedingungen für \n",
            "Investitionen insbesondere durch attraktivere Abschrei-\n",
            "bungsbedingungen, weitergehende Möglichkeiten zur  \n",
            "Verlustverrechnung sowie eine mehrfach erheblich aus-\n",
            "geweitete steuerliche Förderung von Forschungs- und Ent-\n",
            "wicklungsausgaben verbessert. Mit dem Zukunftsfonds und \n",
            "der WIN-Initiative („Wachstums- und Innovationskapital \n",
            "für Deutschland“) soll das Ökosystem für Wagniskapital \n",
            "gestärkt und somit der Zugang zu Kapital für innovative \n",
            "Start-ups erleichtert werden. Die Investitionen in die Ver-\n",
            "kehrsinfrastruktur wurden signifikant gestärkt, insbeson-\n",
            "dere im Bereich der Schiene. Gleichzeitig konnten zuletzt \n",
            "deutliche Fortschritte bei der digitalen Infrastruktur er-\n",
            "reicht werden. Die Diversifikation der Außenhandelsbezie-\n",
            "hungen ist vorangeschritten und neue Abkommen wie jenes \n",
            "mit den Mercosur-Staaten wurden ausverhandelt oder sind \n",
            "in Arbeit.\n",
            "Angesichts der Größe der Herausforderungen ist es umso \n",
            "wichtiger, sich die immensen Stärken Deutschlands bewusst \n",
            "zu machen. Kaum ein anderes Land verfügt über eine solche \n",
            "Vielfalt an exzellenten, gerade auch mittelständischen\n",
            "\n",
            "2021 und 2023 die ausgestoßenen THG-Emissionen je  \n",
            "eine Million Euro Bruttoinlandsprodukt (preisbereinigt) um  \n",
            "34 Tonnen reduziert werden. Dies entspricht einer Entkopp-\n",
            "lungsrate von 13 Prozent in nur zwei Jahren. Das Jahr 2023 \n",
            "markiert damit die stärkste Entkopplung seit der Wieder-\n",
            "vereinigung.\n",
            "WOHLFAHRTSMESSUNG: ERWEITERTE PERSPEKTIVE \n",
            "AUF DAS WOHLERGEHEN UND KONVERGENZ­\n",
            "PROZESSE IN DER GESELLSCHAFT\n",
            "Das im Jahr 2022 eingeführte und inzwischen etablierte \n",
            "Kapitel zur Wohlfahrtsmessung blickt auch im diesjährigen \n",
            "Jahreswirtschaftsbericht auf die Entwicklung wohlfahrts-\n",
            "relevanter Aspekte. Mehr als 30 ausgewählte Indikatoren \n",
            "ergänzen das BIP gezielt und bilden neben ökonomischen \n",
            "auch wesentliche ökologische, soziale und gesellschaftliche \n",
            "Dimensionen ab. Der Blick richtet sich damit sowohl auf \n",
            "das quantitative als auch das qualitative Wachstum, welches \n",
            "die Lebensqualität und Zukunftsaussichten aller Bürgerin-\n",
            "nen und Bürger berücksichtigt.\n",
            "Die Entwicklung der Indikatoren spiegelt am aktuellen \n",
            "Rand die strukturellen Herausforderungen der deutschen \n",
            "Wirtschaft wider. So sind in den nächsten Jahren insbeson-\n",
            "dere in den Bereichen der Produktivitätsentwicklung, der \n",
            "Bildung oder des Innovationspotenzials zusätzliche An-\n",
            "strengungen notwendig. Gleichzeitig sind Erfolge wie eine \n",
            "insgesamt steigende Erwerbsbeteiligung, mehr Frauen in \n",
            "Führungspositionen oder das Voranschreiten der Energie-\n",
            "wende für eine treibhausgasneutrale Zukunft sichtbar. An-\n",
            "dere Indikatoren im Bereich des Umwelt- und Naturschut-\n",
            "zes weisen hingegen noch zu geringe bzw. abflachende \n",
            "Trendverläufe in die gewünschte Richtung auf. Weiterhin \n",
            "wird die Wohlfahrtsindikatorik stetig überprüft und weiter-\n",
            "entwickelt. So sind im diesjährigen Jahreswirtschaftsbericht \n",
            "erstmals Indikatoren zur Vermögensverteilung auf Basis \n",
            "neuer experimenteller Daten der EZB sowie zur Kreislauf-\n",
            "wirtschaft im Zuge der Nationalen Kreislaufwirtschafts-\n",
            "strategie enthalten. Auch in den kommenden Jahren wird \n",
            "es vor dem Hintergrund einer evidenzbasierten Politik \n",
            "wichtig sein, mithilfe eines solchen Indikatorensets die \n",
            "Fortschritte auf einem nachhaltigen und zukunftsfähigen \n",
            "Wachstumspfad zu verfolgen.\n",
            "Referat: IA1 – Grundsatzfragen der Wirtschaftspolitik,  \n",
            "IC-WA – Wirtschaftspolitische Analyse, Wohlfahrtsindikatorik, \n",
            "IC1 – Beobachtung, Analyse und Projektion der gesamt­\n",
            "wirtschaftlichen Entwicklung \n",
            "schlaglichter@bmwk.bund.de\n",
            "Jahreswirtschaftsbericht 2025: Für eine neue wirtschaftliche \n",
            "Dynamik\n",
            "Pressemitteilung des BMWK\n",
            "KONTAKT & MEHR ZUM THEMA\n",
            "\n",
            "wichtiger, sich die immensen Stärken Deutschlands bewusst \n",
            "zu machen. Kaum ein anderes Land verfügt über eine solche \n",
            "Vielfalt an exzellenten, gerade auch mittelständischen \n",
            "Unternehmen. Nach wie vor wird die hohe Fachkompetenz \n",
            "in den deutschen Betrieben und die institutionelle For-\n",
            "schungslandschaft und das damit verbundene Innovations-\n",
            "potenzial in Deutschland weltweit geschätzt. Diese Stärken \n",
            "bilden auch unter Berücksichtigung der im Vergleich zu \n",
            "anderen führenden Wirtschaftsnationen deutlich geringe-\n",
            "ren öffentlichen und privaten Verschuldung eine gute  \n",
            "Ausgangsposition, um die großen gesamtwirtschaftlichen \n",
            "Investitionsbedarfe zu adressieren und die Wettbewerbs-\n",
            "fähigkeit wieder zu verbessern.\n",
            "INVESTIVE IMPULSE SETZEN, UNSICHERHEITEN \n",
            "ABBAUEN\n",
            "Die Transformation zur Klimaneutralität, die digitale Trans-\n",
            "formation und die erforderliche Steigerung der Wettbe-\n",
            "werbsfähigkeit verlangen der Wirtschaft erhebliche Anpas-\n",
            "sungen ab. In kurzer Zeit müssen erhebliche – vorrangig \n",
            "private – finanzielle Mittel mobilisiert werden, um in neue \n",
            "Technologien, Infrastrukturen und Prozesse zu investieren \n",
            "sowie neue Wachstumsmärkte zu erschließen. Dafür \n",
            "braucht es verlässliche Rahmenbedingungen, aber auch zu-\n",
            "sätzliche Investitionsanreize. \n",
            "Der finanzpolitische Kurs der Bundesregierung ist seit 2023 \n",
            "als Ergebnis der Rückführung krisenbedingter Entlastungs- \n",
            "und Stabilisierungsmaßnahmen, die zur Bewältigung der \n",
            "Corona- und der Energiekrise notwendig waren, moderat \n",
            "restriktiv ausgerichtet, wie auch die übereinstimmenden \n",
            "Einschätzungen von IWF, OECD und der EU-Kommission \n",
            "zeigen. Dieser Kurs ist auch im Kontext der zeitweise hohen \n",
            "Preissteigerungsraten zu sehen, die sich seit Ende 2023 deut-\n",
            "lich verringert haben, aber noch immer auf erhöhtem Ni-\n",
            "veau lagen. Im Zuge der Verringerung des fiskalischen Spiel-\n",
            "raums im Klima- und Transformationsfonds um insgesamt \n",
            "60 Milliarden Euro als Konsequenz des Urteils des Bundes-\n",
            "verfassungsgerichts vom November 2023 mussten wirt-\n",
            "schaftspolitische Vorhaben verstärkt priorisiert werden. Das \n",
            "betrifft insbesondere finanzielle Anreize für Investitionen \n",
            "von Unternehmen und privaten Haushalten und dürfte \n",
            "zwischenzeitlich die Unsicherheit bei Wirtschaftsakteuren \n",
            "erhöht haben.\n",
            "Daher ist die Wirtschafts- und Finanzpolitik künftig doppelt \n",
            "gefordert: Es gilt, den in den vergangenen Jahren einge-\n",
            "schlagenen Weg struktureller Reformen konsequent fort-\n",
            "zusetzen und zugleich das Vertrauen der Wirtschaftsakteu-\n",
            "re zu stärken. Für die mittelfristigen Wachstumsaussichten \n",
            "sind die Bereiche unternehmerischer Investitionen und \n",
            "Innovationen sowie Forschung und Entwicklung von be-\n",
            "sonderer Bedeutung. Beispiele für mögliche Impulse zur \n",
            "Stärkung der Investitionstätigkeit wären weitere Verbesse-\n",
            "rungen im Sinne eines innovationsfreundlichen steuerli-\n",
            "chen Umfeldes oder die Einführung einer unbürokratischen \n",
            "und unkomplizierten Investitionsprämie. Maßnahmen zur \n",
            "Stärkung der Investitionstätigkeit würden den Standort \n",
            "Deutschland auch für ausländische Investoren attraktiver \n",
            "machen. Für gewerblich genutzte E-Autos, den klima-\n",
            "freundlichen Wohnungsneubau oder den sozialen Woh-\n",
            "nungsbau wären weitere, auch nachfrageseitige Impulse \n",
            "denkbar. \n",
            "KLIMAPOLITIK: TREIBHAUSGAS (THG)-­\n",
            "EMISSIONEN GEHEN WEITER ZURÜCK\n",
            "Es bleibt im Interesse Deutschlands, national, EU-weit und \n",
            "international dazu beizutragen, die globale Treibhausgas-\n",
            "entwicklung umzukehren und gleichzeitig Maßnahmen zur \n",
            "Anpassung an den Klimawandel zu ergreifen. Denn mit fort-\n",
            "schreitendem Klimawandel steigen die Risiken irreversibler \n",
            "Umweltveränderungen mit nicht zuletzt erheblichen Folgen \n",
            "für die Grundlagen wirtschaftlicher Wertschöpfung. Mit \n",
            "dem europäischen Fit-for-55-Paket und der na-\n",
            "tionalen Umsetzung wurden weitreichende am-\n",
            "bitionierte Maßnahmen für eine wettbewerbs-\n",
            "SCHL AGL ICHTER _02|25_WIRTSCHAFTSPOLITIK 12\n",
            "fähige, ressourceneffiziente Wirtschaft im Sinne des \n",
            "European Green Deals und des europäischen Klimaschutz-\n",
            "ziels auf den Weg gebracht. Auf nationaler Ebene gibt das \n",
            "Klimaschutzgesetz seit 2019 den nationalen rechtlichen \n",
            "Rahmen für die Klimaschutzpolitik vor. \n",
            "In den letzten Jahren konnte die Bundesregierung entschei-\n",
            "dende Schritte zum Erreichen des 2030-Klimaschutzziels \n",
            "gehen: Während zu Beginn der Legislaturperiode noch eine \n",
            "Minderung der Treibhausgase bis 2030 von lediglich 49 Pro-\n",
            "zent projiziert wurde, weisen die aktuellen Projektionsdaten \n",
            "einen deutlich stärkeren Rückgang der Treibhausgasemis-\n",
            "sionen um knapp 64 Prozent aus. Das zeigt, dass das Gesamt-\n",
            "minderungsziel des Bundes-Klimaschutzgesetzes von min-\n",
            "destens 65 Prozent bis 2030 bei anhaltenden Fortschritten \n",
            "in Reichweite liegt. Insgesamt hat sich die Entkopplung von \n",
            "THG-Emissionen und ökonomischer Wertschöpfung in den \n",
            "letzten Jahren deutlich beschleunigt. So konnten zwischen \n",
            "2021 und 2023 die ausgestoßenen THG-Emissionen je  \n",
            "eine Million Euro Bruttoinlandsprodukt (preisbereinigt) um  \n",
            "34 Tonnen reduziert werden. Dies entspricht einer Entkopp-\n",
            "\n",
            "    Question:\n",
            "    What is the main topic of these documents?\n",
            "     Answer:\n",
            "    The main topic of these documents is the economic and political situation of Germany and its economic policies.\n",
            "\n",
            "    Context:\n",
            "    The documents appear to be from the Federal Ministry of Economic Affairs and Energy (BMWK) in Germany. They discuss the country's economic situation,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/spaces/Raijin-ASR/RAG-chat-pdf/blob/main/app.py"
      ],
      "metadata": {
        "id": "fR3aqJaMHr_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/spaces/rafaldembski/PDF-CHATBOT/blob/main/app.py"
      ],
      "metadata": {
        "id": "PXaFR0SbIxRO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NRW0vTRvH5Z3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
